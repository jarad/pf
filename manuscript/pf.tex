\documentclass{elsarticle}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}

\graphicspath{{/Users/niemi/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/Users/niemi/Dropbox/SIR_Particle_Filtering/doesnotexist/}
                       {/Users/Danny/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/data/home/faculty/meiring/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {./Graphs/}
                       % Add your directories here
}

\newcommand{\jarad}[1]{{\color{red}JARAD: #1}}
\newcommand{\danny}[1]{{\color{blue}DANNY: #1}}

\journal{Mathematical Biosciences}

\begin{document}

\begin{frontmatter}

\title{Estimation of a disease epidemic using particle filtering}

\author[danny]{Daniel M. Sheinson}
\author[jarad]{Jarad Niemi}
\author[wendy]{Wendy Meiring}

\address[danny]{Corresponding author -- Department of Statistics and Applied Probability, University of Califonia--Santa Barbara, \\
 Santa Barbara, CA, U.S.A., sheinson@pstat.ucsb.edu, 1-847-609-7824}
 \address[jarad]{Department of Statistics, Iowa State University, Ames, IA, U.S.A., niemi@iastate.edu}
 \address[wendy]{Department of Statistics and Applied Probability, University of Califonia--Santa Barbara, \\
 Santa Barbara, CA, U.S.A., meiring@pstat.ucsb.edu}

\begin{abstract}
This paper presents methodology for tracking and prediction of a disease outbreak via a syndromic surveillance system. We present a nonlinear stochastic state-space model with unknown fixed parameters defining the outbreak's infectivity and recovery rate. We show that standard particle filters fail due to degeneracy of these fixed parameters and suggest the use of a kernel density approximation to the filtered distribution of the fixed parameters to allow their regeneration. In addition, we show that seemingly uninformative uniform priors on fixed parameters can affect posterior inferences and suggest the use of priors bounded only by the support of the parameter. We show the negative impact of using multinomial resampling and suggest the use of either stratified or residual resampling within the particle filter. Finally, we use this improved particle filtering methodology to relax prior assumptions of model parameters yet still providing reasonable estimates for model parameters and disease states. 
\end{abstract}


\begin{keyword}
Bayesian estimation; epidemiological modeling; state-space modeling; sequential Monte Carlo; particle filtering.
\end{keyword}

\end{frontmatter}

\let\thefootnote\relax\footnotetext{Abbreviations: BF - bootstrap filter; APF - auxiliary particle filter; KDPF - kernel density particle filter}

\section{Introduction} \label{sec:intro}
% Introduce syndromic surveillance
Syndromic surveillance system \citep{henning2004overview,hakenewerth2009north,Gins:Mohe:Pate:Bram:Smol:Bril:dete:2009} are commonly used throughout the world to identify emerging disease outbreaks 

% Introduce Bayesian framework, MCMC
In statistical applications where prior knowledge or beliefs about unknown quantities are available, the Bayesian framework is often convenient for performing statistical analysis.  Bayesian inference is conducted through the posterior distribution of any unknown quantities given the observed data. However, the calculation of the posterior distribution frequently involves complicated integrals without an explicit analytical form. Advances in computing have helped alleviate this problem through the use of simulation-based methods such as Markov chain Monte Carlo (MCMC) \citep{Gelf:Smit:samp:1990} to generate samples from the posterior distribution.

% Introduce sequential Monte Carlo
A drawback of MCMC is that, when new data become available, the entire analysis needs to be performed again. That is, samples from the previous analysis cannot be reused and new samples need to be generated, a process that often times requires significant time and computing power. This is computationally inefficient and impractical in fields where data are collected sequentially and real time analysis is needed, such as when tracking an aircraft using radar or monitoring the stock market. Sequential Monte Carlo (SMC) - or particle filtering - methods enable on-line inference by updating the estimate of the posterior as new data become available. Furthermore, SMC methods are flexible, general, easy to implement, and amenable to computing in parallel. For a general introduction, please see \cite{Douc:deFr:Gord:sequ:2001} and \cite{cappe2007overview}.

% Introduce bootstrap filter, auxiliary particle filter
The first and most basic particle filter - named the bootstrap filter - was developed by \citet{Gord:Salm:Smit:nove:1993} and \citet{Kita:mont:1996}. The bootstrap filter uses an iterative algorithm that samples values (i.e. particles) of the unknown quantities from a prior distribution, propagates the particles forward in time using an evolution equation, calculates weights for each particle using a likelihood function based on the data observed up to that time, and resamples the particles based on these weights. One issue that can arise with this algorithm is that when particles that are less likely given the data are generated during propagation, particle weights become unevenly distributed. A greater number of particles are then needed to obtain a representative sample from the posterior density, making the algorithm less efficient. The auxiliary particle filter, developed by \citet{Pitt:Shep:filt:1999}, attempts to provide a more representative sample from the posterior without increasing the number of particles.  This is done by looking ahead at the next data point prior to the resampling step and then sampling particles in areas of higher likelihood, leading to a more even distribution of particle weights.

% Introduce degeneracy, KDPF
A second issue arises in both the bootstrap filter and auxiliary particle filter when one or more of the unknown quantities are held fixed over time since, in this case, new values are not generated during propagation. This problem arises as a result of the resampling step, where particles with large weights emit more copies of themselves into the next iteration of the filter, while particles with small weights get eliminated. This can lead to \emph{particle attrition}, or a degeneracy of particles toward a single value. An algorithm intended to combat particle degeneracy that builds on the auxiliary particle filter and distinguishes fixed parameters from unobserved states that evolve over time was developed by \citet{Liu:West:comb:2001}. Their version of the particle filter avoids degeneracy by resampling fixed parameter values from a normal kernel density approximation of the posterior distribution of the fixed parameters given all data collected up to the current time point. Further improvements to the efficiency of the algorithm include using a measure of nonuniformity of particle weights to determine when to resample \citep{Douc:Capp:Moul:comp:2005}, incorporating an MCMC step to refresh values of the fixed parameters \citep{Gilk:Berz:foll:2001}, and taking advantage of a sufficient statistic structure in the model \citep{Fear:mark:2002}.

% Introduce epidemiological modeling, Skvortsov Math. Bio. paper
Epidemiogical modeling is an area where particle filtering can play an important role. Models of disease outbreaks can be broken into two parts: a dynamic model of how the epidemic evolves over time and a measurement model of syndromic observations. Thus, it is easy to conceptualize an epidemic from a Bayesian viewpoint as a filtering problem using a state-space model with a state evolution equation and an observation equation. Since many mechanistic models of disease transmission are nonlinear and develop in real time, sequential Monte Carlo is a natural fit. For example, \citet{skvortsov2012monitoring} analyzed a simulated epidemic in the form of a stochastic compartmental epidemiological model, using the bootstrap filter to estimate the state of the epidemic and unknown fixed paremeters. Motivated by their approach, we demonstrate that substantial improvement in estimation in terms of computational efficiency and avoiding particle degeneracy can be obtained by incorporating more recent developments in the particle filtering methodology.

% Introduce layout of paper (see main document)
The rest of the article proceeds as follows. Section \ref{sec:ss} contains a description of state-space models and sequential estimation. Section \ref{sec:filtering} describes a variety of particle filtering methodologies including the kernel density approach of \cite{Liu:West:comb:2001}. In Section \ref{sec:apply}, we introduce a nonlinear dynamic model for a disease epidemic similar to the one used in \citet{skvortsov2012monitoring}. In Section \ref{sec:results}, we apply the particle filtering methods described in Section \ref{sec:filtering} to the model described in Section \ref{sec:apply}. In Section \ref{sec:extend}, we benefit from the efficiency of more recent particle filtering methods to estimate a more complicated model. In Section \ref{sec:discussion}, we conclude by pointing to additional particle filtering methods that may be helpful in certain model structures.

\section{State-space models \label{sec:ss}}

State-space models are a general class of statistical models used for analysis of dynamic data and have been used extensively in modeling disease outbreaks \citep{Mart:Cone:Lope:Lope:baye:2008,watkins2009disease,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring}.  State space models are constructed using an observation equation, $y_t \sim p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \sim p_{x,t}(x_t|x_{t-1},\theta)$ where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, and $\theta$ is the unknown fixed parameter, all of which could be vectors. The distributions are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only vary with $x_t$ and $\theta$ and therefore the $t$ subscript is dropped.
%The particle filtering methodology discussed in Sections \ref{sec:filtering} and \ref{sec:discussion} apply equally well when the distributions depend on $t$ so long as they are known.
For simplicity, we will also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the state-space model is
\begin{align*}
y_t &\sim p(y_t|x_t,\theta) \\
x_t &\sim p(x_t|x_{t-1},\theta).
\end{align*}

Special cases of these state-space models include hidden Markov models \citep{cappe2005inference}, where the state $x_t$ has finite support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997}, where each distribution is Gaussian with mean a linear function of the states.

\subsection{Sequential estimation \label {sec:sequential}}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, the distribution of the current state and parameters conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters. It can be updated recursively using Bayes' rule:
\begin{equation}
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$. Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically, e.g. in DLMs when $\theta$ is the observation variance \cite[Sec 4.3,][]{petris2009dynamic}. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. extended Kalman filter and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions such as particle filters.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is a sequential Monte Carlo (SMC) inferential technique based on sequential use of importance sampling. It aims to approximate equation \eqref{eqn:filtered} through a weighted Monte Carlo realization from this distribution, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j}^J w_t^{(j)} \delta_{(x_t^{(j)},\theta^{(j)})} \label{eqn:approx}
\end{equation}
where $w_t^{(j)},j=1,\ldots,J$ are the particle \emph{weights}, $(x_t^{(j)},\theta^{(j)})$ is the particle \emph{location}, and $\delta$ is a Dirac delta function. A variety of SMC techniques have been developed to provide more efficient approximations to equation \eqref{eqn:filtered} in the sense that with the same computation time a better approximation is achieved. We now introduce three fundamental particle filtering techniques: the bootstrap filter, auxiliary particle filter, and kernel density particle filter. Later, we will compare the efficiency of these techniques.

\subsection{Bootstrap filter \label{sec:bf}}

The first successful version of particle filtering is known as the bootstrap filter (BF) \citep{Gord:Salm:Smit:nove:1993}. Since this method and the auxiliary particle filter were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Suppose you have a current approximation as in equation \eqref{eqn:approx}. To move from an approximation of $p(x_t| y_{1:t})$ to that of $p(x_{t+1}| y_{1:t+1})$, we perform the following steps for each particle $j$:

\begin{enumerate}
\item Resample: sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{w_t^{(1)},\ldots,w_t^{(j)},\ldots,w_t^{(J)}\}$ and set
    \[ x_t^{(j)} = x_t^{(k)} \qquad w_t^{(j)} = 1 / J, \]
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(\left. x_{t+1}\right|x_t^{(j)}\right)$, and
\item Calculate weights: $\tilde{w}_{t+1}^{(j)} = w_t^{(j)}p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)$.
\end{enumerate}

\noindent After weights for all particles have been calculated, these weights need to be renormalized via $w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right.$.

This procedure can be applied recursively provided an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$. The algorithm as described above uses a multinomial resampling scheme, i.e. when we say `sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ \ldots'. This is not the only way to resample the particles, and we compare multinomial resampling with three other techniques in Section \ref{sec:resample}. In addition, we can optionally forgo the `Resample' step (hence the necessity of the $w_t^{(j)}$ term in the `Calculate weights' step) to make the algorithm more efficient. We discuss more about resampling in Section \ref{sec:results}.

\subsection{Auxiliary particle filter \label{sec:apf}}

One problem that arises in implementing the BF is that, as $t$ increases, $w_t^{(j)}$ may become small for samples $x_t^{(j)}$ that are less likely given $y_t$. These samples will likely get eliminated during resampling, and thus a greater number of total particles, $J$, is then required to obtain an accurate sample from $p(x_{t+1}|y_{1:t+1})$. An more efficient algorithm in that sense that a more representative sample from $p(x_{t+1}|y_{1:t+1})$ can be obtained for fixed $J$ was developed by \citet{Pitt:Shep:filt:1999} called the auxiliary particle filter (APF). The idea behind the APF is to generate particles with higher likelihood given the data by "looking ahead" at $p(x_{t+1}|x_t^{(j)})$ prior to resampling. Given a weighted random sample of particles at time $t$, the APF approximates $p(x_{t+1}|y_{1:t+1})$ by the following:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$ where
\[ \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)} \right.\right). \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p(y_{t+1}|\mu_{t+1}^{(j)}) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^l.\right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$ and set
         \[ x_t^{(j)} = x_t^{(k)} \qquad w_t^{(j)} = 1 / J \qquad \mu_{t+1}^{(j)} = \mu_{t+1}^{(k)}, \]
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(\left. x_{t+1}\right|x_t^{(j)}\right)$, and
	\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = w_t^{(j)}\frac{p\left(y_{t+1}|x_{t+1}^{(j)}\right)}{p\left(y_{t+1}|\mu_{t+1}^{(j)}\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent As with the BF, the `Resample' step does not necessarily need to be carried out at every iteration of the algorithm, and we may choose to skip this step if particle weights are not out of balance.

The BF and the APF were constructed with the idea that all fixed parameters were known. In order to simultaneously estimate the states and fixed parameters using these methods it is necessary to incorporate the fixed parameters into the state with degenerate evolutions. Due to the resampling step, the number of unique values of the fixed parameters in the particle set will decrease over time, resulting in \emph{degeneracy} \citep{Liu:West:comb:2001}.

\subsection{Kernel density particle filter \label{sec:kd}}

The particle filter introduced by \citet{Liu:West:comb:2001} is a general way of fighting this degeneracy problem by approximating the set of fixed parameter values by a kernel density estimate and then resampling from this approximation at each step in the filter. This enables us to refresh the sampled values of $\theta$ to reduce degeneracy and require a lower number of particles to obtain an accurate weighted sample approximation of the filtered distribution of the states and fixed parameters. We will refer to this filter as the kernel density particle filter (KDPF).

We now reintroduce the fixed parameters $\theta$ into the notation. The KDPF provides an approximation to $p(x_t,\theta| y_{1:t})$ via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ in particle $j$. This does not imply that $\theta$ is dynamic, but rather that the particle can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of the posterior sample $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The KDPF uses a tuning parameter $\Delta$, the discount factor, and two derived quantities $h^2 = 1 - ((3\Delta - 1)/2\Delta)^2$ and $a^2 = 1 - h^2$ that determine how sharp the kernel density approximation is. Typically $\Delta$ is taken to be between 0.95 and 0.99 \citep{Liu:West:comb:2001}.

With the KDPF, we move to an approximation of $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $\left(x_{t+1}^{(j)},\theta\right)$ given by $\left(\mu_{t+1}^{(j)},m_t^{(j)}\right)$ where
    \[
    \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right) \qquad
    m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t.
    \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^l. \right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$ and set \[x_t^{(j)} = x_t^{(k)} \qquad w_t^{(j)} = 1 / J \qquad \left(\mu_{t+1}^{(j)},m_t^{(j)}\right) = \left(\mu_{t+1}^{(k)},m_t^{(k)}\right),\]
	\item Regenerate the fixed parameters:
	\[ \theta_{t+1}^{(j)} \sim N\left( m_t^{(j)}, h^2V_t \right), \]
	\item Propagate:
	\[ x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(j)},\theta_{t+1}^{(j)}\right.\right), \mbox{ and} \]
	\item Calculate weights and renormalize:
	\[ \tilde{w}_{t+1}^{(j)} = w_t^{(j)}\frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent Again, we may choose to forgo the `Resample' step if particle weights are not out of balance. Furthermore, we may also choose not to regenerate the fixed parameters and instead set $\theta_{t+1}^{(j)} = \theta_t^{(j)}$. This makes sense from an efficiency standpoint, since if we do not resample particles, then there is no elimination of unique fixed parameter values.

To use the KDPF with normal kernels it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$, and then the logit transformation can be applied. We discuss the sensitivity of the performance of the KDPF to the choice of transformation later on.

\subsection{General advice \label{sec:advice}}

Practical implementation of any particle filter involves a choice of resampling function and a decision about when resampling should be performed. Throughout our discussion, we have implicitly used multinomial resampling. Alternative resampling functions exist including stratified, residual, and systematic resampling. In addition, the frequency of resampling should be reduced to decrease the amount of Monte Carlo variability added to the particle sample. Typically, a measure of the nonuniformity of particle weights is used including effective sample size, entropy, and coefficient of variation. For a discussion of these topics, please see \cite{Douc:Capp:Moul:comp:2005}.

\section{Particle filtering in epidemiological models \label{sec:apply}}

We now describe how epidemiological modeling can be addressed via state-space models with sequential estimation as described in Section \ref{sec:ss}. Thereby we set the stage for application of the particle filtering methodology described in Section \ref{sec:filtering}. As in \citet{skvortsov2012monitoring}, we consider a modified SIR model with stochastic fluctuations to describe the dynamics of the disease outbreak \citep{herwaarden1995stochepid, dangerfield2009stochepid, anderson2004sars}. According to this model, we keep track of the proportion of a population susceptible to ($s_t$), infected by ($i_t$), and recovered from ($r_t$) disease over time, $t$. $s_t$, $i_t$, and $r_t$ are all nonnegative, and we require $s_t + i_t + r_t = 1$ for all $t$.

When monitoring an epidemic, the true $s_t$, $i_t$ and $r_t$ are unknown and regarded as hidden states of the model. The data that we observe are gathered from syndromic surveillance, or the systematic collection and monitoring of public health data by public health agencies \citep{wagner2006biosurveillance, wilson2006synsurveillance}. These measurements can consist of medical observations such as emergency room visits as well as non-medical data such as absenteeism from work and queries from search engines or social media \citep{chew2010twitter, schuster2010searchquery, signorini2011twitter, Gins:Mohe:Pate:Bram:Smol:Bril:dete:2009}. Thus, we fuse information from syndromic surveillance with the disease transmission dynamics through a state-space model. In our state-space model of a disease outbreak, the observation equation specifies how the observed data depend on the state of the epidemic and other parameters, and the state equation describes how the epidemic evolves over time.

\subsection{Model \label{sec:model}}

First, we describe the state equation. Let $x_t = (s_t,i_t)'$ denote the state of the epidemic at time $t$. Note that, by definition, $r_t$ is completely specified once $s_t$ and $i_t$ are known, and hence is not needed in the state vector. Our compartmental model of disease transmission is governed by three parameters:

\begin{itemize}
\item $\beta$, the contact rate of spread of disease,
\item $\gamma$, the recovery time from infection, and
\item $\nu$, the mixing intensity of the population.
\end{itemize}

\noindent $\beta$, $\gamma$, and $\nu$ are each restricted to be nonnegative. Define $\theta = (\beta,\gamma,\nu)'$ to be the vector of unknown parameters in our model, and let $P$ be the size of the population. Then, we describe the evolution of the epidemic from time $t$ to $t + \tau$ (for $\tau > 0$) by
\begin{equation}
x_{t+\tau}\left|x_t,\theta\right. \sim N_\Omega\left(f_\tau(x_t,\theta),Q_{\tau}(\theta)\right) \label{eqn:state}
\end{equation}
\noindent where
\[
f_\tau(x_t,\theta) = \left(
\begin{array}{c}
s_t - \tau\beta i_ts^{\nu}_t \phantom{- \tau\gamma i_t}\,\, \\
i_t +  \tau\beta i_ts^\nu_t - \tau\gamma i_t
\end{array}
\right)
\qquad
Q_\tau(\theta) = \frac{\beta \tau^2}{P^2} \left(
\begin{array}{ccccc}
1 & -1 \\
-1 & 1 + \gamma/\beta
\end{array}
\right)
\]

\noindent and $\Omega = \{(s_t,i_t): s_t \ge 0, i_t \ge 0, s_t + i_t \le 1\}$. $N_{\Omega}(\mu,\Sigma)$ represents the normal distribution with mean $\mu$ and covariance matrix $\Sigma$ truncated onto the set $\Omega$. Notice that in equation \eqref{eqn:state}, the joint distribution of $s_t$ and $i_t$ has been truncated since they are proportions that must have sum no greater than 1. To generate values from this density during the `Propagate' step of a particle filter, we sample from the untruncated normal distribution, reject any samples outside of $\Omega$, and resample when needed.

%The dynamics of the epidemic with respect to time, $t$, are then described by the following differential equations:
%
%\begin{align}
%\frac{ds}{dt} &= -\beta is^\nu + \epsilon_\beta \label{eqn:dsdt} \\
%\frac{di}{dt} &= \beta is^\nu - \gamma i - \epsilon_\beta + \epsilon_\gamma \label{eqn:didt}
%\end{align}

%The standard deviations of the noise terms are approximated by
%\[\sigma_\beta \approx \frac{\sqrt{\beta}}{P} \mbox{, } \sigma_\gamma \approx \frac{\sqrt{\gamma}}{P}\]
%This comes from a well-known scaling rule of random fluctuations in the contact rate and recovery time \citep{ovaskainen2010extinction, herwaarden1995stochepid, dangerfield2009stochepid}.

Next, we describe the observation equation. We consider observed data that are positive real numbers related to counts of emergency room visits, prescription sales, or calls to a hotline, for example, and we can observe data from these different streams/sources asynchronously in time. That is, at any time $t$, we can observe data from any subset of the streams (or possibly none of them). Let $y_{l,t}$ represent data coming from stream $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$. We model the the log of the observations (so that $y_{l,t}$ is restricted to be positive) by
\begin{equation}
\log y_{l,t} \sim N\left(b_li_t^{\varsigma_l},\sigma_l^2\right) \label{eqn:obs}
\end{equation}
where $b_l$, $\varsigma_l$, and $\sigma_l$ are nonnegative constants \citep{skvortsov2012monitoring}. Initially, we assume $b_l$, $\varsigma_l$, and $\sigma_l$ are known for all $l$. In an extended analysis described in Section \ref{sec:extend}, we demonstrate that improved SMC methods enable us to regard these constants as unknown and include them in $\theta$.

Having formulated the data-generating model, we define $y_t = (y_{1,t},y_{2,t},\ldots,y_{L,t})'$ and specify the likelihood of an observation $y_t$ given $x_t$ and $\theta$ - i.e. $p(y_t|x_t,\theta)$ - by $y_t|x_t,\theta \sim \log N(\mu_t,\Sigma_t)$, where $\mu_t$ is an $L$-length vector with element $l$ equal to $b_li_t^{\varsigma_l}$ and $\Sigma_t$ is an $L \times L$ diagonal matrix with the $l^{\mbox{th}}$ diagonal equal to $\sigma_l^2$. Elements of $y_t$ may be missing, in which case the dimension of $p(y_t|x_t,\theta)$ shrinks by the number of missing elements. %If the $j^{\mbox{th}}$ element of $y_t$ is missing, then the $j^{\mbox{th}}$ element of $\mu_t$ and $j^{\mbox{th}}$ row and column of $\Sigma_t$ are also missing and thus omitted in the calculation of the likelihood.
If all elements of $y_t$ are empty (i.e. if no syndromic data are observed at time $t$), particle weights are kept the same as they were at the previous time step.

%A further note on propagating particles is necessary.  Notice from equation \eqref{eqn:state} that the transition density needed to propagate $x_t$ in our model is a multivariate normal density with mean $f_\tau(x_t,\theta)$, and recall that the mean function approximates the disease dynamics better for small $\tau$.  Thus, for data observed at times $t_1$ and $t_2$ where the time between observations, $t_2 - t_1$, is large, the mean of $p(x_{t_2}|x_{t_1}^{(j)})$ for each particle $j$ may be inaccurate if $\tau$ is set to $t_2 - t_1$.  To ensure that the transition density matches the actual disease dynamics more closely, we can choose $d$ to be an integer larger than 1, set $\tau = (t_2 - t_1) / d$, and propagate $x_{t_1}^{(j)}$ forward $d$ times to generate $x_{t_2}^{(j)}$.

Since $p(x_{t+\tau}|x_t,\theta)$ and $p(y_t|x_t,\theta)$ are nonlinear functions of $x_t$ and $\theta$, a closed-form solution to the posterior $p(x_t,\theta|y_{1:t})$ cannot be obtained. Thus, we use the particle filtering techniques described in Section \ref{sec:filtering} to approximate $p(x_t,\theta|y_{1:t})$ for all $t$.

\section{Results} \label{sec:results}

We now compare the performance of the BF, APF and KDPF using simulated data.  An epidemic lasting $T = 125$ days was simulated from our model for a population of size $P = 5000$. True values of parameters that we estimate (termed "unknown") were set to $\beta = 0.2399$, $\gamma = 0.1066$, and $\nu = 1.2042$, and values of known constants for $L = 4$ streams are given in Table \ref{tab:constants}. Infection was introduced in 10 people in the population at day 0 (i.e. true $i_0 = 10/5000$ and $s_0 = 4990/5000$). The epidemic evolved over time according to equation \eqref{eqn:state} (with $\tau = 1$) and data from randomly selected streams at each day were generated from equation \eqref{eqn:obs}. The simulated epidemic peaks around $t = 50$ days, affecting roughly 17\% of the population (see Figure \ref{fig:data}).

\begin{table}[ht]
\begin{center}
\caption{Values of known constants in model.}
\label{tab:constants}
\begin{tabular}{|cccc|}
\hline
$l$ & $b_l$ & $\varsigma_l$ & $\sigma_l$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\centering
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-epid}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-z}
\end{minipage}
\caption{Simulated epidemic curves (left) and syndromic observations (right).} \label{fig:data}
\end{figure}

\subsection{Particle filter runs} \label{sec:pf}

The BF, APF, and KDPF were each run on the simulated data using $J = 1000, 10000, 20000, \mbox{ and } 40000$ particles to obtain weighted sample approximations of $p(x_t,\theta|y_{1:t})$ for $t = 1,\ldots,T$. For each $J$, separate runs using multinomial, residual, stratified, and systematic resampling were implemented, and an effective sample size threshold set at 80\% of the total number of particles was used to determine when to resample particles \citep{Liu:Chen:Wong:reje:1998}. For the KDPF, the discount factor $\Delta$ was set to 0.99 to determine the variance of the normal kernels used to approximate $p(\theta|y_{1:t})$. The `Regenerate the fixed parameters' step in the KDPF was carried out only when the `Resample' step was performed.

$x_0$ and $\theta_0$ were sampled from prior density $p(x_0,\theta) = p(\theta)p(i_0,s_0)$, where $p(i_0,s_0)$ is the joint pdf of the random variables $i_0$ and $s_0$ and $p(\theta) = p(\beta)p(\gamma)p(\nu)$. Since a very small percentage of the population is infected during the initial stage of the epidemic, we let $p(i_0) = N_{[0,1]}(i_0;0.002,0.0005^2)$. Then, we set $s_0 = 1 - i_0$ and sample $i_0$ and $s_0$ jointly since, at $t = 0$, no infected individuals have recovered from illness yet.

To investigate the impact of different prior distributions of $\theta$ on performance of the particle filters, the runs described above were performed once using uniform priors on $\theta$ and then again using log-normal priors. Uniform priors on $\theta$ were chosen to be the same as those used in \citet{skvortsov2012monitoring}, i.e. $p(\beta) \sim U[0.14, 0.50]$, $p(\gamma) \sim U[0.09, 0.143]$, and $p(\nu) \sim U[0.95,1.3]$. Log-normal priors on $\theta$ are given by $\beta \sim \ln N\left(-1.3296, 0.3248^2\right)$, $\gamma \sim \ln N\left(-2.1764, 0.1183^2\right)$, and $\nu \sim \ln N\left(0.1055, 0.0800^2\right)$. These log-normal priors constrain $\beta$, $\gamma$ and $\nu$ to be positive. The mean and variance of the log of the unknown parameters were chosen so that random draws on the original scale would fall within the uniform bounds with 95\% probability.

Logit and log transformations were applied to the components of $\theta$ in the manner described at the end of Section \ref{sec:kd} so that the normal kernel could be used in the KDPF while constraining $\beta$, $\gamma$, and $\nu$ to be within their respective prior domains (i.e. logit was used with uniform priors and log with log-normal priors). To aid comparison of the particle filters, the same prior draws were used in the BF, APF, and KDPF as long as the number of particles and prior were the same.

\subsection{Comparison of particle filter algorithms under uniform priors} \label{sec:pfcomparison}

First, we compare the performance of the particle filtering algorithms using uniform priors on $\theta$ and systematic resampling, since these priors and resampling scheme were used in \citet{skvortsov2012monitoring}. Figure \ref{fig:pfs} shows 95\% credible bounds of $p(\beta|y_{1:t})$, $p(\gamma|y_{1:t})$, and $p(\nu|y_{1:t})$ for $t = 1,2,\ldots,T$ and $J = 1000, 10000, 20000, 40000$. The bounds for the BF (red lines) and APF (blue lines) start out wide and then degenerate toward a single value because of the elimination of unique particles during resampling. Although the time of degeneracy increases as $J$ gets larger, the BF and APF bounds become misleading during the second half of the epidemic even for $J = 40000$. The bounds for the KDPF (green lines), on the other hand, never degenerate because new values of $\theta$ are sampled from the normal kernel density approximation of $p(\theta|y_{1:t})$ whenever particles are resampled.

The KDPF also has an advantage over the BF and APF in terms of computational efficiency. Notice that the bounds for the KDPF become wider as $J$ increases, but they do not change much for $J > 10000$. This suggests that by 20000 particles, the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ has converged to the true posterior over the entire epidemic period, unlike with the BF and APF. Even though the bounds for $\beta$ with the BF and APF seem to roughly match those of the KDPF for $J = 20000$ and $J = 40000$ over the first half of the epidemic, the KDPF provides the same measure of uncertainty even for $J = 10000$ and does not degenerate in the second half of the epidemic.

Also noteworthy is how the bounds for $\nu$ expand between $t = 70$ and $t = 80$ for $J > 1000$. We typically prefer to see the width of the credible intervals decrease monotonically over time. It is possible for the variance of the filtered distribution to increase, but that likely is not the case here. A more plausible explanation is that the marginal filtered distribution of $\nu$ has been squeezed against the upper limit of the uniform prior distribution of $\nu$. If the distribution of $\nu$ was allowed to stray above 1.3, a similar plot would probably show a shift in the distribution toward lower values as opposed to the widening of the interval that we see in the figure (Figure \ref{fig:resamp}, which we discuss in Section \ref{sec:resample}, can attest to this). We provide further evidence of the sensitivity of the analysis to uniform priors in the next section.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-systematic-uniform-40000}
\caption{2.5\%/97.5\% quantiles of the filtered distributions $p(\beta|y_{1:t})$, $p(\gamma|y_{1:t})$, and $p(\nu|y_{1:t})$ (columns) over time for increasing number of particles (rows). Uniform priors and systematic resampling were used throughout. The axes are identical within each column. True parameter values are indicated by black horizontal lines.} \label{fig:pfs}
\end{figure}

\subsection{Comparison of uniform versus log-normal priors}

Next, we investigate the sensitivity of the KDPF with $J = 10000$ to using log-normal versus uniform priors on $\theta$.  Figure \ref{fig:priors} compares scatterplots of $\beta$ versus $\gamma$ sampled jointly from the filtered distribution $p(\beta,\gamma|y_{1:t})$ at $t = 15, 30, 45, 60$ for different prior distributions and transformations used on $\theta$. The first row illustrates that uniform priors with the logit transformation cause some particles to concentrate close to the uniform bounds for $\gamma$, especially noticeable for $t \in \{15, 30\}$.  This truncation of points indicates that the uniform bounds on $\gamma$ are too restrictive to account for the uncertainty in the recovery time during the climb of the epidemic, when more people are getting sick than recovering from illness. This problem does not present itself with $\beta$, however, because the uniform bounds for $\beta$ were chosen to be wide enough to capture the uncertainty about this parameter over the course of the epidemic.

We also see from the first row of Figure \ref{fig:priors} that the correlation between $\beta$ and $\gamma$ increases during the climb of the epidemic. In addition, points begin to form an S-shape at $t = 30$. This is a byproduct of the logit transformation extending values of $\gamma$ that are close to the boundary to the whole real line. To relax the restriction on $\gamma$ imposed by the uniform bounds, we ran the particle filter again using the same prior draws as in the first row of Figure \ref{fig:priors}, but without any transformation on $\theta$. Scatterplots of $\beta$ versus $\gamma$ for this run are shown in the second row of Figure \ref{fig:priors}, and we see now that particles at $t = 15$ and $t = 30$ that would have been squeezed against the boundary by the logit transformation now lie outside of the uniform bounds for $\gamma$. There is no longer an S-shape in any of the graphs, and also noticeable is that the sample correlation coefficient between $\beta$ and $\gamma$ at $t = 60$ is larger than when the logit transformation was applied to $\theta$. This suggests that the more drastic decrease in correlation between $t = 45$ and $t = 60$ in the first row is not driven by the data, but rather is an artifact of the restrictive uniform bounds enforced by the logit transformation.

Ideally, we'd like to apply some transformation to extend $\theta$ to the whole real line so that the KDPF operates smoothly, but we don't want the transformation to influence the particles in any way. The third row of Figure \ref{fig:priors} shows the KDPF run using log-normal priors with a log transformation on the components of $\theta$. As in the second row of Figure \ref{fig:priors}, the distribution of points do not appear truncated or S-shaped. While a majority of the points in the third row of Figure \ref{fig:priors} are within the uniform bounds, some lie outside because the transformation is not overly restrictive. We prefer log-normal priors on $\theta$ with log transformation because it allows us to use prior knowledge of the epidemic to encourage points to lie between the bounds at $t = 0$, but is more flexible in the event of model mis-specification and allows the data to play a greater role in dictating the evolution of $p(x_t,\theta|y_{1:t})$.

\begin{figure}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-uniform-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, logit transformation}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-semi-uniform-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, no transformation}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-lognormal-lognormal-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{normal prior draws, log transformation}
\end{minipage}

\caption{Scatterplots of $\beta$ (horizontal) versus $\gamma$ (vertical) at $t = 15, 30, 45, 60$ days using the KDPF with $J = 10000$ particles and systematic resampling. Each row corresponds to a specific prior and transformation applied to $\theta$ in the KDPF. For simplicity, in each panel only 500 particles sampled from the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ are shown. The sample correlation coefficient, $r$, displayed in the subtitle of each plot, is computed from the entire particle sample. Red crosses indicate the true values of $\beta$ and $\gamma$ used for simulation ($\beta = 0.2399$ and $\gamma = 0.1066$). Axes are the same in each panel. Dashed horizontal lines indicate the bounds of the uniform prior on $\gamma$ that was used in the top two rows. Uniform bounds on $\beta$ are not shown because they lie outside the range of the x-axis.} \label{fig:priors}

\end{figure}

\subsection{Comparison of resampling schemes \label{sec:resample}}

We turn to a comparison of different techniques for the resampling step of the KDPF with 10000 particles and log-normal priors on $\theta$.  Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy. The four resampling methods used in this paper achieve this at the cost of increasing the Monte Carlo variability of the particle sample. That is, additional variance is introduced into the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ due to resampling of particles. \citet{Douc:Capp:Moul:comp:2005} explains these four methods in detail and determines that:

\begin{enumerate}
\item Multinomial resampling introduces more Monte Carlo variability than does residual or stratified resampling.
\item Residual and stratified resampling introduce the same amount of Monte Carlo variability, on average.
\item Systematic resampling may introduce more or less Monte Carlo variability than does multinomial resampling.
\end{enumerate}

We would like to choose the resampling scheme for which the filtered distribution, $p(x_t,\theta|y_{1:t})$, approaches the true posterior the fastest with increasing number of particles. Figure \ref{fig:resamp} shows 95\% credible bounds of the marginal filtered distributions over time of the three unknown parameters (columns) as $J$ increases (rows) for each of the four different resampling schemes (lines). For our model, multinomial resampling appears to be outperformed by the other three resampling techniques, as the bounds for $\nu$ using 10000 or 20000 particles with multinomial resampling deviates from the true posterior (approximated by the white area in the plots) more than with the other three resampling techniques. This is also the case when looking at the plot for $\gamma$ at $J = 20000$. While systematic resampling seems to perform just as well as the remaining two resampling schemes, we prefer to use stratified resampling since, according to \cite{Douc:Capp:Moul:comp:2005}, we are then guaranteed to decrease the Monte Carlo variability in the particle sample, on average, relative to multinomial sampling.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-KD-normal-40000}
\caption{2.5\%/97.5\% quantiles of the filtered distributions over time of the unknown parameters (columns) for increasing number of particles (rows), color coded by resampling technique. Log-normal priors on $\theta$ and the KDPF particle filter were used for all runs.  Plot axes are identical within columns. Areas shaded gray are outside of 95\% credible bounds calculated by taking the average quantiles of $p(\beta|y_{1:t})$, $p(\gamma|y_{1:t})$, and $p(\nu|y_{1:t})$ among the four resampling techniques using 40000 particles (i.e. the quantiles shown in the last row of the figure).} \label{fig:resamp}
\end{figure}

\subsection{A note on when to resample}

Regardless of resampling technique, it is inefficient to resample at every iteration of the particle filter because particle weights may not be out of balance. In this case, resampling would unnecessarily add Monte Carlo variability to the particle sample at more stages than needed. As mentioned in Section \ref{sec:pf}, a measure of nonuniformity of particle weights called effective sample size was used to determine when to resample for all runs in our study \citep{Liu:Chen:Wong:reje:1998}. This value can be interpreted as the number of independent particle samples, ranging between 1 and $J$, with $J$ corresponding to all particle weights being equal and 1 corresponding to one particle weight being 1 with the rest 0. We use a threshold of 0.8 in our runs, meaning that if the number of independent samples is less than 80\% of the total number of particles, resampling is performed. Other measures of nonuniformity include coefficient of variation and entropy. Future work is needed to determine which choice of nonuniformity measure and threshold level are optimal in certain cases.

\section{Additional Unknown Parameters \label{sec:extend}}

We now turn our focus to extending the analysis to include $\{b_l,\varsigma_l,\sigma_l:l\in1,\ldots,L\}$ as unknown parameters. In addition, we estimate another set of parameters that we add to the model, $\eta_l$ for $l = 1,2,\ldots,L$. The $\eta_l$'s can be interpreted as setting the baseline for the observed syndromic data from each stream. We alter equation \eqref{eqn:obs} to read
\begin{equation}
\log y_{l,t} \sim N\left(b_li_t^{\varsigma_l} + \eta_l,\sigma_l^2\right) \label{eqn:extended-obs}
\end{equation}
\noindent The analysis presented in Section \ref{sec:results} corresponds to $\eta_l$ assumed known and set to 0 for all $l$.

For this section, we consider data coming from only $L = 1$ stream and let $\theta = (\beta, \gamma, \nu, b, \varsigma, \sigma, \eta)'$, dropping the subscript $l$. Using the same simulated epidemic, data were simulated from equation \eqref{eqn:extended-obs} with true values of $b$, $\varsigma$, $\sigma$, and $\eta$ set to $0.25$, $1$, $0.001$ and $2$, respectively. Days at which data were observed from the single stream were randomly selected.

The KDPF with tuning parameter $\Delta$ set to 0.99 was run with $J = 40000$ particles, and stratified resampling was used with an effective sample size threshold of 0.8. As before, fixed parameter values were regenerated only when resampling was performed. $x_0$ and $\theta$ were sampled from the prior density given by
\[ p\left(x_0,\theta\right) = p\left(\beta\right)p\left(\gamma\right)p\left(\nu\right)p\left(b\right)p\left(\varsigma\right)p\left(\sigma\right)p\left(\eta\right)p\left(i_0,s_0\right) \]
\noindent with $p(i_0,s_0)$ defined as in Section \ref{sec:pf}. We used the log-normal priors for $p(\beta)$, $p(\gamma)$, and $p(\nu)$ as in Section \ref{sec:pf}. $p(b)$, $p(\varsigma)$, $p(\sigma)$, and $p(\eta)$ are specified by $\log\left(b\right) \sim  N\left(-1.6090, 0.3536^2\right)$, $\log\left(\varsigma\right) \sim N\left(-0.0114, 0.0771^2\right)$, $\log\left(\sigma\right) \sim N\left(-7.0516, 0.2803^2\right)$, and $\eta \sim N\left(2.5, 1\right)$. Log-normal priors restrict $b$, $\varsigma$, and $\sigma$ to be nonnegative, while $\eta$ is allowed to be any real number. The choice of prior mean and standard deviation on the log scale were made such that random draws of $b$, $\varsigma$, and $\sigma$ on the original scale would be within $(0.1, 0.4)$, $(0.85, 1.15)$, and $(0.0005, 0.0015)$, respectively, with 95\% probability. For comparison purposes, the KDPF with 40000 particles was also run with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed to be known at their true values used for simulating the data (we refer to this run as the original analysis).

Figure \ref{fig:ext} shows 95\% credible intervals over time for the marginal filtered distributions of each of the elements of $\theta$ and $x_t$ for both the extended (blue lines) and the original (red lines) analyses. Most noticeable from Figure \ref{fig:ext} is that the intervals for $\beta$, $\gamma$, $\nu$, $s_t$, and $i_t$ are wider for the extended analysis than they are for the original. This is due to the added uncertainty in $b$, $\varsigma$, $\sigma$, and $\eta$ in the extended analysis. Nonetheless, we are still able to obtain credible intervals for the unknown parameters that cover the true values and intervals for the states that cover the true epidemic curves by increasing the number of particles used in the KDPF.

%Nonetheless, we are still able to adequately estimate the epidemic curves even with less data available and imprecise knowledge of the parameters $b$, $\varsigma$, $\sigma$, and $\eta$ by increasing the number of particles used in the KDPF. In addition, we get a very precise estimate of $\eta$, the parameter that controls the baseline value of the syndromic observations.

We also notice from this figure that the lines appear choppier than in earlier plots, with flat periods followed by sharp changes in uncertainty. This is due to the fact that data are coming from only one stream, leading to more time points where no data are available and making the analysis more sensitive to abnormal data. Gaps in the data lead to a lack of resampling of particles and cause more drastic shifts in the posterior distribution of $\theta$ once data arrive. For example, a sharp shift in the distribution of $\beta$ occurs around $t = 25$ because of an influx of data following a period of no data and hence scarce resampling of particles. Also, we notice a spike in the $s$ and $i$ curves right before $t = 40$ because of a shift in the trajectory of data points.

Lastly, we comment on a widening of the credible intervals for $\nu$ in the extended analysis. This phenomenon suggests that the log-normal prior on $\nu$ that samples particles in $[0.9,1.3]$ is too tight, and that our model provides even less insight about this parameter than our prior belief. Scarce knowledge about $\nu$ is gained over the course of the epidemic in the original analysis due to the nonlinear nature of the evolution equation with respect to $\nu$, and we learn even less about $\nu$ in the extended analysis.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-ext-KD-stratified-normal-40000}
\caption{2.5\%/97.5\% quantiles of the marginal filtered distributions of the unknown parameters and states from the extended analysis (blue lines) over time compared with that from the original analysis (red lines). The KDPF with $J = 40000$ particles was run with stratified resampling. Tick marks are shown along the bottom of the plots for $\beta$ at time points when data were observed (dark gray) and when particles were resampled (blue and red for the extended and original analyses, respectively).} \label{fig:ext}
\end{figure}

\section{Discussion \label{sec:discussion}}

\input{discussion}

\clearpage

\bibliographystyle{model1-num-names}
\bibliography{jarad}

\end{document} 