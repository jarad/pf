\documentclass{elsarticle}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}

\graphicspath{{/Users/niemi/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/Users/niemi/Dropbox/SIR_Particle_Filtering/doesnotexist/}
                       {/Users/Danny/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/data/home/faculty/meiring/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {./Graphs/}
                       % Add your directories here
}

\newcommand{\jarad}[1]{{\color{red}JARAD: #1}}
\newcommand{\danny}[1]{{\color{blue}DANNY: #1}}

\begin{document}

\noindent {\bf Abstract} \\

This paper compares the performance of different particle filtering algorithms (sequential Monte Carlo algorithms) when applied to a stochastic compartmental model of a disease outbreak. As a starting point, we consider a similar model to \citet{skvortsov2012monitoring} which is nonlinear in both the states and the parameters in both the observation and evolution equations, with a multivariate response at each time point. Missing observations are allowed. We review some developments in the particle filtering methodology to account for simultaneous estimation of outbreak intensity and fixed parameters governing the outbreak. Our results indicate that a version of the particle filter developed by \citet{Liu:West:comb:2001} that uses a normal density kernel to resample values of the fixed parameters outperforms the bootstrap filter and the auxiliary particle filter in terms of efficiency and avoiding particle degeneracy. We also demonstrate that using seemingly non-informative uniform priors on the fixed parameters can actually influence the filtered distribution of the states and fixed parameters over time. In addition, we show that using multinomial resampling of particles adds extra Monte Carlo variability to the weighted sample approximation of the posterior, as evidenced by wider credible intervals compared with other methods.  Lastly, we add additional uncertainty to the model and perform estimation using the \citet{Liu:West:comb:2001} particle filter with lognormal priors on the fixed parameters and stratified resampling. \\

\noindent {\bf Keywords} \\

Bayesian estimation; epidemiological modeling; state-space modeling; sequential Monte Carlo; particle filtering. \\

\noindent {\bf Abbreviations} \\

BF - bootstrap filter; APF - auxiliary particle filter; KDPF - kernel density particle filter

\section{Introduction} \label{sec:intro}

% Introduce Bayesian framework, MCMC
In statistical applications where prior knowledge/beliefs about unknown quantities is available, the Bayesian framework is often convenient for performing statistical analysis.  In this case, inference is conducted through the posterior distribution of the unknown quantities given the observed data. However, the calculation of the posterior distribution can involve complicated integrals which frequently do not have an explicit analytical form. Advances in computing have helped alleviate this problem through the use of simulation-based methods such as Markov-chain Monte Carlo (MCMC) \citep{Gelf:Smit:samp:1990}, a technique to generate samples from the posterior distribution through simulation from known statistical densities.

% Introduce sequential Monte Carlo
A drawback of MCMC is that when new data become available, the entire analysis needs to be performed again. That is, samples from the previous analysis cannot be reused and new samples need to be generated, a process that often times requires significant time and computing power. This is computationally inefficient and impractical in fields where data are collected sequentially and need to be analyzed in real time such as tracking an aircraft using radar or monitoring the stock market. Sequential Monte Carlo - or particle filtering - methods enable on-line inference by updating the posterior sample as new data become available. Furthermore, these methods are flexible, general, easy to implement, and amenable to computing in parallel. For a general introduction, please see \cite{Douc:deFr:Gord:sequ:2001} and \cite{cappe2007overview}.

% Introduce bootstrap filter, degeneracy problem
The first and most basic particle filter - named the bootstrap filter - was developed by \citet{Gord:Salm:Smit:nove:1993} and \citet{Kita:mont:1996}. The bootstrap filter uses an iterative algorithm that samples values (i.e. particles) of the unknown quantities from a prior distribution, propagates the particles forward in time using an evolution equation, calculates weights for each particle using a likelihood function based on the data observed up to that time, and resamples the particles based on these weights. Issues arise with this algorithm mainly in the resampling step, where particles with large weights emit more copies of themselves into the next iteration of the filter, while particles with small weights get eliminated. This can lead to \emph{particle attrition}, or a degeneracy of particles toward a single value. The problem is exacerbated further when one or more of the unknown quantities are held fixed over time, since, in this case, new values are not generated during propagation.

% Introduce APF, KDPF, more sophisiticated approaches
One way to fight particle degeneracy is by simply increasing the number of particles. A more efficient way, however, was developed by \citet{Pitt:Shep:filt:1999} called the auxiliary particle filter. The auxiliary particle filter attempts to avoid degeneracy by looking ahead at the next data point before the resampling step and then samples particles in areas of higher likelihood. An algorithm developed by \citet{Liu:West:comb:2001} builds on the auxiliary particle filter and distinguishes fixed parameters from unobserved states that evolve over time. Their version of the particle filter avoids degeneracy by resampling values from a normal kernel density approximation of the posterior distribution of the fixed parameters. Further improvements to the efficiency of the algorithm include using a measure of nonuniformity of particle weights to determine when to resample \citep{Douc:Capp:Moul:comp:2005}, incorporating an MCMC step to refresh values of the fixed parameters \citep{Gilk:Berz:foll:2001}, and taking advantage of a sufficient statistic structure in the model \citep{Fear:mark:2002}.

% Introduce epidemiological modeling, Skvortsov Math. Bio. paper
Epidemiogical modeling is an area where particle filtering can play an important role. Models of disease outbreaks can be broken into two parts: a dynamic model of how the epidemic evolves and a measurement model of syndromic data. Thus, it is easy to conceptualize an epidemic from a Bayesian viewpoint as a filtering problem using a state-space model. Since many mechanistic models of disease transmission are nonlinear and develop in real time, sequential Monte Carlo is a natural fit. For example, \citet{skvortsov2012monitoring} analyzed a simulated epidemic in the form of a stochastic compartmental epidemiological model, using the bootstrap filter to estimate the state of the epidemic and unknown fixed paremeters. In this paper, we show estimation efficiency is improved when incorporating more recent developments in the particle filtering methodology.

% Introduce layout of paper (see main document)
The rest of the article proceeds as follows. Section \ref{sec:model} contains a description of state-space models and sequential estimation. Section \ref{sec:filtering} describes a variety of particle filtering methodologies including the kernel density approach of \cite{Liu:West:comb:2001}. In Section \ref{sec:apply}, we introduce a nonlinear dynamic model for a disease epidemic similar to the one used in \citet{skvortsov2012monitoring}. In section \ref{sec:results}, we apply the particle filtering methods described in Section \ref{sec:filtering} to the model described in Section \ref{sec:apply}. In Section \ref{sec:extend}, we benefit from the efficiency of these particle filtering methods to estimate a more complicated model. In Section \ref{sec:discussion}, we conclude with some additional particle filtering methods that may be helpful in certain model structures.

\section{State-space models \label{sec:ss}}

State-space models are a general class of statistical models used for analysis of dynamic data and have been used extensively in modeling disease outbreaks \citep{Mart:Cone:Lope:Lope:baye:2008,watkins2009disease,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring}.  State space models are constructed using an observation equation, $y_t \sim p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \sim p_{x,t}(x_t|x_{t-1},\theta)$ where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, and $\theta$ is the unknown fixed parameter, all of which could be vectors. The distributions are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only vary with $x_t$ and $\theta$ and therefore the $t$ subscript is dropped.
%The particle filtering methodology discussed in Sections \ref{sec:filtering} and \ref{sec:discussion} apply equally well when the distributions depend on $t$ so long as they are known.
For simplicity, we will also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the state-space model is
\begin{align*}
y_t &\sim p(y_t|x_t,\theta) \\
x_t &\sim p(x_t|x_{t-1},\theta).
\end{align*}

Special cases of these state-space models include hidden Markov models \citep{cappe2005inference}, where the state $x_t$ has finite support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997}, where each distribution is Gaussian with mean a linear function of the states.

\subsection{Sequential estimation \label {sec:sequential}}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, the distribution of the current state and parameters conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters. It can be updated recursively using Bayes' rule:
\begin{equation}
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$. Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically, e.g. in DLMs when $\theta$ is the observation variance \cite[Sec 4.3,][]{petris2009dynamic}. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. extended Kalman filter and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions such as particle filters.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is a sequential Monte Carlo inferential technique based on sequential use of importance sampling. It aims to approximate equation \eqref{eqn:filtered} through a weighted Monte Carlo realization from this distribution, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j}^J w_t^{(j)} \delta_{(x_t^{(j)},\theta^{(j)})} \label{eqn:approx}
\end{equation}
where $w_t^{(j)},j=1,\ldots,J$ are the particle \emph{weights}, $(x_t^{(j)},\theta^{(j)})$ is the particle \emph{location}, and $\delta$ is a Dirac delta function. A variety of techniques have been developed to provide efficient approximations to equation \eqref{eqn:filtered} in the sense that with the same computation time a better approximation is achieved. We now introduce three fundamental particle filtering techniques: bootstrap filter, auxiliary particle filter, and the kernel density particle filter.

\subsection{Bootstrap filter \label{sec:bf}}

The first successful version of particle filtering is known as the bootstrap filter (BF) \citep{Gord:Salm:Smit:nove:1993}. Since this method and the auxiliary particle filter were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Suppose you have a current approximation as in equation \eqref{eqn:approx}. To move from an approximation of $p(x_t,\theta| y_{1:t})$ to that of $p(x_{t+1},\theta| y_{1:t+1})$, we perform the following steps for each particle $j$:

\begin{enumerate}
\item Resample: sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{w_t^{(1)},\ldots,w_t^{(j)},\ldots,w_t^{(J)}\}$ and set
    \[ x_t^{(j)} = x_t^{(k)} \qquad w_t^{(j)} = 1 / J, \]
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(\left. x_{t+1}\right|x_t^{(j)}\right)$, and
\item Calculate weights: $\tilde{w}_{t+1}^{(j)} = w_t^{(j)}p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)$.
\end{enumerate}

\noindent After weights for all particles have been calculated, these weights need to be renormalized via $w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right.$.

This procedure can be applied recursively provided an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$. The algorithm as described above uses a multinomial resampling scheme, i.e. when we say `sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ \ldots'. This is not the only way to resample the particles. In addition, we can optionally forgo the `Resample' step (hence the necessity of the $w_t^{(j)}$ term in the `Calculate weights' step) to make the algorithm more efficient. More on resampling will be explained later.

\subsection{Auxiliary particle filter \label{sec:apf}}

A key problem that arises in implementing the BF is that $w_t^{(j)}$ may be small for samples $x_t^{(j)}$ that are less likely given $y_t$. These samples will likely be eliminated during resampling, resulting in \emph{particle degeneracy}. One way to fight particle degeneracy is to increase the number of particles, $J$, used in the filter. A more efficient method, however, has been developed by \citet{Pitt:Shep:filt:1999} called the auxiliary particle filter (APF). The idea behind the APF is to generate particles with higher likelihood given the data by "looking ahead" at $p(x_{t+1}|x_t^{(j)})$ prior to sampling. Given a weighted random sample of particles at time $t$, the APF approximates $p(x_{t+1}|y_{1:t+1})$ by the following:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$.  %This is typically taken to be the mean of $p(x_{t+1}|x_t^{(j)})$.
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p(y_{t+1}|\mu_{t+1}^{(j)}) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^l.\right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$ and set
         \[ x_t^{(j)} = x_t^{(k)} \qquad w_t^{(j)} = 1 / J \qquad \mu_{t+1}^{(j)} = \mu_{t+1}^{(k)}, \]
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(\left. x_{t+1}\right|x_t^{(j)}\right)$, and
	\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = w_t^{(j)}\frac{p\left(y_{t+1}|x_{t+1}^{(j)}\right)}{p\left(y_{t+1}|\mu_{t+1}^{(j)}\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent As with the BF, the `Resample' step here is optional and we may choose to skip this step if particle weights are not out of balance.

The BF and the APF were constructed with the idea that all fixed parameters were known. In order to simultaneously estimate the states and fixed parameters using these methods it is necessary to incorporate the fixed parameters into the state with degenerate evolutions. Due to the resampling step, the number of unique values of the fixed parameters in the particle set will decrease over time resulting in \emph{degeneracy} \citep{Liu:West:comb:2001}.

\subsection{Kernel density particle filter \label{sec:kd}}

The particle filter introduced by \cite{Liu:West:comb:2001} is a general way of fighting this degeneracy problem by approximating the set of fixed parameter values by a kernel density estimate and then resampling from this approximation at each step in the filter. This enables us to refresh the distribution of $\theta$ to avoid degeneracy and require a lower number of particles to efficiently run the filter. We will refer to this filter as the kernel density particle filter (KDPF).

We now reintroduce the fixed parameter $\theta$ into the notation. The KDPF provides an approximation to the filtered distribution via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ in particle $j$. This does not imply that $\theta$ is dynamic, but rather that the particle can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of the posterior sample $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The KDPF also uses a tuning parameter $\Delta$, the discount factor, and two derived quantities $h^2 = 1 - ((3\Delta - 1)/2\Delta)^2$ and $a^2 = 1 - h^2$ that determine how sharp the kernel density approximation is. Typically $\Delta$ is taken to be between 0.95 and 0.99 \citep{Liu:West:comb:2001}.

With the KDPF, we move to an approximation of $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $\left(x_{t+1}^{(j)},\theta\right)$ given by $\left(\mu_{t+1}^{(j)},m_t^{(j)}\right)$ where
    \[
    \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right) \qquad
    m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t.
    \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^l. \right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k$ from the set $\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$ and set \[x_t^{(j)} = x_t^{(k)} \qquad w_t^{(j)} = 1 / J \qquad \left(\mu_{t+1}^{(j)},m_t^{(j)}\right) = \left(\mu_{t+1}^{(k)},m_t^{(k)}\right),\]
	\item Regenerate the fixed parameters:
	\[ \theta_{t+1}^{(j)} \sim N\left( m_t^{(j)}, h^2V_t \right), \]
	\item Propagate:
	\[ x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(j)},\theta_{t+1}^{(j)}\right.\right), \mbox{ and} \]
	\item Calculate weights and renormalize:
	\[ \tilde{w}_{t+1}^{(j)} = w_t^{(j)}\frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent The `Resample' step is again optional. Furthermore, we may also choose not to regenerate the fixed parameters and instead set $\theta_{t+1}^{(j)} = \theta_t^{(j)}$. This makes sense from an efficiency standpoint, since if we do not resample particles, then there is no elimination of unique fixed parameter values.

To use the KDPF with normal kernels it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$ and then the logit transformation can be applied. We discuss the sensitivity of the performance of the KDPF to using this kind of transformation later on.

\subsection{General advice \label{sec:advice}}

Practical implementation of any particle filter involves a choice of resampling function and a decision about when resampling should be performed. Throughout our discussion we have implicitly used multinomial resampling. Alternatively, better resampling functions exist including stratified, residual, and systematic resampling. In addition, the frequency of resampling should be minimized to reduce Monte Carlo variability introduced during resampling. Typically a measure of the nonuniformity of particle weights is used including effective sample size, entropy, and coefficient of variation. For a discussion of these topics please see \cite{Douc:Capp:Moul:comp:2005}.

\section{Particle filtering in epidemiological models \label{sec:apply}}

We now describe how epidemiological modeling fits into the framework of state-space models with sequential estimation described in Section \ref{sec:ss} to set the stage for application of the particle filtering methodology described in Section \ref{sec:filtering}. As in \citet{skvortsov2012monitoring}, we consider a modified SIR model with stochastic fluctuations to describe the dynamics of the disease outbreak \citep{herwaarden1995stochepid, dangerfield2009stochepid, anderson2004sars}. According to this model, we keep track of the proportion of a population susceptible to ($s$), infected by ($i$), and recovered from ($r$) disease. $s$, $i$, and $r$ are all nonnegative, and we require $s + i + r = 1$.

When monitoring an epidemic, the true $s$, $i$ and $r$ are unknown and regarded as hidden states of the model. The data that we observe are gathered from syndromic surveillance, or the systematic collection and monitoring of public health data by public health agencies \citep{wagner2006biosurveillance, wilson2006synsurveillance}. These measurements can consist of medical observations such as emergency room visits as well as non-medical data such as absenteeism from work and queries from search engines or social media \citep{chew2010twitter, schuster2010searchquery, signorini2011twitter, Gins:Mohe:Pate:Bram:Smol:Bril:dete:2009}. Thus, we fuse information from syndromic surveillance with the disease transmission dynamics through a state-space model. In our state-space model of a disease outbreak, the observation equation specifies how the observed data depend on the state of the epidemic and other parameters, and the state equation describes how the epidemic evolves over time.

\subsection{Model \label{sec:model}}

First, we describe the state equation. Let $s_t$ and $i_t$ be the proportion of the population susceptible to disease and infected by disease at time $t$, respectively, and let $x_t = (i_t,s_t)'$. Our compartmental model of disease transmission is governed by three parameters:

\begin{itemize}
\item $\beta$, the contact rate of spread of disease,
\item $\gamma$, the recovery time from infection, and
\item $\nu$, the mixing intensity of the population.
\end{itemize}

\noindent $\beta$, $\gamma$, and $\nu$ are each restricted to be nonnegative. Define $\theta = (\beta,\gamma,\nu)'$ to be the vector of unknown parameters in our model. Then, we follow \citet{skvortsov2012monitoring} to describe the evolution of the epidemic from time $t$ to $t + \tau$ (for $\tau > 0$) by
\begin{equation}
x_{t+\tau}\left|x_t,\theta\right. \sim N_\Omega\left(f_\tau(x_t,\theta),Q_{\tau}(\theta)\right) \label{eqn:state}
\end{equation}
\noindent where
\[
f_\tau(x_t,\theta) = \left(
\begin{array}{c}
i_t +  \tau\beta i_ts^\nu_t - \tau\gamma i_t \\
s_t - \tau\beta i_ts^{\nu}_t
\end{array}
\right)
\qquad
Q_\tau(\theta) = \left(
\begin{array}{ccccc}
(\beta + \gamma)\tau^2/P^2 & -\beta\tau^2/P^2 \\
-\beta\tau^2/P^2 & \beta\tau^2/P^2
\end{array}
\right)
\]

\noindent and $\Omega = \{(i_t,s_t): i_t \ge 0, s_t \ge 0, s_t + i_t \le 1\}$. Note that, by definition, $r$ is completely specified once $s$ and $i$ are known, and hence is not needed in the state vector. $N_{\Omega}(\mu,\Sigma)$ represents the normal distribution with mean $\mu$ and covariance matrix $\Sigma$ truncated onto the set $\Omega$. Notice that in equation \eqref{eqn:state}, the joint distribution of $s$ and $i$ has been truncated since they are proportions that must have sum no greater than 1. To generate values from this density during the `Propagate' step of the particle filter, we sample from the untruncated normal distribution, reject any samples outside of $\Omega$, and resample when needed.

%The dynamics of the epidemic with respect to time, $t$, are then described by the following differential equations:
%
%\begin{align}
%\frac{ds}{dt} &= -\beta is^\nu + \epsilon_\beta \label{eqn:dsdt} \\
%\frac{di}{dt} &= \beta is^\nu - \gamma i - \epsilon_\beta + \epsilon_\gamma \label{eqn:didt}
%\end{align}

%The standard deviations of the noise terms are approximated by
%\[\sigma_\beta \approx \frac{\sqrt{\beta}}{P} \mbox{, } \sigma_\gamma \approx \frac{\sqrt{\gamma}}{P}\]
%This comes from a well-known scaling rule of random fluctuations in the contact rate and recovery time \citep{ovaskainen2010extinction, herwaarden1995stochepid, dangerfield2009stochepid}.

Next, we describe the observation equation. We think of the observed data as positive real numbers related to counts of emergency room visits, prescription sales, or calls to a hotline, for example, and we can observe data from these different streams/sources asynchronously in time. That is, at any time $t$, we can observe data from any subset of the streams (or possibly none of them). Let $y_{l,t}$ represent data coming from stream $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$. Following \citet{skvortsov2012monitoring}, we model the the log of the observations (so that $y_{l,t}$ is restricted to be positive) by
\begin{equation}
\log y_{l,t} \sim N\left(b_li_t^{\varsigma_l},\sigma_l^2\right) \label{eqn:obs}
\end{equation}
where $b_l$, $\varsigma_l$, and $\sigma_l$ are nonnegative constants. For the moment, we assume $b_l$, $\varsigma_l$, and $\sigma_l$ are known $\forall l$. In an extended analysis described in Section \ref{sec:extend}, we regard these constants as unknown and include them in $\theta$.

Having formulated the data-generating model, we define $y_t = (y_{1,t},y_{2,t},\ldots,y_{L,t})'$ and express the likelihood of an observation $y_t$ given $x_t$ and $\theta$ by
\[ y_t\left|x_t,\theta\right. \sim \ln N\left(\mu_t,\Sigma_t\right) \]
\noindent where $\mu_t$ is an $L$-length vector with element $l$ equal to $b_li_t^{\varsigma_l}$ and $\Sigma_t$ is an $L \times L$ diagonal matrix with the $l^{\mbox{th}}$ diagonal equal to $\sigma_l^2$. Elements of $y_t$ may be missing, in which case the dimension of $p(z_t|x_t,\theta)$ shrinks by the number of missing elements. %If the $j^{\mbox{th}}$ element of $y_t$ is missing, then the $j^{\mbox{th}}$ element of $\mu_t$ and $j^{\mbox{th}}$ row and column of $\Sigma_t$ are also missing and thus omitted in the calculation of the likelihood.
If all elements of $y_t$ are empty (i.e. if no syndromic data are observed at time $t$), particle weights remain the same as they were at the previous time step.

%A further note on propagating particles is necessary.  Notice from equation \eqref{eqn:state} that the transition density needed to propagate $x_t$ in our model is a multivariate normal density with mean $f_\tau(x_t,\theta)$, and recall that the mean function approximates the disease dynamics better for small $\tau$.  Thus, for data observed at times $t_1$ and $t_2$ where the time between observations, $t_2 - t_1$, is large, the mean of $p(x_{t_2}|x_{t_1}^{(j)})$ for each particle $j$ may be inaccurate if $\tau$ is set to $t_2 - t_1$.  To ensure that the transition density matches the actual disease dynamics more closely, we can choose $d$ to be an integer larger than 1, set $\tau = (t_2 - t_1) / d$, and propagate $x_{t_1}^{(j)}$ forward $d$ times to generate $x_{t_2}^{(j)}$.

Since $p(x_{t+\tau}|x_t,\theta)$ and $p(y_t|x_t,\theta)$ are nonlinear functions of $x_t$ and $\theta$, a closed-form solution to the posterior $p(x_t,\theta|y_{1:t})$ cannot be obtained. Thus, we use particle filtering techniques described in Section \ref{sec:filtering} to approximate $p(x_t,\theta|y_{1:t})$ for all $t$.

\section{Results} \label{sec:results}

We now compare the performance of the BF, APF and KDPF using simulated data.  An epidemic lasting $T = 125$ days was simulated from our model for a population of size $P = 5000$. True values of unknown parameters were set to $\beta = 0.2399$, $\gamma = 0.1066$, and $\nu = 1.2042$, and values of known constants for $L = 4$ streams are given in Table \ref{tab:constants}. Infection was introduced in 10 people in the population at day 0 (i.e. true $i_0 = 10/5000$ and $s_0 = 4990/5000$). The epidemic evolved over time according to equation \eqref{eqn:state} and data from randomly selected streams at each day were generated from equation \eqref{eqn:obs}. The simulated epidemic peaks around $t = 50$ days, affecting roughly 17\% of the population (see Figure \ref{fig:data}).

\begin{table}[ht]
\begin{center}
\caption{Values of known constants in model.}
\label{tab:constants}
\begin{tabular}{|cccc|}
\hline
$l$ & $b_l$ & $\varsigma_l$ & $\sigma_l$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\centering
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-epid}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-z}
\end{minipage}
\caption{Displays simulated epidemic curves (left) and syndromic observations (right).} \label{fig:data}
\end{figure}

\subsection{Particle filter runs} \label{sec:pf}

The BF, APF, and KDPF were each run on the simulated data using $J = 100, 1000, 10000, 20000 \mbox{ and } 40000$ particles to obtain weighted sample approximations of $p(x_t,\theta|y_{1:t})$ for $t = 1,\ldots,T$. For each $J$, separate runs using multinomial, residual, stratified, and systematic resampling were implemented and an effective sample size threshold set at 80\% of the total number of particles was used to determine when to resample particles \citep{Liu:Chen:Wong:reje:1998}. For the KDPF, the discount factor $\Delta$ was set to 0.99 to determine the variance of the normal kernels used to approximate $p(\theta|y_{1:t})$.

$x_0$ and $\theta_0$ were sampled from prior density $p(x_0,\theta) = p(\theta)p(i_0,s_0)$ where $p(i_0,s_0)$ is the joint pdf of the random variables $i_0$ and $s_0$ and $p(\theta) = p(\beta)p(\gamma)p(\nu)$. Since a very small percentage of the population is infected during the initial stage of the epidemic, we let $p(i_0) = N_{[0,1]}(i_0;0.002,0.0005^2)$. We also set $s_0 = 1 - i_0$ and sample $i_0$ and $s_0$ jointly since, at $t = 0$, no infected individuals have recovered from illness yet.

To investigate the impact of different prior distributions of $\theta$ on performance of the particle filters, the runs described above were performed once using uniform priors on $\theta$ and then again using log-normal priors. Uniform priors on $\theta$ were chosen to be the same as those used in \citet{skvortsov2012monitoring}, given by
\[ p\left(\beta\right) \sim U\left[0.14, 0.50\right] \qquad p\left(\gamma\right) \sim U\left[0.09, 0.143\right] \qquad p\left(\nu\right) \sim U\left[0.95,1.3\right]. \]
\noindent Log-normal priors on $\theta$ are given by
\[ \beta \sim \ln N\left(-1.3296, 0.3248^2\right) \qquad \gamma \sim \ln N\left(-2.1764, 0.1183^2\right) \qquad \nu \sim \ln N\left(0.1055, 0.0800^2\right). \]
\noindent These log-normal priors constrain $\beta$, $\gamma$ and $\nu$ to be positive. The mean and variance of the log of the unknown parameters were chosen so that random draws on the original scale would fall within the uniform bounds with 95\% probability.

Logit and log transformations were applied to the components of $\theta$ in the manner described at the end of Section \ref{sec:kd} so that the normal kernel could be used in the KDPF while constraining $\beta$, $\gamma$, and $\nu$ to be within their respective prior domains (i.e. logit was used with uniform priors and log with log-normal priors). To aid comparison of the particle filters, the same prior draws were used in the BF, APF, and KDPF as long as the number of particles and prior were the same.

\subsection{Comparison of particle filter algorithms} \label{sec:pfcomparison}

First, we compare the performance of the particle filtering algorithms using uniform priors on $\theta$ and systematic resampling, since these priors and resampling scheme were used in \citet{skvortsov2012monitoring}. Figure \ref{fig:pfs} shows 95\% credible bounds of the marginal filtered distributions of the unknown parameters over time as $J$ increases. The bounds for the BF (red lines) and APF (blue lines) start out wide and then degenerate toward a single value because of the elimination of unique particles during resampling. Although the time of degeneracy increases as $J$ gets larger, the BF and APF bounds become misleading during the second half of the epidemic even for $J = 40000$. The bounds for the KDPF (green lines), on the other hand, never degenerate because new values of $\theta$ are sampled from the normal kernel density approximation of $p(\theta|y_{1:t})$ at each iteration of the particle filter.

The KDPF also has an advantage over the BF and APF in terms of computational efficiency. Notice that the bounds for the KDPF become wider as $J$ increases, but they do not change much for $J > 10000$. This suggests that by 20000 particles the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ has converged to the true posterior over the entire epidemic period, unlike with the BF and APF. Even though the bounds for $\beta$ with the BF and APF seem to roughly match those of the KDPF for $J = 20000$ and $J = 40000$, the KDPF provides the same measure of uncertainty for $J = 10000$.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-systematic-uniform-40000}
\caption{2.5\%/97.5\% quantiles of the filtered distributions over time of the unknown parameters (columns) for increasing number of particles (rows). Uniform priors and systematic resampling were used throughout. The axes are identical within each column. True parameter values are indicated by black horizontal lines.} \label{fig:pfs}
\end{figure}

\subsection{Comparison of priors}

Next, we investigate the sensitivity of the KDPF with $J = 10000$ to using log-normal versus uniform priors on $\theta$.  Figure \ref{fig:priors} compares scatterplots of $\beta$ versus $\gamma$ taken from their joint filtered distributions at $t = 15, 30, 45, 60$ for different prior distributions and transformations used on $\theta$. The first row illustrates that uniform priors with the logit transformation cause some particles for $\gamma$ to concentrate close to the uniform bounds, especially noticeable for $t \in \{15, 30\}$.  This truncation of points indicates that the uniform bounds on $\gamma$ are too restrictive to account for the uncertainty in the recovery time during the climb of the epidemic. This problem does not present itself with $\beta$ because more people are getting sick than recovering from illness during the first half of the epidemic. Thus, we learn more about the rate of spread of disease than the recovery time and the range of $\beta$ values moves well within the uniform bounds by $t = 15$.

We also see from the first row of Figure \ref{fig:priors} that $\beta$ and $\gamma$ become correlated during the climb of the epidemic. In addition, points begin to form an S-shape at $t = 30$. This is a product of the logit transformation extending values of $\gamma$ close to the boundary to the whole real line. To relax the restriction on $\gamma$ imposed by the uniform bounds, we ran the particle filter again using the same prior draws as in the first row of Figure \ref{fig:priors}, but without any transformation on $\theta$. Scatterplots of $\beta$ versus $\gamma$ for this run are shown in the second row of Figure \ref{fig:priors}, and we see now that particles at $t = 15$ and $t = 30$ that would have been squeezed against the boundary by the logit transformation now lay outside of the uniform bounds for $\gamma$. There is no longer an S-shape in any of the graphs, and also noticeable is that the sample correlation coefficient between $\beta$ and $\gamma$ at $t = 60$ is larger than when the logit transformation was applied to $\theta$. This suggests that the more drastic decrease in correlation between $t = 45$ and $t = 60$ in the first row is not driven by the data, but rather is an artifact of the restrictive uniform bounds enforced by the logit transformation.

Ideally, we'd like to apply some transformation to extend $\theta$ to the whole real line so that the KDPF operates smoothly, but we don't want the transformation to influence the data in any way. The third row of Figure \ref{fig:priors} shows the KDPF run using log-normal priors with a log transformation on the components of $\theta$. As in the second row of Figure \ref{fig:priors}, the distribution of points do not appear truncated or S-shaped. While a majority of the points in the third row of Figure \ref{fig:priors} are within the uniform bounds, some lay outside because the transformation is not overly restrictive. We prefer this choice of prior distribution and transformation because it allows us to use prior knowledge of the epidemic to encourage points to lie between the bounds, but is flexible in the event of model mis-specification and allows the data to dictate the evolution of $p(x_t,\theta|y_{1:t})$.

\begin{figure}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-uniform-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, logit transformation}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-semi-uniform-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, no transformation}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-lognormal-lognormal-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{normal prior draws, log transformation}
\end{minipage}

\caption{Displays scatterplots of $\beta$ (horizontal) versus $\gamma$ (vertical) at $t = 15, 30, 45, 60$ days using the KDPF with $J = 10000$ particles and systematic resampling. Each row corresponds to a specific prior and transformation applied to $\theta$ in the KDPF. For simplicity, in each panel only 500 particles sampled from the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ are shown. The sample correlation coefficient, $r$, displayed in the subtitle of each plot, is computed from the original particle sample. Red crosses indicate the true values of $\beta$ and $\gamma$ used for simulation ($\beta = 0.2399$ and $\gamma = 0.1066$). Axes are the same in each panel. Dashed horizontal lines indicate the bounds of the uniform prior on $\gamma$. Uniform bounds on $\beta$ are not shown because they lay outside the range of the x-axis.} \label{fig:priors}

\end{figure}

\subsection{Comparison of resampling schemes}

We turn to a comparison of different techniques for the resampling step of the KDPF with 10000 particles and log-normal priors.  Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy. The four resampling methods used in this paper achieve this at the cost of increasing the Monte Carlo variability of the particle sample. That is, additional variance is introduced into the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ due to resampling of particles. \citet{Douc:Capp:Moul:comp:2005} explains these four methods in detail and determines that:

\begin{enumerate}
\item Multinomial resampling introduces more Monte Carlo variability than does residual or stratified resampling.
\item Residual and stratified resampling introduce the same amount of Monte Carlo variability on average.
\item Systematic resampling may introduce more or less Monte Carlo variability than does multinomial resampling.
\end{enumerate}

We would like to choose the resampling scheme for which the filtered distribution approaches the true posterior the fastest with increasing number of particles. Figure \ref{fig:resamp} shows 95\% credible bounds of the marginal filtered distributions of the unknown parameters as $J$ increases for the four different resampling schemes. For our model and data set, multinomial appears to be outperformed by the other three resampling techniques, as the bounds for $\nu$ using 10000 or 20000 particles and multinomial resampling seem to deviate from the true posterior (approximated by the white area in the plots) more than with the other three resampling techniques. This also appears to be true when looking at the plot for $\gamma$ at $J = 20000$. While systematic resampling seems to perform just as well as the remaining two resampling schemes, we prefer to use stratified resampling since, according to \cite{Douc:Capp:Moul:comp:2005}, we are then guaranteed to decrease the Monte Carlo variability in the particle sample, on average, relative to multinomial sampling.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-KD-normal-40000}
\caption{2.5\%/97.5\% quantiles of the filtered distributions over time of the unknown parameters (columns) for increasing number of particles (rows), color coded by resampling technique. Log-normal priors on $\theta$ and the KDPF particle filter were used for all runs.  Plot axes are identical within columns. Areas shaded gray are outside of 95\% credible bounds calculated by taking the average quantiles of the marginal filtered distributions of $\theta$ from the four resampling techniques using the KDPF with log-normal priors on $\theta$ and 40000 particles.} \label{fig:resamp}
\end{figure}

\subsection{A note on when to resample}

Regardless of resampling technique, it is inefficient to resample at every single iteration of the particle filter because particle weights may not be out of balance, and so resampling would be unnecessarily adding Monte Carlo variability to the particle sample. As mentioned in Section \ref{sec:pf}, a measure of nonuniformity of particle weights called effective sample size was used to determine when to resample for all runs in our study \citep{Liu:Chen:Wong:reje:1998}. This value can be interpreted as the number of independent particle samples and ranges between 1 and $J$, with $J$ corresponding to all particle weights being equal and 1 corresponding to one particle weight being 1 with the rest 0. We use a threshold of 0.8 in our runs, meaning that if the number of independent samples is less than 80\% of the total number of particles, resampling is performed. Other measures of nonuniformity include coefficient of variation and entropy, and future work is needed to determine which choice of nonuniformity measure and threshold level are optimal in certain cases.

\section{Additional Unknown Parameters \label{sec:extend}}

We now turn our focus to extending the analysis to include the $b_l$'s, $\varsigma_l$'s, and $\sigma_l$'s as unknown parameters. In addition, we add another set of parameters to the model, $\eta_l$ for $l = 1,2,\ldots,L$, that sets the baseline for the observed syndromic data from each stream. That is, we alter the observation equation to read
\begin{equation}
\log y_{l,t} \sim N\left(b_li_t^{\varsigma_l} + \eta_l,\sigma_l^2\right) \label{eqn:extended-obs}
\end{equation}
\noindent We can think of the analysis presented in Section \ref{sec:results} as being performed on this model with $\eta_l$ set to 0 $\forall l$.

Now, we consider data coming from just 1 stream and let $\theta = (\beta, \gamma, \nu, b, \varsigma, \sigma, \eta)'$, dropping the subscript $l$ $b_l$, $\varsigma_l$, $\sigma_l$, and $\eta_l$ for convenience. Using the same simulated epidemic, data were simulated from equation \eqref{eqn:extended-obs} with true values of $b$, $\varsigma$, $\sigma$, and $\eta$ set to $0.25$, $1$, $0.001$ and $2$, respectively. Days at which data were observed from the single stream were randomly selected.

The KDPF with tuning parameter $\delta$ set to 0.99 was run with $J = 20000$ particles, and stratified resampling was used with an effective sample size threshold of 0.8. $x_0$ and $\theta$ were sampled from the prior density given by
\[ p\left(x_0,\theta\right) = p\left(\beta\right)p\left(\gamma\right)p\left(\nu\right)p\left(b\right)p\left(\varsigma\right)p\left(\sigma\right)p\left(\eta\right)p\left(i_0,s_0\right) \]
\noindent where $p(i_0,s_0)$ is defined the same way as in Section \ref{sec:pf}, and the log-normal priors given for $p(\beta)$, $p(\gamma)$, and $p(\nu)$ in Section \ref{sec:pf} are used again as well. $p(b)$, $p(\varsigma)$, and $p(\sigma)$ are defined by

\begin{align*}
\log\left(b\right) &\sim  N\left(-1.6090, 0.3536^2\right) \\
\log\left(\varsigma\right) &\sim N\left(-0.0114, 0.0771^2\right) \\
\log\left(\sigma\right) &\sim N\left(-7.0516, 0.2803^2\right) \\
\eta &\sim N\left(2.5, 1\right)
\end{align*}

Log-normal priors restrict $b$, $\varsigma$, and $\sigma$ to be nonnegative, while $\eta$ is allowed to take values along the whole real line. The choice of prior mean and standard deviation on the log scale were made such that random draws of $b$, $\varsigma$, and $\sigma$ on the original scale would be within $(0.1, 0.4)$, $(0.85, 1.15)$, and $(0.0005, 0.0015)$, respectively, with 95\% probability. For comparison purposes, the KDPF with 20000 particles was also run with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed to be known at their true values used for simulating the data (referred to as the original analysis).

Figure \ref{fig:ext} shows 95\% credible intervals over time for marginal filtered distributions of the elements of $\theta$ and $x$ for both the extended (blue lines) and the original (red lines) analyses. We notice from this figure that the lines appear choppier than in earlier plots, with flat periods followed by sharp changes in uncertainty. This is due to the fact that data are coming from only one stream, leading to more time points where no data are available and making the analysis more sensitive to abnormal data. Gaps in the data lead to a lack of resampling of particles and causes more drastic shifts in the posterior distribution of $\theta$ once data arrive. For example, a sharp shift in the distribution of $b\beta$ occurs around $t = 25$ because of an influx of data following a period of no data and scarce resampling of particles. Also, we notice a spike in the $s$ and $i$ curves right before $t = 40$ because of a shift in the trajectory of data points.

Also noticeable from Figure \ref{fig:ext} is that the intervals for $\beta$, $\gamma$, $\nu$, $s$, and $i$ are wider for the extended analysis than they are for the original. This is expected due to the added uncertainty in $b$, $\varsigma$, and $\sigma$ in the extended model. Nonetheless, we are still able to adequately estimate the epidemic curves even with less data available and imprecise knowledge of the parameters $b$, $\varsigma$, $\sigma$, and $\eta$ by increasing the number of particles used in the KDPF.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-ext-KD-stratified-normal-20000}
\caption{2.5\%/97.5\% quantiles of the marginal filtered distributions of the unknown states and parameters from the extended analysis (blue lines) over time compared with that from the original analysis (red lines). The KDPF with $J = 20000$ particles was run with stratified resampling. Tick marks are shown along the bottom of the plots for $\beta$ at time points when data were observed (dark gray) and when particles were resampled (blue and red for the extended and original analyses, respectively).} \label{fig:ext}
\end{figure}

\section{Discussion \label{sec:discussion}}

\input{discussion}

\clearpage

\bibliographystyle{model1-num-names}
\bibliography{jarad}

\end{document} 