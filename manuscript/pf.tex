\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}

\graphicspath{{/Users/niemi/Dropbox/SIR_Particle_Filtering/Graphs/PF/}
                       {/Users/niemi/Dropbox/SIR_Particle_Filtering/doesnotexist/}
                       % Add your directories here     
}

\newcommand{\jarad}[1]{{\color{red}JARAD: #1}}
\newcommand{\danny}[1]{{\color{blue}DANNY: #1}}

\begin{document}

\section{Introduction}

The rest of the article proceeds as follows. Section \ref{sec:model} contains a description of state-space models and the nonlinear dynamic model used in \citet{skvortsov2012monitoring}. Section \ref{sec:filtering} describes a variety of particle filtering methodologies including the kernel density approach of \cite{Liu:West:comb:2001}. In Section \ref{sec:apply}, we apply these methods to the \citet{skvortsov2012monitoring} model. In Section \ref{sec:extend}, we use the efficiency of these particle filtering methods to estimate a more complicated model. In Section \ref{sec:discussion}, we conclude with some additional particle filtering methods that may be helpful in certain model structures.

\section{State-space models \label{sec:model}}

State-space models are a general class of statistical models used for analysis of dynamic data in many fields including \jarad{references to uses of state-space models}. State space models are constructed using an observation equation, $y_t \stackrel{ind}{\sim} p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \stackrel{ind}{\sim} p_{x,t}(x_t|x_{t-1},\theta)$ where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, and $\theta$ is the unknown fixed parameter, all of which could be vectors. The distributions are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only varying with $x_t$ and $\theta$ and therefore the $t$ subscript is dropped. 
%The particle filtering methodology discussed in Sections \ref{sec:filtering} and \ref{sec:discussion} apply equally well when the distributions depend on $t$ so long as they are known. 
For simplicity, we will also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the state-space model is 
\begin{align*}
y_t &\stackrel{ind}{\sim} p(y_t|x_t,\theta) \\
x_t &\stackrel{ind}{\sim} p(x_t|x_{t-1},\theta).
\end{align*}

Special cases of these state-space models include hidden Markov models \jarad{HMM reference}, where the state $x_t$ has finite support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997}, where the distributions are both Gaussian whose means are linear functions of the states. 

\subsection{Sequential estimation}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, the distribution of the current state and parameter conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters It can be updated recursively using Bayes' rule:
\begin{equation} 
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$. Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically, e.g. in DLMs when $\theta$ is the observation variance \cite[Sec 4.3,][]{petris2009dynamic}. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. extended Kalman filter \jarad{ref} and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions, i.e. particle filters. 




\section{Particle filtering \label{sec:filtering}}

Particle filtering is a sequential Monte Carlo inferential technique based on sequential use of importance sampling. It aims to approximate equation \eqref{eqn:filtered} through a weighted Monte Carlo realization from this distribution, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j}^J w_t^{(j)} \delta_{x_t^{(j)},\theta^{(j)}}
\end{equation}
where $\sum_{j=}^J w_t^{(j)}$ are the particle \emph{weights} $\delta_{x_t,\theta}$, $(x_t,\theta)$ is the particle \emph{location}, and $\delta$ is a Dirac delta function. A variety of techniques have been developed to provide efficient approximations to equation \eqref{eqn:filtered} in the sense that with the same computation time a better approximation is achieved. We now introduce three fundamental particle filtering techniques: bootstrap filter, auxiliary particle filter, and the kernel density filter.

\subsection{Bootstrap Filter}

The first successful version of particle filtering is known as the bootstrap filter \citep{Gord:Salm:Smit:nove:1993}. Since this method and the auxiliary particle filter were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Suppose you have a current approximation as in equation \eqref{eqn:particle}, then for each particle $j$,

\begin{enumerate}
\item Resample: sample an index $k\in \{1,\ldots,j,\ldots,J\}$ with probability $w_t^{(j)}$,
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(\left. x_{t+1}^{(k)}\right|y_{1:t}\right)$, and
\item Calculate weights: $\tilde{w}_{t+1}^{(j)} = p\left(y_{t}\left|x_{t+1}^{(j)}\right.\right)$.
\end{enumerate}

\noindent After weights for all particles have been calculated, these weights need to be renormalized via $w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right.$

This procedure provides an approximation to $p(x_t,\theta| y_{1:t})$ and therefore can be applied recursively provided an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$.

\danny{start here}

Notice from equation \eqref{eqn:state} that the transition density needed for prediction in our model is a multivariate normal density with mean $f_\tau(x_t,\theta)$ and covariance matrix $Q$.  A few notes on how to carry out the prediction step for this density are necessary. First, recall that that the mean function approximates the disease dynamics better for small $\tau$.  Thus, if the time between observations, $t_{t+1} - t_k$, is large, we can still set $\tau$ to be small and carry out the prediction step $(t_{t+1} - t_k) / \tau$ times to generate $x_{t_{t+1}}^{(j)}$.  Second, $i$ and $s$ are proportions and thus constrained to be in $[0,1]$.  However, the stochastic nature of the state equation makes it possible to generate states that do not satisfy this constraint.  In our implementation of the particle filter, we throw away samples in which $i$ or $s$ fall outside of $[0,1]$ and resample.

So how do we reintroduce unknown parameters $\theta$ into the particle filtering framework?  The approach taken by \citet{skvortsov2012monitoring} is to extend the state vector, $x_t$, to include the unknown the parameters and constrain them to remain constant over time in the state equation.  That is, let $x_t = (i_t,s_t,\beta,\gamma,\nu)'$ and redefine $f_\tau$ and $Q$ as
\[
f_\tau(x_t) = \left(
\begin{array}{c}
i_t + \tau i_t(\beta s^{\nu}_t - \gamma) \\
s_t - \tau\beta i_ts^{\nu}_t \\
\beta \\
\gamma \\
\nu
\end{array}
\right)
\mbox{, }
Q = \left(
\begin{array}{ccccc}
(\beta + \gamma)\tau^2/P^2 & 0 & 0 & 0 & 0 \\
0 & \beta\tau^2/P^2 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}
\right)
 \]

\noindent The main shortcoming of implementing the bootstrap filter in this manner, however, is that particles generated that are less likely given the data will have small weights and get eliminated in the resampling step.  This leads to what is known as particle degeneracy, or a degeneracy of the distribution of the unknown parameters toward one value.  This poses a problem in the context of Bayesian inference where we would like the posterior distribution to incorporate proper uncertainty about $\theta$.

\subsection{Auxiliary Particle Filter}

One way to fight particle degeneracy is to increase the number of particles, $n$, used in the filter.  A more efficient method, however, has been developed by \citet{Pitt:Shep:filt:1999} called the auxiliary particle filter.  The idea behind the auxiliary particle filter is to generate particles with higher likelihood given the data by "looking ahead" at the predictive distribution of each particle prior to sampling.  Given a weighted random sample of particles at time $t_k$, the auxiliary particle filter approximates $p(x_{t_{t+1}}|y_{1:k+1})$ by the following:

\begin{enumerate}
\item Estimate - For each particle $i$, calculate a point estimate of $x_{t_{t+1}}^{(j)}$ called $\mu_{t+1}^{(j)}$.  This is typically taken to be the mean of $p(x_{t_{t+1}}|x_t^{(j)})$.
\item Auxiliary weights - Calculate auxiliary weights $\tilde{g}_{t+1}^{(j)}$ for each particle by $\tilde{g}_{t+1}^{(j)} = w_k^{(j)} p(y_{t+1}|\mu_{t+1}^{(j)})$ and normalize the auxiliary weights by \[g_{t+1}^{(j)} = \frac{\tilde{g}_{t+1}^{(j)}}{\sum_{l=1}^n \tilde{g}_{t+1}^l}\]
\item Resample - Sample with replacement from the particles according to the normalized auxiliary weights $g_{t+1}^{(j)}$.
\item Prediction - For each particle $i = {1,\ldots,n}$, sample a new particle $x_{t_{t+1}}^{(j)}$ from the transition density $p(x_{t_{t+1}}|x_t^{(j)}$).
\item Update - Calculate new weights of the particles by \[\tilde{w}_{t+1}^{(j)} = \frac{p(y_{t+1}|x_{t_{t+1}}^{(j)})}{p(y_{t+1}|\mu_{t+1}^{(j)})}\]
\item Normalize - Normalize the weights of the particles so they sum to 1: \[w_{t+1}^{(j)} = \frac{\tilde{w}_{t+1}^{(j)}}{\sum_{l=1}^n \tilde{w}_{t+1}^l}\]
\end{enumerate}

\noindent Like the bootstrap filter, the auxiliary particle filter is initiated by taking $n$ samples from the prior distribution $p(x_0)$ and setting the initial weights $w_0^{(j)} = 1/n$.  Optionally, one can alternate between the bootstrap filter and the auxiliary particle filter as new observations are processed.  For instance, in cases where the distribution of the weights is adequate, we can avoid the estimation step of calculating auxiliary weights so that the algorithm runs more efficiently.

\subsection{Kernel Density Particle Filter}

Both the bootstrap filter and the auxiliary particle filter as described in the previous two sections do not consider additional unknown parameters $\theta$, but rather require any additional parameters to be concatenated to the state vector $x_t$.  Thus, the values sampled from the prior distribution at time $t_0$ are carried through the entire particle filter.  This makes degeneracy of the unknown parameters difficult to avoid unless a large number of particles is used.  \citet{Liu:West:comb:2001} devised a modification to the auxiliary particle filter that takes into account additional unknown parameters $\theta$ in a flexible way.  They introduce a kernel density estimate of the distribution of $\theta$ for each particle so that new values of the unknown parameters can be sampled at each time step.  This enables us to refresh the distribution of $\theta$ to avoid degeneracy and require a lower number of particles for the filter.

Define $x_t = (i_t,s_t)'$ and $\theta = (\beta,\gamma,\nu)'$ and let $f_\tau$ and $Q$ be as they were in equations \eqref{eqn:f} and \eqref{eqn:q}.  Let $\{x_t^1,\ldots,x_t^n\}$ and $\{\theta_k^1,\ldots,\theta_k^n\}$ with associated weights $\{w_k^1,\ldots,w_k^n\}$ be an approximation to the distribution $p(x_t,\theta|y_{1:k})$.  Note that the subscript $k$ for $\theta$ does not imply that $\theta$ changes with time, but rather indicates current knowledge of the distribution of $\theta$ at time $t_k$.  In addition, let $\bar{\theta}_k$ and $V_k$ be the weighted sample mean and weighted sample covariance matrix of the posterior sample ${\theta_k^1,\ldots,\theta_k^n}$.  According to the kernel density particle filter, we move to an approximation of $p(x_{t_{t+1}},\theta|y_{1:{k+1}})$ by the following steps:

\begin{enumerate}
\item Estimate - For each particle $i$, calculate a point estimate of $(x_{t_{t+1}}^{(j)},\theta)$ given by $(\mu_{t+1}^{(j)},m_k^{(j)})$ where
    \begin{align*}
    \mu_{t+1}^{(j)} &= E(x_{t_{t+1}}|x_t^{(j)},\theta_k^{(j)}) \\
    m_k^{(j)} &= a\theta_k^{(j)} + (1-a)\bar{\theta}_k
    \end{align*}
\item Auxiliary weights - Calculate auxiliary weights $\tilde{g}_{t+1}^{(j)}$ for each particle by $\tilde{g}_{t+1}^{(j)} = w_k^{(j)} p(y_{t+1}|\mu_{t_{t+1}}^{(j)},m_k^{(j)})$ and normalize the auxiliary weights by \[g_{t+1}^{(j)} = \frac{\tilde{g}_{t+1}^{(j)}}{\sum_{l=1}^n \tilde{g}_{t+1}^l}\]
\item Resample - Sample with replacement from the particles according to the normalized auxiliary weights $g_{t+1}^{(j)}$.
\item Prediction - For each particle $i = {1,\ldots,n}$, sample $\theta_{t+1}^{(j)}$ from a normal density with mean $m_k^{(j)}$ and covariance matrix $h^2V_k$.  Then, sample $x_{t_{t+1}}^{(j)}$ from the transition density $p(x_{t_{t+1}}|x_t^{(j)},\theta_{t+1}^{(j)}$).
\item Update - Calculate new weights of the particles by \[\tilde{w}_{t+1}^{(j)} = \frac{p(y_{t+1}|x_{t_{t+1}}^{(j)},\theta_{t+1}^{(j)})}{p(y_{t+1}|\mu_{t_{t+1}}^{(j)},m_k^{(j)})}\]
\item Normalize - Normalize the weights of the particles so they sum to 1: \[w_{t+1}^{(j)} = \frac{\tilde{w}_{t+1}^{(j)}}{\sum_{l=1}^n \tilde{w}_{t+1}^l}\]
\end{enumerate}

Notice above that the mean and variance of the kernel density estimate of $\theta_k^{(j)}$ depend also on scalars $a$ and $h$.  $a$ specifies the weight given to each particle versus the overall mean in calculating the mean of the kernel density, and $h$ is the bandwidth of the kernel. We have the relationship $a^2 = 1 - h^2$ and let $h^2 = 1 - ((3\delta - 1)/2\delta)^2$.  $\delta$ called the discount factor and is usually taken to be between 0.95 and 0.99.


\section{Particle filtering in epidemiological models \label{sec:apply}}

\subsection{Disease Dynamics}

As in \citet{skvortsov2012monitoring}, we consider the modified SIR model with stochastic fluctuations to describe the dynamics of the disease outbreak.  According to this model, we keep track of the number of susceptible, infected, and recovered individuals - denoted by $S$, $I$, and $R$, respectively - over the course of the epidemic.  We require $S + I + R = P$ where $P$ represents the total population size, and we define $s = S/P$, $i = I/P$, and $r = R/P$ so that $s + i + r = 1$.  The dynamics of the epidemic with respect to time, $t$, are then described by the following differential equations:

\begin{align}
\frac{ds}{dt} &= -\beta is^\nu + \epsilon_\beta \label{eqn:dsdt} \\
\frac{di}{dt} &= \beta is^\nu - \gamma i - \epsilon_\beta + \epsilon_\gamma \label{eqn:didt}
\end{align}

\noindent Here, $\beta$ represents the contact rate of spread of the disease, $\gamma$ is the recovery time, and $\nu \ge 0$ is the mixing intensity of the population.  $\epsilon_\beta$ and $\epsilon_\gamma$ are uncorrelated, Gaussian random variables with zero mean and standard deviations $\sigma_\beta$ and $\sigma_\gamma$, respectively.  Note that, by definition, $r$ is completely specified once $s$ and $i$ are known.

As in \citet{skvortsov2012monitoring} once again, we will approximate the standard deviation of the noise terms $\sigma_\beta$ and $\sigma_\gamma$ by the scaling rule \[\sigma_\beta \approx \frac{\sqrt{\beta}}{P} \mbox{, } \sigma_\gamma \approx \frac{\sqrt{\gamma}}{P}\] and use Euler's method to approximate the solution of the differential equations given in \eqref{eqn:dsdt} and \eqref{eqn:didt}.  Letting $s_t$ and $i_t$ be the proportion of the population susceptible to disease and infected by disease at time $t$, respectively, the evolution of the epidemic from time $t$ to time $t + \tau$, $\tau > 0$, is approximated by

\begin{align}
s_{t+\tau} &= s_t - \tau\beta i_ts^\nu_t + \xi_s \label{eqn:s} \\
i_{t+\tau} &= i_t + \tau i_t(\beta s^\nu_t - \gamma) + \xi_i \label{eqn:i}
\end{align}

\noindent where $\xi_s$ and $\xi_i$ are independent, Gaussian random variables with zero mean and variances $(\beta + \gamma)\tau^2/P^2$ and $\beta\tau^2/P^2$, respectively.  Note that this approximation is more accurate for small $\tau$.

\subsection{Model}

When monitoring the epidemic, the true $s$, $i$ and $r$ are unknown and regarded as hidden states of the model.  We instead observe measurements consisting of public health data, medical data such as hospital visits, and non-medical data such as absenteeism from work or school.  Thus, we characterize a state-space model of the epidemic by two equations: the observation equation and the state equation.  The observation equation specifies how the observed data depend on the state of the epidemic and other parameters.  We can observe data from multiple syndromes that arrive asynchronously in time.  Let $z_{j,k}$ represent data coming from syndrome $j$ at discrete-time index $k$ (i.e. at time $t_k$), with $k = 1,2,\ldots,T$ and $j = 1,2,\ldots,J$.  Since the $z_{j,k}$'s are constrained to be positive, let $y_{j,k} = \log z_{j,k}$ and we assume a power law relationship between the log of the observations and the proportion of the population infected by disease:

\begin{equation}
y_{j,k} = b_ji^{\varsigma_j}_t + \tau_j \label{eqn:obs}
\end{equation}

\noindent Here, the $b_j$'s and $\varsigma_j$'s are non-negative constants and $\tau_j$ is a normally distributed random variable with zero mean and standard deviation $\sigma_j$.

Next, we specify the state equation.  The state equation explains how the unobserved states of the epidemic evolve over time.  Let $x_t = (i_t,s_t)'$ represent the state of the epidemic at time $t$, and let $\theta$ represent any unknown, fixed parameters in the model.  For the time being, we let $\theta = (\beta, \gamma, \nu)'$ and regard the $b_j$'s, $\varsigma_j$'s, and $\sigma_j$'s as known.  Letting $\tau > 0$ be small, the state equation is given by

\begin{equation}
x_{t+\tau} = f_\tau(x_t,\theta) + w_{\tau}(\theta) \label{eqn:state}
\end{equation}

\noindent where

\begin{equation}
f_\tau(x_t,\theta) = \left(
\begin{array}{c}
i_t + \tau i_t(\beta s^{\nu}_t - \gamma) \\
s_t - \tau\beta i_ts^{\nu}_t
\end{array}
\right) \label{eqn:f}
\end{equation}

\noindent and $w_\tau$ is a Gaussian random variable with zero mean and covariance matrix

\begin{equation}
Q = \left(
\begin{array}{ccccc}
(\beta + \gamma)\tau^2/P^2 & 0 \\
0 & \beta\tau^2/P^2
\end{array}
\right) \label{eqn:q}
\end{equation}

\noindent Having formulated the observation and state equations, we can now express the likelihood of an observation $y_{j,k}$ given the state of the epidemic at time $t_k$, $x_t$, and the unknown parameters $\theta$ by \[p(y_{j,k}|x_t,\theta) = N(y_{j,k};b_ji^{\varsigma_j}_t,\sigma^2_j)\] where $N(.;\mu,\sigma^2)$ represents the pdf of the normal distribution with mean $\mu$ and variance $\sigma^2$.

We are interested now in how to update the posterior distribution of the states and unknown parameters given new data.  That is, given the posterior distribution at time $t_k$, denoted by $p(x_t,\theta|y_{1:k})$, how do we obtain $p(x_{t_{t+1}},\theta|y_{1:k+1})$?  Theoretically, this is obtained by Bayes rule:

\begin{equation}
p(x_{t_{t+1}},\theta|y_{1:k+1}) = \frac{p(y_{j,k}|x_t,\theta)p(x_t,\theta|y_{1:k})}{\int p(y_{j,k}|x_t,\theta)p(x_t,\theta|y_{1:k}) \mathrm{d} x_t} \label{eqn:bayes}
\end{equation}

\noindent However, because the densities in the numerator are nonlinear functions of $x_t$ and $\theta$, a closed-form solution to equation \eqref{eqn:bayes} cannot be obtained.  Instead, we use a numerical simulation approach called sequential monte carlo or particle filtering, discussed in the next section.


\subsection{Comparison of Particle Filters}

An epidemic was simulated from our model for $T = 154$ days for a population of size $P = 5000$.  True values of unknown parameters were set to $\beta = 0.2399$, $\gamma = 0.1066$, and $\nu = 1.2042$.  Values of known constants for $J = 4$ syndromes are given in table \ref{tab:true}.  Infection was introduced in 10 people in the population at day 0 (i.e. $i_0 = 10/5000$ and $s_0 = 4990/5000$).  Observations arrived daily from the different syndromes asynchronously, and the epidemic evolved over time according to equation \eqref{eqn:state} with $\tau = 0.2$ days.

\begin{table}[ht]
\begin{center}
\caption{Values of known constants in model.} 
\label{tab:true}
\begin{tabular}{|cccc|}
\hline
$j$ & $b_j$ & $\varsigma_j$ & $\sigma_j$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\end{center}
\end{table}

The bootstrap filter (BF), auxilliary particle filter (APF), and kernel density particle filter (KD) were each run using $n = 1000$ particles to obtain weighted samples from $p(x_t,\theta|y_{1:k})$ for $k = 1,\ldots,T$.  Recall that for BF and APF, $\theta$ is included in $x_t$.  In any case, $x_{t_0}$ and $\theta_{0}$ were sampled from the prior density given by

\[p(x_{t_0},\theta) = p(i_{t_0})p(s_{t_0})p(\beta)p(\gamma)p(\nu)\] where
\begin{align*}
p(i_{t_0}) &= N_{[0,1]}(i_{t_0};y_{j,1}/b_j,\sigma^2_j) \\
p(s_{t_0}) &= N_{[0,1]}(s_{t_0};1 - y_{j,1}/b_j,\sigma^2_j) \\
p(\beta) &= U_{[0.14,0.5]}(\beta) \\
p(\gamma) &= U_{[0.09,.143]}(\gamma) \\
p(\nu) &= U_{[0.95,1.3]}(\nu)
\end{align*}

\noindent Here, $N_{[a,b]}(.;\mu,\sigma^2)$ is the pdf of a normal distribution with mean $\mu$ and variance $\sigma^2$ truncated onto the interval $[a,b]$, and $U_{[a,b]}(.)$ is the pdf of a uniform distribution on the interval $[a,b]$.

Figure \ref{fig:quant} shows the mean and 95\% credible intervals of the proportion of infected individuals in the population over time using the quantiles of the weighted particle sample at each time point, and figure \ref{fig:hist} shows histograms of the unknown parameters after running the particle filters for $k = 25, 154$ days.  Notice in figure \ref{fig:quant} how the bootstrap and auxiliary particle filters degenerate early on in the epidemic because of particle deficiency, while the kernel density particle filter keeps a balanced distribution centered around the true proportion of the population infected.  In addition, it is possible to learn about the distribution of the unknown parameters over the course of the epidemic as is indicated in figure \ref{fig:hist}.  At day 25, the marginal posterior distribution of the contact rate parameter $\beta$ appears normal centered around the true value, and the marginal posterior distributions of $\gamma$ and $\nu$ appear more or less uniform, indicating that a lack of knowledge about these parameters within the first 25 days of the epidemic.  By the end of the epidemic, however, the distributions of all three parameters have shifted left.

It is possible to forecast the epidemic curve after observing measurements up until time $t_k$ by propagating particles through the state equation \eqref{eqn:state} and setting $w_\tau = 0$.  This enables us to predict the timing and intensity of the peak of the epidemic early on.  Figure \ref{fig:pred} generates 95\% credible intervals of the predictive distribution $p(i_{t_m}|y_{1:k})$ for $m > k$ after running the kernel density particle filter for $k = 10, 15, \ldots, 35$ days.  Notice that the credible bounds all envelope the true proportion of infected individuals, and the timing of the peak of the epidemic appears to be consistent.

\begin{figure}[ht]
\centering
\includegraphics[height=4in]{PF-quant-1000}
\caption{Displays particle filter results for the proportion of the population infected by the epidemic using $n = 1000$ particles.  The solid black line shows the true proportion of infected in the population from simulation, and colored lines show the weighted mean of the proportion of infected after running all 3 particle filters.  The 95\% credible bounds represent the 2.5\% and 97.5\% quantiles of the proportion of infected individuals in the weighted particle sample.} \label{fig:quant}
\end{figure}

\begin{figure}[ht]
\centering
\begin{minipage}[b]{.4\linewidth}
\caption*{$k = 25$ Days}
\includegraphics[width=1.0\textwidth]{Hist-KD-1000-day25}
\end{minipage}
\begin{minipage}[b]{.4\linewidth}
\caption*{$k = 154$ Days}
\includegraphics[width=1.0\textwidth]{Hist-KD-1000-day154}
\end{minipage}
\caption{Displays histograms of the unknown parameter values after running the kernel density particle filter using $n = 1000$ particles for $k = 25$ (left) and $k = 154$ (right) days.  True parameter values used for simulation are shown in the subtitles of the graphs.} \label{fig:hist}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[height=5in]{EpidPred-KD-1000}
\caption{Displays 95\% credible bounds on the predicted epidemic curve after running the kernel density particle filter with $n = 1000$ particles for $k = 10,15,20,25,30,35$ days.} 
\label{fig:pred}
\end{figure}

\section{Extending the model \label{sec:extend}}

\section{Discussion \label{sec:discussion}}

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document}