\documentclass{elsarticle}

\usepackage{amsmath}
\usepackage{natbib}

\graphicspath{{/Users/niemi/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/Users/niemi/Dropbox/SIR_Particle_Filtering/doesnotexist/}
                       {/Users/Danny/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/data/home/faculty/meiring/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {./Graphs/}
}

\journal{Mathematical Biosciences}

\begin{document}

\begin{frontmatter}

\title{Comparison of the performance of particle filter algorithms applied to tracking of a disease epidemic}

\author[danny]{Daniel M. Sheinson}
\author[jarad]{Jarad Niemi}
\author[wendy]{Wendy Meiring}

\address[danny]{Corresponding author -- Department of Statistics and Applied Probability, University of California--Santa Barbara, \\ Santa Barbara, CA 93106, U.S.A., sheinson@pstat.ucsb.edu, 1-847-609-7824}
\address[jarad]{Department of Statistics, Iowa State University, Ames, IA 50011, U.S.A., niemi@iastate.edu}
\address[wendy]{Department of Statistics and Applied Probability, University of California--Santa Barbara, \\ Santa Barbara, CA 93106, U.S.A., meiring@pstat.ucsb.edu}

\begin{abstract}
We present methodology for tracking and prediction of a disease outbreak via a syndromic surveillance system. A nonlinear stochastic state-space model with unknown fixed parameters defines the outbreak's infectivity and recovery rate. We show that basic particle filters may fail due to degeneracy of these fixed parameters and suggest the use of a kernel density approximation to the filtered distribution of the fixed parameters to allow their regeneration. In addition, we show that seemingly uninformative uniform priors on fixed parameters can affect posterior inferences and suggest the use of priors bounded only by the support of the parameter. We show the negative impact of using multinomial resampling and suggest the use of either stratified or residual resampling within the particle filter. Finally, we use this improved particle filtering methodology to relax prior assumptions on model parameters yet still provide reasonable estimates for model parameters and disease states.
\end{abstract}

\begin{keyword}
Bayesian estimation; epidemiological modeling; state-space modeling; sequential Monte Carlo; particle filtering.
\end{keyword}

\end{frontmatter}

\let\thefootnote\relax\footnotetext{Abbreviations: BF - bootstrap filter; APF - auxiliary particle filter; KDPF - kernel density particle filter}

\section{Introduction} \label{sec:intro}

Syndromic surveillance systems \citep{henning2004overview, wagner2006biosurveillance, wilson2006synsurveillance, hakenewerth2009north, Gins:Mohe:Pate:Bram:Smol:Bril:dete:2009} are commonly used throughout the world to identify emerging disease outbreaks \citep{neill2006bayesian}, estimate their severity \citep{merl2009statistical}, and predict their duration \cite{ludkovski2010optimal}. Examples of these surveillance systems are emergency department visits for influenza-like illness, over-the-counter sales of cold and flu medicines, or poison control center calls due to gastrointestinal symptoms. A single disease outbreak may result in additional observations in some or all of these surveillance systems.

State-space models are frequently used to simultaneously model the underlying disease dynamics and the observation process \citep{Mart:Cone:Lope:Lope:baye:2008,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring,unkel2012statistical}. Although the general form of these models may be reasonably assumed, the models will have unknown fixed parameters that define the disease dynamics and the observation process for a particular outbreak. Based on similar previous outbreaks, the range and likely values for these fixed parameters may be available.

In statistical applications where prior knowledge or beliefs about unknown quantities are available, the Bayesian framework is often convenient for performing statistical analysis.  Bayesian inference is conducted through the posterior distribution of any unknown quantities, obtained by updating prior information using observed data. However, the calculation of the posterior distribution frequently involves complicated integrals without an explicit analytical form. The most common approach to approximating these posterior distributions is Markov chain Monte Carlo (MCMC) \citep{Gelf:Smit:samp:1990}. In a sequential context, e.g. syndromic surveillance, MCMC is inefficient due to the increase in computational cost incurred by the need for the entire MCMC to be rerun as each new observation arrives. Sequential Monte Carlo (SMC) - or particle filtering - methods enable on-line inference by updating the estimate of the posterior as new data become available. Furthermore, SMC methods can be flexible, general, easy to implement, and amenable to parallel computing. For a general introduction, please see \cite{Douc:deFr:Gord:sequ:2001} and \cite{cappe2007overview}.

Early SMC methods, including the bootstrap filter \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996} and the auxiliary particle filter \citep{Pitt:Shep:filt:1999}, assumed all fixed parameters were known. A key defining step in these filters is the use of resampling, which results in particles with low probability being eliminated and particles with high probability being duplicated. When all fixed parameters are known, these filters work remarkably well. In the presence of unknown fixed parameters, these filters suffer dramatically from a \emph{degeneracy} issue due to the fixed parameters (being treated as dynamic states with degenerate evolutions) never being regenerated, and thus only a few distinct values for the fixed parameters remain after a few time points. To combat this degeneracy, a number of alternative approaches have been introduced. In this paper, we focus on the \emph{kernel density particle filter} \citep{Liu:West:comb:2001} due to its wide applicability, ease of implementation, and good performance.

The rest of the article proceeds as follows. Section \ref{sec:ss} contains a description of state-space models and sequential estimation. Section \ref{sec:filtering} describes a variety of particle filtering methodologies including the kernel density approach of \cite{Liu:West:comb:2001}. In Section \ref{sec:apply}, we introduce a nonlinear dynamic model for a disease epidemic similar to the one used in \citet{skvortsov2012monitoring}. In Section \ref{sec:results}, we apply the particle filtering methods described in Section \ref{sec:filtering} to the model described in Section \ref{sec:apply}. In Section \ref{sec:extend}, we benefit from the efficiency of more recent particle filtering methods to estimate a more complicated model. In Section \ref{sec:discussion}, we conclude by reviewing more advanced particle filtering methods.

\section{State-space models \label{sec:ss}}

State-space models are a general class of statistical models used for analysis of dynamic data and have been used extensively in modeling disease outbreaks \citep{Mart:Cone:Lope:Lope:baye:2008,watkins2009disease,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring}.  State space models are constructed using an observation equation, $y_t \sim p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \sim p_{x,t}(x_t|x_{t-1},\theta)$, where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, and $\theta$ is an unknown fixed parameter, all of which could be vectors. The $y_t$'s are assumed independent given $x_t$ and $\theta$, and the $x_t$'s are assumed independent given $x_{t-1}$ and $\theta$. The distributions $p_{y,t}$ and $p_{x,t}$ are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only vary with $x_t$ and $\theta$, and therefore the $t$ subscript is dropped. For simplicity, we also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the general state-space model is
\[
y_t \sim p(y_t|x_t,\theta) \qquad
x_t \sim p(x_t|x_{t-1},\theta).
\]
A fully specified Bayesian model is obtained by also specifying the prior $p(x_0,\theta)$.

The dimension of $x_t$ need not remain constant with respect to $t$. For instance, we could describe a process where $x_t$ depends on the entire history of states up to $t$ by letting $x_{t-1} = (x^*_1, x^*_2, \ldots, x^*_{t-1})'$ and defining $x_t = (x^*_t, x_{t-1})'$, where the $x^*_t$ is the new state generated at time $t$.

Special cases of these state-space models include hidden Markov models \citep{cappe2005inference,petris2009dynamic}, where the state $x_t$ has discrete support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997}, where each distribution is Gaussian whose mean is a linear function of the states and whose variance does not depend on the mean. The disease outbreak models discussed in Section \ref{sec:apply} are specific cases of state-space models, but we introduce these models in generality here because the particle filtering methods discussed in Section \ref{sec:filtering} apply to any model with this form.

\subsection{Sequential estimation \label {sec:sequential}}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, the distribution of the current state and parameters conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters. It can be updated recursively using Bayes' rule:
\begin{equation}
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$. Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically, e.g. in DLMs when $\theta$ is the observation variance \cite[Sec 4.3,][]{petris2009dynamic}. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. extended Kalman filter and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions such as particle filters.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is a sequential Monte Carlo (SMC) inferential technique based on repeated use of importance sampling. It aims to approximate the filtered distribution at time $t$ through a weighted Monte Carlo realization from this distribution in terms of $J$ particles, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j=1}^J w_t^{(j)} \delta_{\left(x_t^{(j)},\theta^{(j)}\right)} \label{eqn:approx}
\end{equation}
where $\left(x_t^{(j)},\theta^{(j)}\right)$ is the location of the $j^{\mbox{th}}$ particle at time $t$, $w_t^{(j)}$ is the weight of that particle with $\sum_{j=1}^J w_t^{(j)}=1$, and $\delta$ is the Dirac delta function. A variety of SMC techniques have been developed to provide more efficient approximations to equation \eqref{eqn:filtered} in the sense that with the same computation time a better approximation is achieved. We now review three fundamental particle filtering techniques: the bootstrap filter, auxiliary particle filter, and kernel density particle filter. In Section \ref{sec:results}, we compare the efficiency of these techniques in the syndromic surveillance context.

\subsection{Bootstrap filter \label{sec:bf}}

The first successful version of particle filtering is known as the bootstrap filter (BF) \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996}. Since this method and the auxiliary particle filter were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, to obtain an approximation to the filtered distribution at time $t+1$, perform the following steps for each particle $j=1,\ldots,J$:

\begin{enumerate}
\item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{w_t^{(1)},\ldots,w_t^{(j)},\ldots,w_t^{(J)}\right\}$,
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left( x_{t+1}\left|x_t^{(k)}\right.\right)$, and
\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right) \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right. .\]
\end{enumerate}

\noindent This procedure can be applied recursively beginning with an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$, usually obtained by sampling from the prior with uniform weights.

\subsection{Auxiliary particle filter \label{sec:apf}}

One problem that arises in implementing the BF is that $w_t^{(j)}$ will be small for particles where $p\left(y_{t}\left|x_{t}^{(j)}\right.\right)$ is small, and these particles will contribute little to the approximation to $p(x_{t}|y_{1:t})$. The auxiliary particle filter (APF) aims to mitigate this by anticipating which particles will have small weight using a look ahead strategy \citep{Pitt:Shep:filt:1999}. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the APF approximates $p(x_{t+1}|y_{1:t+1})$ by the following:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g.
\[ \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)} \right.\right). \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^{(l)}.\right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\right\}$,
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)}\right.\right)$, and
	\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)}\right.\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent The point estimate used in Step 1 can be any point estimate, although the expectation is commonly used. Step 3 is exactly the same as the BF with appropriate modifications to the weight calculation to adjust for the `look ahead' in steps 1 and 2. APF weights tend to be closer to uniform than BF weights, in which case a better approximation to $p(x_{t}|y_{1:t})$ is achieved.

The BF and the APF were constructed with the idea that all fixed parameters are known. In order to simultaneously estimate the time-evolving states and fixed parameters using either the BF or APF, it is necessary to incorporate the fixed parameters into the state with degenerate evolutions. That is, one regards the fixed parameters as elements of the state vector $x_t$ and specifies the state evolution equation such that these elements do not change over time. Due to the possible duplication of some particles and elimination of others through resampling, the number of unique values of the fixed parameters in the particle set will decrease over time, resulting in \emph{degeneracy} in the fixed parameters \citep{Liu:West:comb:2001}.

\subsection{Kernel density particle filter \label{sec:kd}}

The particle filter introduced by \citet{Liu:West:comb:2001} is a general way of fighting this degeneracy problem by approximating the set of fixed parameter values by a kernel density estimate and then regenerating values from this approximation. We refer to this filter as the kernel density particle filter (KDPF). This filter approximates $p(x_t,\theta| y_{1:t})$ via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ for particle $j$. This does not imply that the true $\theta$ is dynamic, but rather that particle $j$ can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The KDPF uses a tuning parameter $\Delta$, the discount factor that takes values in $(0,1)$, and two derived quantities $h^2 = 1 - ((3\Delta - 1)/2\Delta)^2$ and $a^2 = 1 - h^2$ that determine how smooth the kernel density approximation is. Lower values of $\Delta$ result in a smoother approximation. However, the goal here is simply to jitter particles around to refresh values of the fixed parameters, and so $\Delta$ is typically taken to be between 0.95 and 0.99 \citep{Liu:West:comb:2001}.

Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the KDPF provides an approximation of $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:

\begin{enumerate}
\item For each particle $j$, set $m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t.$ and calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g. $\mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right)$.
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^{(l)}. \right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\right\}$,
	\item Regenerate the fixed parameters: sample $\theta_{t+1}^{(j)} \sim N\left(\theta; m_t^{(k)}, h^2V_t \right)$,
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)},\theta_{t+1}^{(j)}\right.\right)$, and
	\item Calculate weights and renormalize:
	\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)},m_t^{(k)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent The KDPF adds the kernel density regeneration to the auxiliary particle filter. Here, we use a normal kernel, where $N(.;\mu,\Sigma)$ represents the pdf of normal distribution with mean $\mu$ and covariance matrix $\Sigma$.

To use the KDPF with normal kernels, it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$, and then the logit transformation can be applied. We investigate the sensitivity of the performance of the particle filters to the choice of transformation in Section \ref{sec:results}.

\subsection{Resampling \label{sec:advice}}

In addition to choosing which particle filter algorithm to use, successful implementation also depends on which resampling scheme to use and when to resample. Throughout our discussion, we have explicitly used multinomial resampling, but alternative resampling schemes exist including stratified, residual, and systematic resampling \citep{Douc:Capp:Moul:comp:2005}. Stratified resampling works by sampling uniformly over the interval $[(j-1) / J, j / J]$ for $j = 1, 2, \ldots, J$ and calculating the number of copies of particle $j$ according to the empirical cumulative distribution function of the particle sample (i.e. the ``inversion method'' \citep{Douc:Capp:Moul:comp:2005}). Residual resampling is implemented by calculating the number of copies of particle $j$ according to $\lfloor w^{(j)} J \rfloor$, i.e. the integer part of $w^{(j)} J$, and distributing the remaining $J - \sum^J_{j=1} \lfloor w^{(j)} J \rfloor$  particles according to a multinomial distribution with associated probabilities $(w^{(j)} J - \lfloor w^{(j)} J \rfloor) / (J - \sum^J_{j=1} \lfloor w^{(j)} J \rfloor)$. Finally, systematic resampling is carried out in the same way as the stratified approach, except that only one uniform draw is sampled from $[0, 1/J]$ and the remaining $J-1$ are calculated by adding $(j-1) / J$ to the sampled value.

Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy, but this introduces additional Monte Carlo variability to the particle sample. Out of the four methods mentioned in this paper, systematic resampling requires the least computational effort. However, it is the most unpredictable in terms of performance. In Section \ref{sec:resample}, we discuss some advantages and disadvantages of the different resampling methods when applied to our specific model of a disease outbreak and suggest the use of stratified or residual resampling. For a more complete description of these resampling techniques, please refer to \citet{Douc:Capp:Moul:comp:2005}.

The frequency of resampling should be reduced to balance the loss of information due to degeneracy with the loss of information due to the additional Monte Carlo variability introduced during resampling. Typically, a measure of the nonuniformity of particle weights is used to determine if resampling should be performed at a given iteration of a particle filter. The common measures are effective sample size, coefficient of variation, and entropy. We use effective sample size \citep{Liu:Chen:Wong:reje:1998}, a value ranging between 1 and $J$ that can be interpreted as the number of independent particle samples. An effective sample size of $J$ corresponds to all particle weights being equal, and a value of 1 corresponds to one particle weight being 1 with the rest 0. Using this measure of nonuniformity in our runs, we set a threshold of $0.8J$, meaning that if the number of independent samples is less than 80\% of the total number of particles at time $t$, resampling is performed at that time.

The BF, APF, and KDPF algorithms described in the previous sections were constructed under the assumption that resampling is performed at every iteration of the filter. However, in practice, we omit the resampling step in each algorithm at each time point where the effective sample size exceeds $0.8J$. If resampling is not performed, we modify the algorithm at that timepoint by 1) omitting the `Resample' step, 2) replacing all instances of the sampled index $k$ with the particle index $j$, and 3) adjusting the calculation of $\tilde{w}_{t+1}^{(j)}$ by multiplying by $w_t^{(j)}$, i.e. the particle weights get carried over. For the KDPF, regeneration is not performed when resampling is not performed since, in this case, there is no reduction in the number of unique fixed parameter values. When resampling is not performed, the `Regenerate the fixed parameters' step should read $\theta_{t+1}^{(j)} = \theta_t^{(j)}$.

\subsection{Theoretical justification of particle filters}

The methods described above are all theoretically justified as the number of particles becomes large. Specifically, the following result from Section 3.5.1 of \cite{del2004feynman} holds for bounded functions $f_t$ and any $p>1$:
\[ E_{\eta_t}^J \left[ \left| \eta_t^J(f_t)-\eta_t(f_t)\right|^p\right]^{1/p} \le \frac{a(p) b(t) ||f||}{\sqrt{J}} \]
where $\eta_t(f_t)$ is the expectation of $f_t$ under the true filtered distribution shown in equation \eqref{eqn:filtered} at time $t$, $\eta_t^J(f_t)$ is the expectation of $f_t$ under the particle approximation given by equation \eqref{eqn:approx} at time $t$ using $J$ particles, $a(p)$ is a function of $p$, $b(t)$ is an increasing function of $t$ that depends on which algorithm is used, and $||\cdot||$ is the supremum norm. The key here is that for a fixed $t$, we can increase the number of particles $J$ to reduce our Monte Carlo error, and this is true for all of the algorithms above, including the BF and APF. The question then becomes what does $b(t)$ look like for each algorithm, but this is a difficult question to answer. Instead, in Section \ref{sec:results}, we compare the performance of these algorithms at various values for $J$ within a specific model.

\section{Particle filtering in epidemiological models \label{sec:apply}}

The methodology described in Section \ref{sec:ss} and Section \ref{sec:filtering} provides a general framework applicable to a wide array of scientific questions. Tracking a disease outbreak using syndromic surveillance systems is one situation to which this methodology can be applied. In this section, we describe a state-space model of an influenza outbreak similar to one used in \citet{skvortsov2012monitoring}, and we explain how it fits into the general framework outlined in Sections \ref{sec:ss} and \ref{sec:filtering}. In Section \ref{sec:results}, we analyze the performance of the particle filter algorithms in this specific modeling situation.

\subsection{Model \label{sec:model}}

We incorporate a compartmental model (or SIR model) of an epidemic in which we track the proportion of the population that is susceptible ($s_t$), infectious ($i_t$), and recovered ($r_t$), i.e. no longer able to be infected, at time $t$. Mathematically, $s_t$, $i_t$, and $r_t$ are all nonnegative and $s_t + i_t + r_t = 1$ for all $t$. When monitoring an epidemic, the true $s_t$, $i_t$ and $r_t$ are unknown and regarded as hidden states of the model. The observed data are gathered via syndromic surveillance. In our state-space model of a disease outbreak, the observation equation specifies how the observed data depend on the state of the epidemic and the state equation describes how the epidemic evolves over time.

\subsubsection{State equation \label{sec:state}}

First, we describe the state equation. Let $x_t = (s_t,i_t)'$ denote the state of the epidemic at time $t$. Note that, by definition, $r_t$ is completely specified once $s_t$ and $i_t$ are known, and hence is not needed in the state vector. We consider a compartmental model of disease transmission that is governed by three parameters:

\begin{itemize}
\item $\beta$, the contact rate of spread of disease,
\item $\gamma$, the recovery time from infection (i.e. the reciprocal of the average infectious period), and
\item $\nu$, the mixing intensity of the population.
\end{itemize}

\noindent $\beta$, $\gamma$, and $\nu$ are each restricted to be nonnegative. Define $\theta = (\beta,\gamma,\nu)'$ to be the vector of unknown parameters in our model, and let $P$ be the size of the population. Then, we describe the evolution of the epidemic from time $t$ to $t + 1$ by

\begin{equation}
x_{t+1}\left|x_t,\theta\right. \sim N_\Omega\left(x_{t+1};f(x_t,\theta),Q(\theta)\right) \label{eqn:state}
\end{equation}
\noindent where
\[
f(x_t,\theta) = \left(
\begin{array}{c}
s_t - \beta i_ts^\nu_t \phantom{- \gamma i_t}\,\, \\
i_t +  \beta i_ts^\nu_t - \gamma i_t
\end{array}
\right)
\qquad
Q(\theta) = \frac{\beta}{P^2} \left(
\begin{array}{ccccc}
1 & -1 \\
-1 & 1 + \gamma/\beta
\end{array}
\right)
\]

\noindent and $\Omega = \{(s_t,i_t): s_t \ge 0, i_t \ge 0, s_t + i_t \le 1\}$. $N_{\Omega}(.; \mu,\Sigma)$ represents the pdf of the truncated normal distribution onto the set $\Omega$ with mean and covariance matrix of its corresponding untruncated normal distribution given by $\mu$ and $\Sigma$, respectively.

In equation \eqref{eqn:state}, $Q(\theta)$ is determined by calculating the variances and covariance of $s_{t+1}$ and $i_{t+1}$ in the discrete time approximation of a modified SIR model with stochastic fluctuations \citep{herwaarden1995stochepid, dangerfield2009stochepid, anderson2004sars}, given by
\begin{align*}
s_{t+1} &= s_t - \beta i_ts^\nu_t + \epsilon_\beta \\
i_{t+1} &= i_t + \beta i_ts^\nu_t - \gamma i_t - \epsilon_\beta + \epsilon_\gamma
\end{align*}
where $\epsilon_\beta$ and $\epsilon_\gamma$ are random components with $\epsilon_\beta \sim N(0, \sqrt{\beta} / P)$ and $\epsilon_\gamma \sim N(0, \sqrt{\gamma} / P)$. The variances of these terms come from a scaling law for stochastic fluctuations in a dynamical system generated by random contacts among the population \citep{ovaskainen2010extinction, herwaarden1995stochepid, dangerfield2009stochepid, skvortsov2012monitoring}.

$R_0 = \beta / \gamma$ is called the \emph{basic reproductive number} \citep{heff2005repratio}, or the average number of people infected by one sick person in a population where everyone is susceptible. $R_0 > 1$ is required for an infection to spread into an epidemic. In many cases, prior information about this quantity for a specific type of infection is more readily available than prior knowledge about $\beta$ or $\gamma$ individually. 

The mixing parameter $\nu$ describes the heterogeneity of social interactions within the population, where $\nu = 1$ corresponds to a population with homogenous mixing, i.e. an infectious person is equally likely to infect any susceptible, and $\nu = 0$ corresponds to a population with no social interaction. Other values of $\nu$ represent populations with heterogenous mixing, i.e. an individual is more likely to interact with some people more than others, leading to less severe epidemics than those that would occur in homogenous populations for a fixed $R_0$ \citep{stroud2006powerlaw, novozhilov2008hetero}.

\subsubsection{Observation equation \label{sec:obs}}

Next, we describe the observation equation. We consider observed data that are positive real numbers related to counts of emergency room visits, prescription sales, or calls to a hotline, for example, and we can observe data from these different streams/sources asynchronously in time. That is, at any time $t$, we can observe data from any subset of the streams (or possibly none of them). Let $y_{l,t}$ represent data coming from stream $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$. We model the log of the observations (so that $y_{l,t}$ is restricted to be positive) by
\begin{equation}
\log y_{l,t} \sim N\left(\log y_{l,t};b_li_t^{\varsigma_l} + \eta_l,\sigma_l^2\right) \label{eqn:obs}
\end{equation}
where $b_l$, $\varsigma_l$, and $\sigma_l$ are nonnegative constants \citep{skvortsov2012monitoring} and $\eta_l$ is a real number that determines the baseline level of incoming syndromic data from stream $l$.

The form of the mean of $\log y_{l,t}$ in equation \eqref{eqn:obs} is derived from a simplification of the power-law relationship, described in \citet{skvortsov2012monitoring} and \citet{Gins:Mohe:Pate:Bram:Smol:Bril:dete:2009}, between syndromic observations and the proportion of the population that is infectious. $b_l$ is a multiplicative constant that depends on the syndromic data source, $\varsigma_l$ is the power-law exponent, and $\sigma_l$ is the standard deviation term that determines the magnitude of random fluctuations in the syndromic observations from stream $l$. $b_l$, $\varsigma_l$, $\sigma_l$, and $\eta_l$ are typically not known, but we will assume for the moment that they are known for all $l$. In an extended analysis described in Section \ref{sec:extend}, we demonstrate that improved SMC methods enable us to regard these parameters also as unknown and include them in a higher-dimensional $\theta$.

Having formulated the data-generating model, we define $y_t = (y_{1,t},\ldots,y_{L,t})'$ and specify the likelihood of an observation $y_t$ given $x_t$ and $\theta$ by $p(y_t|x_t,\theta) = \mbox{LN}(y_t;\mu_t,\Sigma_t)$, where $\mu_t$ is an $L$-length vector with element $l$ equal to $b_li_t^{\varsigma_l} + \eta_l$, $\Sigma_t$ is an $L \times L$ diagonal matrix with the $l^{\mbox{th}}$ diagonal equal to $\sigma_l^2$, and $\mbox{LN}(.;\mu,\Sigma)$ is the pdf of the log-normal distribution with mean $\mu$ and covariance matrix $\Sigma$ on the log scale. Elements of $y_t$ may be missing, in which case the dimension of $p(y_t|x_t,\theta)$ shrinks by the number of missing elements. If all elements of $y_t$ are empty (i.e. if no syndromic data are observed at time $t$), we adopt the convention that $p(y_t|x_t,\theta) = 1$ (in the context of a particle filter algorithm, this ensures that particle weights are kept the same as they were at time $t-1$).

Since $p(x_{t+1}|x_t,\theta)$ and $p(y_t|x_t,\theta)$ are nonlinear functions of $x_t$ and $\theta$, a closed-form solution to the posterior $p(x_t,\theta|y_{1:t})$ cannot be obtained. Thus, we use the particle filtering techniques described in Section \ref{sec:filtering} to approximate $p(x_t,\theta|y_{1:t})$ for all $t$.

\section{Comparing particle filters} \label{sec:results}

We now compare the performance of the BF, APF and KDPF using simulated data analogous to that analyzed by \citet{skvortsov2012monitoring} using the BF. In addition, using the KDPF, we compare the performance of bounded versus unbounded priors on the fixed parameters, as well as different resampling schemes. Lastly, we discuss the role of the discount factor $\Delta$ when implementing the KDPF and compare results from the KDPF with an MCMC approach.

\subsection{Simulation}

40 epidemics were simulated according to equation \eqref{eqn:state} for a population of size $P = 5000$. Each simulated epidemic was tracked for $T = 125$ days. True values of parameters $\theta$ that we estimate (termed `unknown parameters') were different for each simulated outbreak, determined by sampling from the log-normal prior distribution, $p(\theta)$, that we describe in Section \ref{sec:pf}. For all simulations, infection was introduced in 10 people in the population at day 0 (i.e. true $i_0 = 10/5000$ and $s_0 = 4990/5000$). Among the 40 simulations, the average time at which the epidemics peak is 57.25 days and the average proportion of the population that has been infected by $t = 125$ is 73.74\%. The left panel of Figure \ref{fig:data} shows the evolution of $s_t$, $i_t$, and $r_t$ for a single simulation.

Data from randomly selected streams at each day were generated from equation \eqref{eqn:obs}. The right panel of Figure \ref{fig:data} displays the observed data from the simulated epidemic shown on the left. Values of known constants for $L = 4$ streams were kept the same for each simulation and are given in Table \ref{tab:constants} ($\eta_l$ was set to 0 for all $l$). Values for $b_l$, $\varsigma_l$, and $\sigma_l$ were chosen to be the same as those used in the numerical study carried out by \citet{skvortsov2012monitoring}. The values chosen for $\varsigma$ were motivated by evidence from \citet{chew2010twitter} that suggests values close to 1 based on real syndromic data. Because of a lack of reporting on reasonable values for $b_l$ and $\sigma_l$, these parameters were set to values convenient for simulation.

\begin{table}
\begin{center}
\begin{tabular}{|cccc|}
\hline
$l$ & $b_l$ & $\varsigma_l$ & $\sigma_l$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\caption{Values of known constants in model.}
\label{tab:constants}
\end{center}
\end{table}

\begin{figure}
\centering
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-epid-1}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-z-1}
\end{minipage}
\caption{Simulated epidemic curves (left) and syndromic observations (right) for a single simulated flu epidemic. True values of unknown parameters used for simulation were $\beta = 0.2543748$, $\gamma = 0.1113561$, and $\nu = 1.2460013$.} \label{fig:data}
\end{figure}

\subsection{Particle filter runs} \label{sec:pf}

For each simulated data set, the BF, APF, and KDPF were run using $J = 100, 1000, 10000, \mbox{ and } 20000$ particles to obtain weighted sample approximations of $p(x_t,\theta|y_{1:t})$ for $t = 1,\ldots,T$. For each $J$, separate runs using multinomial, residual, stratified, and systematic resampling were implemented \citep{smcUtils}, and an effective sample size threshold was set at 80\% of the total number of particles to determine when to resample particles \citep{Liu:Chen:Wong:reje:1998}. For the KDPF, sensitivity to changes in the discount factor $\Delta$ was explored by running with $\Delta = 0.9, 0.95, 0.96, 0.97, 0.98, \mbox{ and } 0.99$.

To start each particle filter run, the initial state, $x_0$, and fixed parameters, $\theta_0$, for $J$ particles were sampled from the prior density $p(x_0,\theta) = p(\theta)p(i_0,s_0)$, where $p(i_0,s_0)$ is the joint pdf of the random variables $s_0$ and $i_0$ and $p(\theta)$ is the joint prior density of $\beta$, $\gamma$, and $\nu$. Since a very small percentage of the population is infected during the initial stage of the epidemic, we let $p(i_0) = N_{[0,1]}(i_0;0.002,0.0005^2)$ and set $s_0 = 1 - i_0$, i.e. no infected individuals have recovered from illness yet.

To investigate the impact of different prior distributions for $\theta$ on the performance of the particle filters, the runs described above were performed once using uniform priors on $\theta$ and then again using log-normal priors. Uniform priors on $\theta$ were chosen to be the same as those used in \citet{skvortsov2012monitoring}, i.e. \hbox{$p(\beta) = \mbox{Unif}(\beta; 0.14, 0.50)$}, \hbox{$p(\gamma) = \mbox{Unif}(\gamma; 0.09, 0.143)$}, and \hbox{$p(\nu) = \mbox{Unif}(\nu; 0.95, 1.3)$}, where $\mbox{Unif}(.; a, b)$ represents the pdf of the continuous uniform distribution on the interval $(a,b)$. These priors allow for values of $R_0$ in a range of approximately 1 to 5.5 and an average infectious of period in a range of roughly 7 to 11 days. $R_0$ values for strains of influenza have been estimated to be around 2-3 \citep{mills2004influenza, heff2005repratio, zhang2011flu}, and so while these priors impose restrictive bounds on the parameters, they are not particularly informative for tracking a flu epidemic.

We define log-normal priors on $\theta$ by \hbox{$p(R_0) = \mbox{LN}(R_0; 0.7520, 0.1768^2)$}, \hbox{$p(\gamma) = \mbox{LN}(\gamma; -2.1764, 0.1183^2)$}, and \hbox{$p(\nu) = \mbox{LN}(\nu; 0.1055, 0.0800^2)$}. Here, we incorporate prior information on $R_0$ instead of $\beta$ directly, since prior knowledge of the basic reproductive number may be easier to obtain than for the contact rate itself. When implementing the particle filter algorithms, the prior draws for $\beta$ were determined by multiplying the sampled values of $R_0$ and $\gamma$ at time 0. These log-normal priors constrain $\beta$, $\gamma$ and $\nu$ to be positive. The mean and variance of the prior distributions on $\log \gamma$ and $\log \nu$ were chosen such that random draws of $\gamma$ and $\nu$ would fall within the bounds of their respective uniform priors with 95\% probability, and the mean and variance of $\log R_0$ were chosen such that $R_0$ would fall between 1.5 and 3 with 95\% probability.

Logit and log transformations were applied to the components of $\theta$ in the manner described at the end of Section \ref{sec:kd} so that the normal kernel could be used in the KDPF while constraining $\beta$, $\gamma$, and $\nu$ to be within their respective prior domains (i.e. logit was used with uniform priors and log with log-normal priors).

\subsection{Comparison of particle filter algorithms under uniform priors} \label{sec:pfcomparison}

First, we compare the performance of the particle filtering algorithms using uniform priors on $\theta$ and systematic resampling, since these priors and resampling scheme were used in \citet{skvortsov2012monitoring}. For ease of comparison, the same prior draws were used in each particle filtering algorithm for fixed $J$. Figure \ref{fig:pfs} shows 95\% credible bounds of $p(\beta|y_{1:t})$, $p(\gamma|y_{1:t})$, and $p(\nu|y_{1:t})$ for $t = 1,2,\ldots,T$ and $J = 100, 1000, 10000, 20000$ using the simulated data displayed in Figure \ref{fig:data}. Initially, these credible interval bounds for the BF and APF match those of the KDPF, but quickly degenerate toward a single value due to elimination of unique particles during resampling. Although the time of degeneracy increases as $J$ gets larger, the BF and APF bounds become misleading during the second half of the epidemic even for $J = 20000$. The bounds for the KDPF, on the other hand, have dramatically reduced degeneracy since new values of $\theta$ are regenerated from the kernel density approximation.

The KDPF also has an advantage over the BF and APF in terms of computational efficiency. Notice that the bounds for the KDPF become wider as $J$ increases, but they do not change between $J = 10000$ and $J = 20000$. This suggests that by 10000 particles, the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ has converged to the true posterior over the entire epidemic period, unlike with the BF and APF. Even though the bounds for $\beta$ for the BF and APF seem to roughly match those of the KDPF for $J = 20000$ over the first half of the epidemic, the KDPF provides the same measure of uncertainty for $J = 10000$ and does not degenerate during the second half of the epidemic.

Also noteworthy is how the bounds for $\nu$ expand between $t = 70$ and $t = 80$ for $J \ge 10000$. We typically expect to see the width of credible intervals for an unknown fixed paramter decrease monotonically over time as data is accumulated. A plausible explanation here is that $p(\nu|y_{1:t})$ has been squeezed against the lower bound of the prior for $\nu$. Rerunning the analysis (results not shown) using more relaxed prior bounds on $\nu$ shows a shift in the distribution toward higher values as opposed to the widening of the interval that we see in Figure \ref{fig:pfs}.

The behavior of the BF, APF, and KDPF shown in Figure \ref{fig:pfs} is consistent across the 40 simulations. For instance, for $J = 20000$, the 95\% credible intervals at $t = 125$ for each of $\beta$, $\gamma$ and $\nu$ cover the truth for 19 out of 20 simulations using the KDPF. The BF and APF runs using the same number of particles, on the other hand, result in 95\% credible intervals at $t = 125$ that cover the truth for no more than 7 out of 20 simulations when considering $\beta$, $\gamma$, and $\nu$ individually.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-1-systematic-unif-logit-99-61-params}
\caption{Sequential 95\% credible intervals for $\beta$ (left column), $\gamma$ (middle column), and $\nu$ (right column) for increasing number of particles (rows) for the BF (red), APF (blue), and KDPF (green), compared with the truth (black lines), when using systematic resampling and uniform priors. Data were generated from the simulated epidemic shown in Figure \ref{fig:data}. For the KDPF, $\Delta$ was set to 0.99. } \label{fig:pfs}
\end{figure}

\subsection{Comparison of logit versus log transformation in the KDPF \label{sec:priors}}

As mentioned in \ref{sec:kd}, implementing the KDPF using a normal kernel density approximation of $p(\theta|y_{1:t})$ to regenerate the fixed parameters requires applying some transformation to the components of $\theta$ so that their support is on the real line. The logit function is a convenient choice for mapping fixed parameters with bounded support to the real line; the log function is convenient for fixed parameters with positive support. Thus, we investigate the sensitivity of the results with $J = 10000$ to these two types of transformations using the KDPF.

Figure \ref{fig:priors} compares scatterplots of $\beta$ versus $\gamma$ sampled jointly from the filtered distribution $p(\beta,\gamma|y_{1:t})$ at $t = 0, 20, 40, 60$. In the top row, the same prior as in \citet{skvortsov2012monitoring} is used, i.e. \hbox{$p(\gamma,\beta) = \mbox{Unif}(\gamma;0.09, 0.143) \mbox{Unif}(\beta; 0.14, 0.50)$}. In order to ensure regeneration in the KDPF does not extend past these bounds, a logit transformation is applied in the manner described at the end of Section \ref{sec:kd} with $a = 0.14$ and $b = .50$ for $\beta$ and $a = 0.09$ and $b = .143$ for $\gamma$, and the kernel density is created on this transformed space. In the top row, the samples concentrate on the boundaries of the uniform prior on $\gamma$, particularly for $t = 20$ and $t = 40$. This suggests that the prior bounds on $\gamma$ are too restrictive to account for the uncertainty in this recovery time.

To test this theory, we reran the KDPF using the same prior draws as those used in the first row of Figure \ref{fig:priors}, but we apply a log transformation to $\theta$ (to ensure $\beta,\gamma,\nu>0$) instead of the logit transformation. The results are shown in the second row of Figure \ref{fig:priors}. Despite starting within the uniform bounds at $t=0$, the samples stray outside the uniform bounds for $\gamma$, suggesting that the data are informing us that reasonable parameter values can be found outside the bounds.

The bottom row of Figure \ref{fig:priors} displays results from running the KDPF using prior samples taken from the log-normal priors on the components of $\theta$. As in the second row, a log transformation was applied to $\theta$ so that the kernel density could be used for jittering particles. The log-normal priors on $\theta$ are more informative than the uniform priors in that a greater number of particles are concentrated near the true values of $\beta$ and $\gamma$ at $t = 0$. Yet, the distribution of particles at $t = 20$ and $t = 40$ appear more spread out in the bottom row than in the top row because the log-normal priors are less restrictive on the sample space of $\beta$ and $\gamma$ than are the uniform priors. 

At $t = 60$ in the bottom row, sampled particle values have moved inside the uniform bounds and show an ellipse-shaped distribution similar to what is shown in the second row at $t = 60$. This suggests that the tail of points concentrated along the upper uniform bound at $t = 60$ in the top row is an artifact of the over-restrictive uniform prior and not influenced by the data. We suggest using log-normal priors on positive elements of $\theta$ as opposed to uniform priors on bounded parameters. This allows us to use prior knowledge of the epidemic to encourage points to lie in a reasonable range while retaining flexibility in the event of model mis-specification either in the likelihood or the prior.

\begin{figure}
\centering
\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{PF-betaGammaScat-1-10000-KD-systematic-99-61}
\caption{Scatterplots of $\beta$ (horizontal) versus $\gamma$ (vertical) with true values (red crosses) at $t = 0, 20, 40, 60$ days using the KDPF with $J = 10000$ particles, systematic resampling, and $\Delta = 0.99$. The logit transformation (top row) on $\theta$ shifted and scaled to $(0,1)$ and log transformation (second row and bottom row) were used prior to regenerating the fixed parameters. The same uniform prior draws of $\theta$ were sampled in each of the first two rows at $t = 0$, and log-normal prior draws were sampled in the bottom row. For demonstration, each panel shows 500 particles sampled from the weighted sample approximation of $p(x_t,\theta|y_{1:t})$. Axes are the same in each panel. Dashed horizontal and vertical lines indicate the bounds of the uniform priors on $\gamma$ and $\beta$, respectively. The upper bound on the uniform prior on $\beta$ is not shown because it lies outside the range of the horizontal axis.} \label{fig:priors}
\end{minipage}
\end{figure}

\subsection{Comparison of resampling schemes \label{sec:resample}}

As mentioned in Section \ref{sec:advice}, resampling is meant to rebalance the weights of the particles in order to avoid degeneracy, but this comes at the cost of increasing the Monte Carlo variability of the particle sample. Up to this point, we have used only systematic resampling, as in \citet{skvortsov2012monitoring}. Alternatively, we could have chosen multinomial, residual, or stratified resampling. \citet{Douc:Capp:Moul:comp:2005} explains each of these methods in detail and shows that 1) multinomial resampling introduces more Monte Carlo variability than do residual or stratified resampling, 2) residual and stratified resampling introduce the same amount of Monte Carlo variability, on average, and 3) systematic resampling can introduce more Monte Carlo variability than does multinomial resampling.

With this in mind, we turn to a comparison of different techniques for the resampling step using the KDPF with log-normal priors on $\theta$ and $\Delta = 0.99$. To aid in comparison of the different resampling techniques, the same prior draws were used in all particle filter runs for fixed $J$. We would like to choose the resampling scheme for which the filtered distribution, $p(x_t,\theta|y_{1:t})$, approaches the true posterior the fastest as a function of the number of particles. If the filtered distributions have converged to the true posterior, then 95\% credible intervals should cover the true parameter value about 95\% of the time. 

Using $J = 100$ particles (not shown), sequential 95\% credible intervals over the second half of the epidemic for each of $\beta$, $\gamma$, and $\nu$ cover the true parameter value for less than half of the 40 simulated data sets, indicating that more particles are needed to approximate the true posterior. Figure \ref{fig:resamp} shows that coverage probabilities approach the nominal level for all four resampling techniques as $J$ increases. Multinomial resampling, however, appears to be outperformed by the other three resampling techniques, as coverage for all three model parameters using $J = 1000$ particles dips lower during the second half of the epidemic than it does for any of the other three methods. This is also true for $\beta$ with $J = 10000$ particles. By increasing the number of particles to $J = 20000$ (not shown), all four resampling methods yield coverage probabilities for each parameter that remain within 95\% confidence bounds around the nominal coverage level throughout the epidemic.

Although the other three methods perform about the same with this specific model, we prefer to use either residual or stratified resampling because of an example shown in \cite{Douc:Capp:Moul:comp:2005} where systematic resampling has more Monte Carlo variability than any of the other three resampling schemes.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-coverage-95-40-KD-orig-resamp-params}
\caption{Proportion of the 40 simulated data sets for which 95\% credible intervals for the three model parameters (columns) cover the true value used for simulation for different $t$ (x-axis) and $J$ (rows) using the KDPF with log-normal priors on $\theta$ and $\Delta = 0.99$. Solid gray horizontal line denotes nominal coverage (95\%) and dashed lines give 95\% confidence bounds around the true coverage.} \label{fig:resamp}
\end{figure}

\subsection{Additional notes \label{sec:issues}}

\subsubsection{Discount factor}

We recommend, based on the results from Sections \ref{sec:pfcomparison}, \ref{sec:priors}, and \ref{sec:resample}, that the KDPF with residual or stratified resampling and prior distributions bounded only by the support of the parameters be used in preference to other choices mentioned. With this implementation of the particle filter, the practitioner is still left to choose a value for the discount factor, $\Delta$. As mentioned in Section \ref{sec:kd}, $\Delta$ is a tuning parameter that determines the smoothness of the kernel density approximation to $p(\theta|y_{1:t})$ when implementing the KDPF. Choosing $\Delta$ close to 0 results in a smoother approximation and more substantial jittering of particles, while $\Delta$ close to 1 leads to a choppier approximation and more subtle jittering of particles.

To test the sensitivity of the KDPF to different values of $\Delta$, we ran the KDPF with log-normal priors and stratified resampling on each of the 40 simulated data sets using different values of $\Delta$ (0.9, 0.95, 0.96, 0.97, 0.98, and 0.99) and different number of particles (100, 1000, 10000, and 20000). We then calculated 95\% credible intervals from the marginal filtered distributions of each of the model parameters. The results (not shown) indicate that lower values of $\Delta$ lead to a higher probability of the 95\% credible intervals covering the truth, but that coverage probabilities for all values of $\Delta$ reach the nominal level for $J \ge 10000$. Choosing a lower value for $\Delta$ may help the performance of the KDPF for lower number of particles, but \citet{Liu:West:comb:2001} recommends choosing a value between 0.95 and 0.99. We've used a value of $\Delta = 0.99$ in this paper because we seek an implementation of the KDPF that performs well while requiring minimal jittering of particles.

\subsubsection{Comparison with MCMC}

Analyzing our model using a sequential Monte Carlo approach is convenient because it allows for sequential updating of parameter estimates and real-time tracking of a disease epidemic. Furthermore, the KDPF algorithm is easy to implement, offering an alternative to traditional MCMC methods that often are more difficult to develop and require more computing effort. In this case, the KDPF provides similar results to a full MCMC analysis.

The full MCMC was implemented by sampling each of the states and parameters conditional on the rest using a Gibbs sampler, where each new sample was generated from a Metropolis-Hastings step using a Gaussian, random-walk proposal distribution. MCMC samples were generated conditional on the first $T$ observations from the simulated data sets pictured in Figure \ref{fig:data}, where $T = 30, 60, 90, \mbox{ and } 125$. For each $T$, three MCMC chains with different starting values for the unknown parameters were run. For each chain, the standard deviations of the proposal distributions were tuned during a burn-in period of 100000 samples, after which 10 million iterations of the MCMC were generated. Because of the nonlinear nature of the state equation, mixing of the MCMC was poor and many iterations were required for the chains to explore the sample space of the parameters. Because of the poor mixing and slow convergence, starting values for the states were taken at their true simulated values.

Table \ref{tab:mcmc} shows that 95\% credible intervals taken from the marginal filtered distributions of the model parameters using the KDPF with $J = 20000$ particles compare well with those calculated from the MCMC samples. While the MCMC on the full data set took weeks to run, the KDPF with $J = 20000$ particles provides results in less than an hour. In addition, results obtained from the KDPF are theoretically justified as $J$ gets large, and we trust the KDPF more in this case due to the poor mixing of the MCMC.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|cc|cc|cc|}
\hline
 & & \multicolumn{2}{|c|}{$\beta$} & \multicolumn{2}{|c|}{$\gamma$} & \multicolumn{2}{|c|}{$\nu$} \\
\hline
$T$ & Method & Lower & Upper & Lower & Upper & Lower & Upper \\
\hline
30 & MCMC & 0.192 & 0.250 & 0.091 & 0.140 & 0.952 & 1.305 \\
   & KDPF & 0.190 & 0.251 & 0.089 & 0.140 & 0.948 & 1.292 \\
\hline
60 & MCMC & 0.224 & 0.246 & 0.113 & 0.133 & 1.000 & 1.332 \\
   & KDPF & 0.226 & 0.248 & 0.116 & 0.135 & 0.982 & 1.251 \\
\hline
125 & MCMC & 0.235 & 0.240 & 0.123 & 0.128 & 1.051 & 1.189 \\
   & KDPF & 0.234 & 0.242 & 0.122 & 0.131 & 1.005 & 1.208 \\
\hline
 & Truth & \multicolumn{2}{|c|}{0.237} & \multicolumn{2}{|c|}{0.124} & \multicolumn{2}{|c|}{1.157} \\
\hline
\end{tabular}
\caption{95\% credible intervals for model parameters calculated from MCMC samples conditional on the first $T$ data points compared with those calculated from the marginal filtered distributions at time $T$ using the KDPF with $J = 20000$ particles, log-normal priors, stratified resampling, and $\Delta = 0.99$.}
\label{tab:mcmc}
\end{center}
\end{table}

\section{Additional Unknown Parameters \label{sec:extend}}

We now turn our focus to extending the analysis to include $\{b_l,\varsigma_l,\sigma_l,\eta_l:l\in1,\ldots,L\}$ as unknown parameters, thereby increasing the number of unknown parameters beyond those considered by \citet{skvortsov2012monitoring} using the BF. For this section, we consider data coming from only $L = 1$ stream and let $\theta = (\beta, \gamma, \nu, b, \varsigma, \sigma, \eta)'$, dropping the subscript $l$. Keeping the same simulated evolution of the true epidemic as shown in the left panel of Figure \ref{fig:data}, a single stream of syndromic observations (data $y_t$) was simulated from equation \eqref{eqn:obs} with true values of $b$, $\varsigma$, $\sigma$, and $\eta$ set to $0.258056$, $1.027544$, $0.000737$ and $2.346405$, respectively. Days at which data were observed from the single stream were randomly selected.

The KDPF with tuning parameter $\Delta$ set to 0.99 was run with $J = 60000$ particles, and stratified resampling was used with an effective sample size threshold of $0.8J$. As before, fixed parameter values were regenerated only when resampling was performed. Initial particles for states and parameters were sampled from their prior with the parameters being independent of the state. The prior for the state and log-normal priors for $R_0$, $\gamma$, and $\nu$ are the same as those defined in Section \ref{sec:pf}. The priors for $b$, $\varsigma$, $\sigma$, and $\eta$ are \hbox{$p(b) = \mbox{LN}(b; -1.6090, 0.3536^2)$}, \hbox{$p(\varsigma) = \mbox{LN}(\varsigma; -0.0114, 0.0771^2)$}, \hbox{$p(\sigma) = \mbox{LN}(\sigma; -7.0516, 0.2803^2)$}, and \hbox{$p(\eta) = N(\eta; 2.5, 1)$}. The choice of prior mean and standard deviation on the log scale were made such that random draws of $b$, $\varsigma$, and $\sigma$ on the original scale would be within $(0.1, 0.4)$, $(0.85, 1.15)$, and $(0.0005, 0.0015)$, respectively, with 95\% probability. To assess the loss in precision of our estimates due to incorporating additional unknown parameters into our analysis, we compared with results from running the KDPF using 60000 particles with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed known at their true values used for simulating the data (we refer to the run with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed known as the original analysis).

Figure \ref{fig:ext} shows sequential 95\% credible intervals for both the extended (blue lines) and the original (red lines) analyses. Most noticeable from Figure \ref{fig:ext} is that the intervals for $\beta$, $\gamma$, $\nu$, $s_t$, and $i_t$ are wider for the extended analysis than they are for the original. This is due to the added uncertainty in $b$, $\varsigma$, $\sigma$, and $\eta$ in the extended analysis. Nonetheless, we are still able to obtain credible intervals for the unknown parameters that cover the true values, as well as intervals for the states that cover the true epidemic curves, by increasing the number of particles used in the KDPF.

In Figure \ref{fig:ext}, the lines appear choppy or block-like. This results from data coming from only one stream, leading to more time points where no data are available and making the analysis more sensitive to abnormal data. Gaps in the data lead to a lack of resampling of particles and cause more drastic shifts in the filtered distribution once data arrive.
For instance, we notice a spike in the $s_t$ curve right after $t = 40$ because of a shift in the trajectory of data points near the epidemic peak.

Lastly, we comment on a widening of the credible intervals for some of the model parameter in the extended analysis, particularly $\nu$ and $\varsigma$. This phenomenon suggests that the log-normal priors used on these parameters that samples particles are too restrictive, and that our model provides even less insight about this parameter than our prior belief. Scarce knowledge about $\nu$ is gained over the course of the epidemic in the original analysis due to the nonlinear nature of the evolution equation with respect to $\nu$, and we in fact lose information about $\nu$ as well as $\varsigma$ in the extended analysis relative to our specified prior. While the extended analysis could be rerun with a different prior, we present this specific analysis to illustrate the sensitivity of the filtered distribution of $\nu$ and $\varsigma$ to assumptions about other parameters. The improved efficiency of the KDPF provides insight into this sensitivity (within reasonable computing time) in the absence of assumptions made about fixed parameters in the observation equation in our initial analysis and in the prior BF analysis by \citet{skvortsov2012monitoring}.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-ext-1-60000-KD-stratified-orig-log-99-61}
\caption{Sequential 95\% credible intervals for the states and fixed parameters from the original (red) and extended (blue) analyses where the KDPF with $J = 60000$ particles was run with stratified resampling and $\Delta = 0.99$. Tick marks are shown along the bottom of the plot for $s_t$ at time points when data were observed (dark gray) and when particles were resampled (blue and red for the extended and original analyses, respectively).} \label{fig:ext}
\end{figure}

\section{Discussion \label{sec:discussion}}

Presented here is a strategy for simultaneous estimation of the current outbreak state and fixed parameters related to disease transmission using syndromic data. We introduce a stochastic epidemiological compartment model of a disease outbreak for data from syndromic surveillance that could possibly be multivariate and have any pattern of missingness. We suggest the use of the kernel density particle filter \citep{Liu:West:comb:2001} using priors on fixed parameters that are bounded only by their support. We suggest the use of stratified (or residual) resampling when effective sample size has dropped markedly and regeneration of fixed parameters values should only occur when resampling is performed. We showed how this approach is capable of estimating a model with additional unknown fixed parameters.

The KDPF provides a generic sequential inferential strategy that reduces particle degeneracy in many situations. Advanced techniques exist that are better than the KDPF at fighting particle degeneracy, but require more practitioner input. For example, particle degeneracy could be combated within an SMC algorithm by incorporating a MCMC step to refresh fixed parameter values \citep{Gilk:Berz:foll:2001,Stor:part:2002}. However, this would require the practitioner to define a MCMC algorithm in addition to the SMC algorithm. In addition to this requirement, the algorithm would no longer be truly sequential as the computational effort increases with time. Alternatively, if the practitioner is willing to modify their model, they can take advantage of a sufficient statistic structure \citep{Fear:mark:2002}, Rao-Blackwellization \citep{Douc:Gods:Andr:on:2000}, or both \citep{carvalho2010particle}. Possible modifications to the model in Section \ref{sec:model} to allow alternative strategies (such as these) include setting $\nu=1$, removing fixed parameters from $Q$, and eliminating the truncation in equation \eqref{eqn:state}.

In this paper, we outline a strategy for real time tracking of a disease epidemic using data from syndromic surveillance, but this strategy can be applied to many other fields requiring on-line data analysis. We present improved particle filtering methods in general within the framework of sequential estimation of states and unknown fixed parameters in state-space models to inspire future work in epidemiological modeling and other scientific areas as well.

\clearpage

\bibliographystyle{model1-num-names}
\bibliography{jarad}

\end{document} 