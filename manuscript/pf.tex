\documentclass{elsarticle}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}

\graphicspath{{/Users/niemi/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/Users/niemi/Dropbox/SIR_Particle_Filtering/doesnotexist/}
                       {/Users/Danny/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/data/home/faculty/meiring/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {./Graphs/}
                       % Add your directories here
}

\newcommand{\jarad}[1]{{\color{red}JARAD: #1}}
\newcommand{\danny}[1]{{\color{blue}DANNY: #1}}

\journal{Mathematical Biosciences}

\begin{document}

\begin{frontmatter}

\title{Estimation of a disease epidemic using particle filtering}

\author[danny]{Daniel M. Sheinson}
\author[jarad]{Jarad Niemi}
\author[wendy]{Wendy Meiring}

\address[danny]{Corresponding author -- Department of Statistics and Applied Probability, University of Califonia--Santa Barbara, \\
 Santa Barbara, CA, U.S.A., sheinson@pstat.ucsb.edu, 1-847-609-7824}
 \address[jarad]{Department of Statistics, Iowa State University, Ames, IA, U.S.A., niemi@iastate.edu}
 \address[wendy]{Department of Statistics and Applied Probability, University of Califonia--Santa Barbara, \\
 Santa Barbara, CA, U.S.A., meiring@pstat.ucsb.edu}

\begin{abstract}
This paper presents methodology for tracking and prediction of a disease outbreak via a syndromic surveillance system. We present a nonlinear stochastic state-space model with unknown fixed parameters defining the outbreak's infectivity and recovery rate. We show that standard particle filters fail due to degeneracy of these fixed parameters and suggest the use of a kernel density approximation to the filtered distribution of the fixed parameters to allow their regeneration. In addition, we show that seemingly uninformative uniform priors on fixed parameters can affect posterior inferences and suggest the use of priors bounded only by the support of the parameter. We show the negative impact of using multinomial resampling and suggest the use of either stratified or residual resampling within the particle filter. Finally, we use this improved particle filtering methodology to relax prior assumptions of model parameters yet still providing reasonable estimates for model parameters and disease states. 
\end{abstract}


\begin{keyword}
Bayesian estimation; epidemiological modeling; state-space modeling; sequential Monte Carlo; particle filtering.
\end{keyword}

\end{frontmatter}

\let\thefootnote\relax\footnotetext{Abbreviations: BF - bootstrap filter; APF - auxiliary particle filter; KDPF - kernel density particle filter}

\section{Introduction} \label{sec:intro}
% Introduce syndromic surveillance
Syndromic surveillance systems \citep{henning2004overview, wagner2006biosurveillance, wilson2006synsurveillance, hakenewerth2009north, Gins:Mohe:Pate:Bram:Smol:Bril:dete:2009} are commonly used throughout the world to identify emerging disease outbreaks \citep{neill2006bayesian}, estimate their severity \citep{merl2009statistical}, and predict their duration \cite{ludkovski2010optimal}. Examples of these surveillance systems are emergency department visits for influenza-like illness, over-the-counter sales of cold and flu medicines, or poison control center calls due to gastrointestinal symptoms. A single disease outbreak may result in additional observations in some or all of these surveillance systems. 

State-space models are frequently used to simultaneously model the underlying disease dynamics and the observation process \citep{Mart:Cone:Lope:Lope:baye:2008,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring,unkel2012statistical}. Although the general form of these models may be known, the models will have unknown fixed parameters that define the disease dynamics and the observation process for this particular outbreak. Based on similar previous outbreaks, the range and likely values for these fixed parameters may be available. 

% Introduce Bayesian framework, MCMC
In statistical applications where prior knowledge or beliefs about unknown quantities are available, the Bayesian framework is often convenient for performing statistical analysis.  Bayesian inference is conducted through the posterior distribution of any unknown quantities given the observed data. However, the calculation of the posterior distribution frequently involves complicated integrals without an explicit analytical form. The most common approach to approximating these posterior distributions is Markov chain Monte Carlo (MCMC) \citep{Gelf:Smit:samp:1990}. In a sequential context, e.g. syndromic surveillance, MCMC is inefficient due to their entire MCMC needing to be rerun for each new observation and the computational expense increases with each new observation. Sequential Monte Carlo (SMC) - or particle filtering - methods enable on-line inference by updating the estimate of the posterior as new data become available. Furthermore, SMC methods can be flexible, general, easy to implement, and amenable to parallel computing. For a general introduction, please see \cite{Douc:deFr:Gord:sequ:2001} and \cite{cappe2007overview}.

% Introduce bootstrap filter, auxiliary particle filter
Early SMC methods, including the bootstrap filter \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996} and the auxiliary particle filter \citep{Pitt:Shep:filt:1999}, assumed all fixed parameters were known. A key defining step in these filters is the use of a resampling step which results in particles with low probability being eliminated and particles with high probability being duplicated. When all fixed parameters are known, these filters work remarkably well. In the presence of unknown fixed parameters, these filters suffer dramatically from a \emph{degeneracy} issue due to the fixed parameters (being treated as dynamic states with degenerate evolutions) never being regenerated and thus only a few distinct values for the fixed parameters remain after a few time points. To combat this degeneracy, a number of alternative approaches have been introduced. In this paper, we focus on the \emph{kernel density particle filter} \citep{Liu:West:comb:2001} due to its wide applicability, ease of implementation, and good performance. 

% Introduce layout of paper (see main document)
The rest of the article proceeds as follows. Section \ref{sec:ss} contains a description of state-space models and sequential estimation. Section \ref{sec:filtering} describes a variety of particle filtering methodologies including the kernel density approach of \cite{Liu:West:comb:2001}. In Section \ref{sec:apply}, we introduce a nonlinear dynamic model for a disease epidemic similar to the one used in \citet{skvortsov2012monitoring}. In Section \ref{sec:results}, we apply the particle filtering methods described in Section \ref{sec:filtering} to the model described in Section \ref{sec:apply}. In Section \ref{sec:extend}, we benefit from the efficiency of more recent particle filtering methods to estimate a more complicated model. In Section \ref{sec:discussion}, we conclude by reviewing more advanced particle filtering methods.

\section{State-space models \label{sec:ss}}

State-space models are a general class of statistical models used for analysis of dynamic data and have been used extensively in modeling disease outbreaks \citep{Mart:Cone:Lope:Lope:baye:2008,watkins2009disease,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring}.  State space models are constructed using an observation equation, $y_t \sim p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \sim p_{x,t}(x_t|x_{t-1},\theta)$ where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, and $\theta$ is an unknown fixed parameter, all of which could be vectors. The distributions are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only vary with $x_t$ and $\theta$ and therefore the $t$ subscript is dropped.
%The particle filtering methodology discussed in Sections \ref{sec:filtering} and \ref{sec:discussion} apply equally well when the distributions depend on $t$ so long as they are known.
For simplicity, we will also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the general state-space model is
\[ 
y_t \sim p(y_t|x_t,\theta) \qquad
x_t \sim p(x_t|x_{t-1},\theta).
\]
A fully specified Bayesian model is obtained by specifying the prior $p(x_0,\theta)$. 

Special cases of these state-space models include hidden Markov models \citep{cappe2005inference}, where the state $x_t$ has finite support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997}, where each distribution is Gaussian whose mean is a linear function of the states and whose variance does not depend on the mean. The disease outbreak models discussed in Section \ref{sec:apply} are specific cases of state-space models, but we introduce these models in generality here because the particle filtering methods discussed in Section \ref{sec:filtering} apply to any model with this form. 

\subsection{Sequential estimation \label {sec:sequential}}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, the distribution of the current state and parameters conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters. It can be updated recursively using Bayes' rule:
\begin{equation}
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$. Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically, e.g. in DLMs when $\theta$ is the observation variance \cite[Sec 4.3,][]{petris2009dynamic}. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. extended Kalman filter and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions such as particle filters.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is a sequential Monte Carlo (SMC) inferential technique based on repeated use of importance sampling. It aims to approximate the filtered distribution through a weighted Monte Carlo realization from this distribution, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j=1}^J w_t^{(j)} \delta_{\left(x_t^{(j)},\theta^{(j)}\right)} \label{eqn:approx}
\end{equation}
where $w_t^{(j)}$ is the particle \emph{weight} with $\sum_{j=1}^J w_t^{(j)}=1$, $(x_t^{(j)},\theta^{(j)})$ is the particle \emph{location}, and $\delta$ is the Dirac delta function. A variety of SMC techniques have been developed to provide more efficient approximations to equation \eqref{eqn:filtered} in the sense that with the same computation time a better approximation is achieved. We now introduce three fundamental particle filtering techniques: the bootstrap filter, auxiliary particle filter, and kernel density particle filter. In Section \ref{sec:results}, we will compare the efficiency of these techniques in the syndromic surveillance context. 

\subsection{Bootstrap filter \label{sec:bf}}

The first successful version of particle filtering is known as the bootstrap filter (BF) \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996}. Since this method and the auxiliary particle filter were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, to obtain an approximation to the filtered distribution at time $t+1$, perform the following steps for each particle $j=1,\ldots,J$:

\begin{enumerate}
\item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{w_t^{(1)},\ldots,w_t^{(j)},\ldots,w_t^{(J)}\right\}$ and set $w_t^{(j)} = 1 / J.$
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left( x_{t+1}\left|x_t^{(k)}\right.\right)$.
\item Calculate weights and renormalize: 
\[ \tilde{w}_{t+1}^{(j)} = w_t^{(k)}p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right) \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right. .\]
\end{enumerate}

\noindent This procedure can be applied recursively provided an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$, usually obtained by sampling from the prior. 

\subsection{Auxiliary particle filter \label{sec:apf}}

One problem that arises in implementing the BF is that $w_t^{(j)}$ will be small for particles where $p\left(y_{t}\left|x_{t}^{(j)}\right.\right)$ is small and these particles will contribute little to approximation of $p(x_{t}|y_{1:t})$. The auxiliary particle filter (APF) tries to anticipate which particles will have small weight using a look ahead strategy \citet{Pitt:Shep:filt:1999}. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the APF approximates $p(x_{t+1}|y_{1:t+1})$ by the following:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g.
\[ \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)} \right.\right). \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p(y_{t+1}|\mu_{t+1}^{(j)}) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^l.\right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$ and set $w_t^{(j)} = 1 / J$.
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)}\right.\right)$, and
	\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = w_t^{(j)}\frac{p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)}\right.\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent The point estimate used in Step 1 can be any point estimate although the expectation is common used. Step 3 is exactly the same as the BF with appropriate modifications to the weight calculation to adjust for the `look ahead' in Step 1. APF weights tend to be more uniform than BF weights which result in a better approximation to $p(x_{t}|y_{1:t})$. 

The BF and the APF were constructed with the idea that all fixed parameters were known. In order to simultaneously estimate the states and fixed parameters using these methods it is necessary to incorporate the fixed parameters into the state with degenerate evolutions, i.e. $\theta_{t+1}^{(j)}=\theta_t^{(j)}$ \danny{do we need to specify this?}. Due to the resampling step, the number of unique values of the fixed parameters in the particle set will decrease over time, resulting in \emph{degeneracy} in the fixed parameters \citep{Liu:West:comb:2001}.

\subsection{Kernel density particle filter \label{sec:kd}}

The particle filter introduced by \citet{Liu:West:comb:2001} is a general way of fighting this degeneracy problem by approximating the set of fixed parameter values by a kernel density estimate and then regenerating values from this approximation. We refer to this filter as the kernel density particle filter (KDPF). This filter provides an approximation to $p(x_t,\theta| y_{1:t})$ via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ for particle $j$. This does not imply that $\theta$ is dynamic, but rather that the particle can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of the posterior sample $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The KDPF uses a tuning parameter $\Delta$, the discount factor, and two derived quantities $h^2 = 1 - ((3\Delta - 1)/2\Delta)^2$ and $a^2 = 1 - h^2$ that determine how smooth the kernel density approximation is. Lower values of $\Delta$ result in a smoother approximation, but since we are simply interested in jittering the particles around $\Delta$ is typically taken to be between 0.95 and 0.99 \citep{Liu:West:comb:2001}.

Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the KDPF provides approximation of $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $\left(x_{t+1}^{(j)},\theta\right)$ given by $\left(\mu_{t+1}^{(j)},m_t^{(j)}\right)$ where
    \[
    \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right) \qquad
    m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t.
    \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^l. \right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$ and set $w_t^{(j)} = 1 / J$.
	\item Regenerate the fixed parameters:
	\[ \theta_{t+1}^{(j)} \sim N\left( m_t^{(k)}, h^2V_t \right), \]
	\item Propagate:
	\[ x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)},\theta_{t+1}^{(j)}\right.\right), \mbox{ and} \]
	\item Calculate weights and renormalize:
	\[ \tilde{w}_{t+1}^{(j)} = w_t^{(j)}\frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)},m_t^{(k)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent The KDPF adds the kernel density regeneration to the auxiliary particle filter. 

To use the KDPF with normal kernels it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$, and then the logit transformation can be applied. We discuss the sensitivity of the performance of the KDPF to the choice of transformation later on.

\subsection{Resampling \label{sec:advice}}

Practical implementation of any particle filter involves a choice of resampling scheme and a decision about when resampling should be performed. Throughout our discussion, we have explicitly used multinomial resampling, but alternative resampling schemes exist including stratified, residual, and systematic resampling. As discussed in Section \ref{sec:resample}, we suggest the use of stratified or residual sampling. In addition, the frequency of resampling should be reduced to balance the loss of information due to degeneracy with the loss of information due to the additional Monte Carlo variability introduced during resampling. Typically, a measure of the nonuniformity of particle weights is used including effective sample size, entropy, or coefficient of variation. For a discussion of these topics, please see \cite{Douc:Capp:Moul:comp:2005}.

\subsection{Theoretical justification of particle filters}

\danny{Should we put something in here with links to literature on theoretical justification of particle filters? Essentially the idea would be to state that all these methodologies are justified as the number of particles becomes large which is why in Section \ref{sec:results} we use an increasing number of particles.}

\section{Particle filtering in epidemiological models \label{sec:apply}}

We now describe how epidemiological modeling can be addressed via state-space models with sequential estimation as described in Section \ref{sec:ss}. Thereby we set the stage for application of the particle filtering methodology described in Section \ref{sec:filtering}. As in \citet{skvortsov2012monitoring}, we consider a modified SIR model with stochastic fluctuations describing the dynamics of the disease outbreak \citep{herwaarden1995stochepid, dangerfield2009stochepid, anderson2004sars}. According to this model, we track the population proportion that is susceptible ($s_t$), infectious ($i_t$), and recovered ($r_t$), i.e. no longer able to be infected, at time $t$. Mathematically, $s_t$, $i_t$, and $r_t$ are all nonnegative and $s_t + i_t + r_t = 1$ for all $t$.

When monitoring an epidemic, the true $s_t$, $i_t$ and $r_t$ are unknown and regarded as hidden states of the model. The observed data are gathered via syndromic surveillance. In our state-space model of a disease outbreak, the observation equation specifies how the observed data depend on the state of the epidemic and the state equation describes how the epidemic evolves over time.

\subsection{Model \label{sec:model}}

First, we describe the state equation. Let $x_t = (s_t,i_t)'$ denote the state of the epidemic at time $t$. Note that, by definition, $r_t$ is completely specified once $s_t$ and $i_t$ are known, and hence is not needed in the state vector. Our compartmental model of disease transmission is governed by three parameters:

\begin{itemize}
\item $\beta$, the contact rate of spread of disease,
\item $\gamma$, the recovery time from infection, and
\item $\nu$, the mixing intensity of the population.
\end{itemize}

\noindent $\beta$, $\gamma$, and $\nu$ are each restricted to be nonnegative. Define $\theta = (\beta,\gamma,\nu)'$ to be the vector of unknown parameters in our model, and let $P$ be the size of the population. Then, we describe the evolution of the epidemic from time $t$ to $t + \tau$ (for $\tau > 0$) by
\begin{equation}
x_{t+\tau}\left|x_t,\theta\right. \sim N_\Omega\left(f_\tau(x_t,\theta),Q_{\tau}(\theta)\right) \label{eqn:state}
\end{equation}
\noindent where
\[
f_\tau(x_t,\theta) = \left(
\begin{array}{c}
s_t - \tau\beta i_ts^{\nu}_t \phantom{- \tau\gamma i_t}\,\, \\
i_t +  \tau\beta i_ts^\nu_t - \tau\gamma i_t
\end{array}
\right)
\qquad
Q_\tau(\theta) = \frac{\beta \tau^2}{P^2} \left(
\begin{array}{ccccc}
1 & -1 \\
-1 & 1 + \gamma/\beta
\end{array}
\right)
\]

\noindent and $\Omega = \{(s_t,i_t): s_t \ge 0, i_t \ge 0, s_t + i_t \le 1\}$. $N_{\Omega}(\mu,\Sigma)$ represents the normal distribution with mean $\mu$ and covariance matrix $\Sigma$ truncated onto the set $\Omega$. %Notice that in equation \eqref{eqn:state}, the joint distribution of $s_t$ and $i_t$ has been truncated since they are proportions that must have sum no greater than 1. To generate values from this density during the `Propagate' step of a particle filter, we sample from the untruncated normal distribution, reject any samples outside of $\Omega$, and resample when needed.

%The dynamics of the epidemic with respect to time, $t$, are then described by the following differential equations:
%
%\begin{align}
%\frac{ds}{dt} &= -\beta is^\nu + \epsilon_\beta \label{eqn:dsdt} \\
%\frac{di}{dt} &= \beta is^\nu - \gamma i - \epsilon_\beta + \epsilon_\gamma \label{eqn:didt}
%\end{align}

%The standard deviations of the noise terms are approximated by
%\[\sigma_\beta \approx \frac{\sqrt{\beta}}{P} \mbox{, } \sigma_\gamma \approx \frac{\sqrt{\gamma}}{P}\]
%This comes from a well-known scaling rule of random fluctuations in the contact rate and recovery time \citep{ovaskainen2010extinction, herwaarden1995stochepid, dangerfield2009stochepid}.

We consider observed data that are positive real numbers related to counts of emergency room visits, prescription sales, or calls to a hotline, for example, and we can observe data from these different streams/sources asynchronously in time. That is, at any time $t$, we can observe data from any subset of the streams (or possibly none of them). Let $y_{l,t}$ represent data coming from stream $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$. We model the the log of the observations (so that $y_{l,t}$ is restricted to be positive) by
\begin{equation}
\log y_{l,t} \sim N\left(b_li_t^{\varsigma_l},\sigma_l^2\right) \label{eqn:obs}
\end{equation}
where $b_l$, $\varsigma_l$, and $\sigma_l$ are nonnegative constants \citep{skvortsov2012monitoring}. Initially, we assume $b_l$, $\varsigma_l$, and $\sigma_l$ are known for all $l$. In an extended analysis described in Section \ref{sec:extend}, we demonstrate that improved SMC methods enable us to regard these constants as unknown and include them in $\theta$.

Having formulated the data-generating model, we define $y_t = (y_{1,t},\ldots,y_{L,t})'$ and specify the likelihood of an observation $y_t$ given $x_t$ and $\theta$ - i.e. $p(y_t|x_t,\theta)$ - by $y_t|x_t,\theta \sim \log N(\mu_t,\Sigma_t)$, where $\mu_t$ is an $L$-length vector with element $l$ equal to $b_li_t^{\varsigma_l}$ and $\Sigma_t$ is an $L \times L$ diagonal matrix with the $l^{\mbox{th}}$ diagonal equal to $\sigma_l^2$. Elements of $y_t$ may be missing, in which case the dimension of $p(y_t|x_t,\theta)$ shrinks by the number of missing elements. %If the $j^{\mbox{th}}$ element of $y_t$ is missing, then the $j^{\mbox{th}}$ element of $\mu_t$ and $j^{\mbox{th}}$ row and column of $\Sigma_t$ are also missing and thus omitted in the calculation of the likelihood.
If all elements of $y_t$ are empty (i.e. if no syndromic data are observed at time $t$), particle weights are kept the same as they were at the previous time step.

%A further note on propagating particles is necessary.  Notice from equation \eqref{eqn:state} that the transition density needed to propagate $x_t$ in our model is a multivariate normal density with mean $f_\tau(x_t,\theta)$, and recall that the mean function approximates the disease dynamics better for small $\tau$.  Thus, for data observed at times $t_1$ and $t_2$ where the time between observations, $t_2 - t_1$, is large, the mean of $p(x_{t_2}|x_{t_1}^{(j)})$ for each particle $j$ may be inaccurate if $\tau$ is set to $t_2 - t_1$.  To ensure that the transition density matches the actual disease dynamics more closely, we can choose $d$ to be an integer larger than 1, set $\tau = (t_2 - t_1) / d$, and propagate $x_{t_1}^{(j)}$ forward $d$ times to generate $x_{t_2}^{(j)}$.

Since $p(x_{t+\tau}|x_t,\theta)$ and $p(y_t|x_t,\theta)$ are nonlinear functions of $x_t$ and $\theta$, a closed-form solution to the posterior $p(x_t,\theta|y_{1:t})$ cannot be obtained. Thus, we use the particle filtering techniques described in Section \ref{sec:filtering} to approximate $p(x_t,\theta|y_{1:t})$ for all $t$.

\section{Comparing particle filters} \label{sec:results}

We now compare the performance of the BF, APF and KDPF using simulated data analogous to that used in \citep{skvortsov2012monitoring}. In this section, we intend to demonstrate advantages in using KDPF over BF and APF when fixed parameters are included, advantages in using unbounded priors relative to bounded priors, and advantages in using stratified or residual resampling relative to multinomial resampling. 

\subsection{Simulation}

An epidemic lasting $T = 125$ days was simulated from our model for a population of size $P = 5000$. True values of parameters that we estimate (termed "unknown") were set to $\beta = 0.2399$, $\gamma = 0.1066$, and $\nu = 1.2042$, and values of known constants for $L = 4$ streams are given in Table \ref{tab:constants}. Infection was introduced in 10 people in the population at day 0 (i.e. true $i_0 = 10/5000$ and $s_0 = 4990/5000$). The epidemic evolved over time according to equation \eqref{eqn:state} (with $\tau = 1$) and data from randomly selected streams at each day were generated from equation \eqref{eqn:obs}. The simulated epidemic peaks around $t = 50$ days an results in around 80\% of the susceptible population becoming infected (see Figure \ref{fig:data}).

\begin{table}[hb]
\begin{center}
\caption{Values of known constants in model.}
\label{tab:constants}
\begin{tabular}{|cccc|}
\hline
$l$ & $b_l$ & $\varsigma_l$ & $\sigma_l$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\centering
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-epid}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-z}
\end{minipage}
\caption{Simulated epidemic curves (left) and syndromic observations (right).} \label{fig:data}
\end{figure}

\subsection{Particle filter runs} \label{sec:pf}

The BF, APF, and KDPF were each run on the simulated data using $J = 1000, 10000, 20000, \mbox{ and } 40000$ particles to obtain weighted sample approximations of $p(x_t,\theta|y_{1:t})$ for $t = 1,\ldots,T$. For each $J$, separate runs using multinomial, residual, stratified, and systematic resampling \danny{we haven't really introduced these} were implemented, and an effective sample size \danny{we haven't introduced this} threshold set at 80\% of the total number of particles was used to determine when to resample particles \citep{Liu:Chen:Wong:reje:1998}. For the KDPF, the discount factor $\Delta$ was set to 0.99 to determine the variance of the normal kernels used to approximate $p(\theta|y_{1:t})$. 

The initial state, $x_0$, and fixed parameters, $\theta_0$, were sampled from prior density $p(x_0,\theta) = p(\theta)p(i_0,s_0)$, where $p(i_0,s_0)$ is the joint pdf of the random variables $i_0$ and $s_0$ and $p(\theta) = p(\beta)p(\gamma)p(\nu)$. Since a very small percentage of the population is infected during the initial stage of the epidemic, we let $p(i_0) = N_{[0,1]}(i_0;0.002,0.0005^2)$ and set $s_0 = 1 - i_0$, i.e. no infected individuals have recovered from illness yet.

To investigate the impact of different prior distributions of $\theta$ on performance of the particle filters, the runs described above were performed once using uniform priors on $\theta$ and then again using log-normal priors. Uniform priors on $\theta$ were chosen to be the same as those used in \citet{skvortsov2012monitoring}, i.e. $p(\beta) \sim U[0.14, 0.50]$, $p(\gamma) \sim U[0.09, 0.143]$, and $p(\nu) \sim U[0.95,1.3]$. Log-normal priors on $\theta$ are given by $\beta \sim \ln N\left(-1.3296, 0.3248^2\right)$, $\gamma \sim \ln N\left(-2.1764, 0.1183^2\right)$, and $\nu \sim \ln N\left(0.1055, 0.0800^2\right)$. These log-normal priors constrain $\beta$, $\gamma$ and $\nu$ to be positive. The mean and variance of the log of the unknown parameters were chosen so that random draws on the original scale would fall within the uniform bounds with 95\% probability.

Logit and log transformations were applied to the components of $\theta$ in the manner described at the end of Section \ref{sec:kd} so that the normal kernel could be used in the KDPF while constraining $\beta$, $\gamma$, and $\nu$ to be within their respective prior domains (i.e. logit was used with uniform priors and log with log-normal priors). To aid in comparing particle filters, the same prior draws were used in the BF, APF, and KDPF as long as the number of particles and prior were the same. \danny{But isn't the prior different, e.g. uniform vs log-normal?}

\subsection{Comparison of particle filter algorithms under uniform priors} \label{sec:pfcomparison}

First, we compare the performance of the particle filtering algorithms using uniform priors on $\theta$ and systematic resampling, since these priors and resampling scheme were used in \citet{skvortsov2012monitoring}. Figure \ref{fig:pfs} shows 95\% credible bounds of $p(\beta|y_{1:t})$, $p(\gamma|y_{1:t})$, and $p(\nu|y_{1:t})$ for $t = 1,2,\ldots,T$ and $J = 1000, 10000, 20000, 40000$. Initially, bound for the BF and APF match those of KDPF, but quickly degenerate toward a single value due to elimination of unique particles during resampling. Although the time of degeneracy increases as $J$ gets larger, the BF and APF bounds become misleading during the second half of the epidemic even for $J = 40000$. The bounds for the KDPF, on the other hand, have dramatically reduced degeneracy since new values of $\theta$ are regenerated from the kernel density approximation.

The KDPF also has an advantage over the BF and APF in terms of computational efficiency. Notice that the bounds for the KDPF become wider as $J$ increases, but they do not change much for $J > 10000$. This suggests that by 20000 particles, the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ has converged to the true posterior over the entire epidemic period, unlike with the BF and APF. Even though the bounds for $\beta$ with the BF and APF seem to roughly match those of the KDPF for $J = 20000$ and $J = 40000$ over the first half of the epidemic, the KDPF provides the same measure of uncertainty even for $J = 10000$ and does not degenerate in the second half of the epidemic.

Also noteworthy is how the bounds for $\nu$ expand between $t = 70$ and $t = 80$ for $J > 1000$. We typically expect to see the width of credible intervals decrease monotonically over time as data is accumulated. A more plausible explanation is that the marginal filtered distribution of $\nu$ has been squeezed against the upper bound in the prior for $\nu$. If the distribution of $\nu$ was allowed to stray above 1.3, a similar plot would probably show a shift in the distribution toward lower values as opposed to the widening of the interval that we see in the figure (Figure \ref{fig:resamp}, which we discuss in Section \ref{sec:resample}, can attest to this \danny{Do you mean you actually ran the analysis? If yes, we should just state what we've seen and say `results not shown'.}). 

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-systematic-uniform-40000}
\caption{Sequential 95\% credible intervals for $\beta$ (left column), $\gamma$ (middle column), and $\nu$ (right column) for increasing number of particles (rows) for the BF (red), APF (blue), and KDPF (green) with the truth (black) when using systematic resampling and uniform priors.} \label{fig:pfs}
\end{figure}

\subsection{Comparison of uniform versus log-normal priors}

Next, we investigate the sensitivity of the KDPF with $J = 10000$ to using log-normal versus uniform priors on $\theta$.  Figure \ref{fig:priors} compares scatterplots of $\beta$ versus $\gamma$ sampled jointly from the filtered distribution $p(\beta,\gamma|y_{1:t})$ at $t = 15, 30, 45, 60$ for different prior distributions and transformations used on $\theta$. The first row illustrates that uniform priors with the logit transformation cause some particles to concentrate close to the prior bounds for $\gamma$, especially noticeable for $t \in \{15, 30\}$.  This truncation of points indicates that the prior bounds on $\gamma$ are too restrictive to account for the uncertainty in the recovery time during the climb of the epidemic, when more people are getting sick than recovering from illness. This problem does not present itself with $\beta$, however, because the uniform bounds for $\beta$ were chosen to be wide enough to capture the uncertainty about this parameter over the course of the epidemic.

We also see from the first row of Figure \ref{fig:priors} that the correlation between $\beta$ and $\gamma$ increases during the climb of the epidemic. In addition, points begin to form an S-shape at $t = 30$. This is a byproduct of the logit transformation extending values of $\gamma$ that are close to the boundary to the whole real line. To relax the restriction on $\gamma$ imposed by the prior bounds, we ran the particle filter again using the same prior draws as in the first row of Figure \ref{fig:priors}, but without any transformation on $\theta$. Scatterplots of $\beta$ versus $\gamma$ for this run are shown in the second row of Figure \ref{fig:priors}, and we see now that particles at $t = 15$ and $t = 30$ that would have been squeezed against the boundary by the logit transformation now lie outside of the uniform bounds for $\gamma$. There is no longer an S-shape in any of the graphs, and also noticeable is that the sample correlation between $\beta$ and $\gamma$ at $t = 60$ is larger than when the logit transformation was applied to $\theta$. \danny{perhaps say something about how the prior is introducing bias} %This suggests that the more drastic decrease in correlation between $t = 45$ and $t = 60$ in the first row is not driven by the data, but rather is an artifact of the restrictive uniform bounds enforced by the logit transformation.

Ideally, we'd like to apply some transformation to extend $\theta$ to the whole real line so that the KDPF operates smoothly, but we don't want the transformation to influence the particles in any way. The third row of Figure \ref{fig:priors} shows the KDPF run using log-normal priors with a log transformation on the components of $\theta$. As in the second row of Figure \ref{fig:priors}, the distribution of points do not appear truncated or S-shaped. While a majority of the points in the third row of Figure \ref{fig:priors} are within the uniform bounds, some lie outside because the transformation is not overly restrictive. We prefer log-normal priors on positive elements of $\theta$ because it allows us to use prior knowledge of the epidemic to encourage points to lie between the bounds at $t = 0$, but is more flexible in the event of model mis-specification and allows the data to play a greater role in dictating the evolution of $p(x_t,\theta|y_{1:t})$.

\begin{figure}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-uniform-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, logit transformation}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-semi-uniform-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, no transformation}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-lognormal-lognormal-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{normal prior draws, log transformation}
\end{minipage}

\caption{Scatterplots of $\beta$ (x-axis) versus $\gamma$ (y-axis) at $t = 15, 30, 45, 60$ days using the KDPF with $J = 10000$ particles and systematic resampling. Each row corresponds to a specific prior and transformation applied to $\theta$ in the KDPF. For demonstration, each panel shows 500 particles sampled from the weighted sample approximation of $p(x_t,\theta|y_{1:t})$. The sample correlation coefficient, $r$, displayed in the subtitle of each plot, is computed from the entire particle sample. Red crosses indicate the true values of $\beta$ and $\gamma$ used for simulation. Axes are the same in each panel. Dashed horizontal lines indicate the bounds of the uniform prior on $\gamma$ that was used in the top two rows. Uniform bounds on $\beta$ are not shown because they lie outside the range of the x-axis.} \label{fig:priors}

\end{figure}

\subsection{Comparison of resampling schemes \label{sec:resample}}

We turn to a comparison of different techniques for the resampling step of the KDPF with 10000 particles and log-normal priors on $\theta$.  Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy. The four resampling methods used in this paper \danny{so far we've only used one} achieve this at the cost of increasing the Monte Carlo variability of the particle sample. \danny{we should explain why this is the case. Easiest by imaging using multinomial resampling over and over with no new additional data} That is, additional variance is introduced into the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ due to resampling of particles. \citet{Douc:Capp:Moul:comp:2005} explains these four methods in detail and shows that 1) multinomial resampling introduces more Monte Carlo variability than does residual or stratified resampling, 2) residual and stratified resampling introduce the same amount of Monte Carlo variability, on average, and 3) systematic resampling can introduce more Monte Carlo variability than does multinomial resampling. \danny{now that I read this it would be nice if we could set up an example within the context of this model where systematic resampling does worse} 

We would like to choose the resampling scheme for which the filtered distribution, $p(x_t,\theta|y_{1:t})$, approaches the true posterior the fastest as a function of the number of particles. Figure \ref{fig:resamp} shows 95\% credible bounds of the marginal filtered distributions over time of the three unknown parameters (columns) as $J$ increases (rows) for each of the four different resampling schemes (lines). For our model, multinomial resampling appears to be outperformed by the other three resampling techniques, as the bounds for $\nu$ using 10000 or 20000 particles with multinomial resampling deviates from the true posterior (approximated by the white area in the plots) more than with the other three resampling techniques. This is also the case when looking at the plot for $\gamma$ at $J = 20000$. While systematic resampling seems to perform just as well as the remaining two resampling schemes, we prefer to use stratified resampling since, according to \cite{Douc:Capp:Moul:comp:2005}, we are then guaranteed to decrease the Monte Carlo variability in the particle sample, on average, relative to multinomial sampling.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-KD-normal-40000}
\caption{Sequential 95\% credible intervals for the three model parameters (columns) for increasing number of particles (rows) for multinomial (red), residual (green), stratified (blue), and systematic (light blue \danny{redo with different colors, red-green colorbindness and blue vs light blue}) using the KDPF with log-normal priors on $\theta$. White are is the time-wise average among the four resampling techniques using 40000 particles.} \label{fig:resamp}
\end{figure}

\subsection{A note on when to resample}

Regardless of resampling technique, it is inefficient to resample at every iteration of the particle filter because particle weights may not be out of balance. In this case, resampling would unnecessarily add Monte Carlo variability to the particle sample at more stages than needed. As mentioned in Section \ref{sec:pf}, a measure of nonuniformity of particle weights called effective sample size was used to determine when to resample for all runs in our study \citep{Liu:Chen:Wong:reje:1998}. This value can be interpreted as the number of independent particle samples, ranging between 1 and $J$, with $J$ corresponding to all particle weights being equal and 1 corresponding to one particle weight being 1 with the rest 0. We use a threshold of 0.8 in our runs, meaning that if the number of independent samples is less than 80\% of the total number of particles, resampling is performed. Other measures of nonuniformity include coefficient of variation and entropy. Future work is needed to determine which choice of nonuniformity measure and threshold level are optimal in certain cases.

\section{Additional Unknown Parameters \label{sec:extend}}

We now turn our focus to extending the analysis to include $\{b_l,\varsigma_l,\sigma_l:l\in1,\ldots,L\}$ as unknown parameters. In addition, we estimate another set of parameters that we add to the model, $\eta_l$ for $l = 1,2,\ldots,L$. The $\eta_l$'s can be interpreted as setting the baseline for the observed syndromic data from each stream. We alter equation \eqref{eqn:obs} to read
\begin{equation}
\log y_{l,t} \sim N\left(b_li_t^{\varsigma_l} + \eta_l,\sigma_l^2\right) \label{eqn:extended-obs}
\end{equation}
\noindent The analysis presented in Section \ref{sec:results} corresponds to $\eta_l$ assumed known and set to 0 for all $l$.

For this section, we consider data coming from only $L = 1$ stream and let $\theta = (\beta, \gamma, \nu, b, \varsigma, \sigma, \eta)'$, dropping the subscript $l$. Using the same simulated epidemic, data were simulated from equation \eqref{eqn:extended-obs} with true values of $b$, $\varsigma$, $\sigma$, and $\eta$ set to $0.25$, $1$, $0.001$ and $2$, respectively. Days at which data were observed from the single stream were randomly selected.

The KDPF with tuning parameter $\Delta$ set to 0.99 was run with $J = 40000$ particles, and stratified resampling was used with an effective sample size threshold of 0.8. As before, fixed parameter values were regenerated only when resampling was performed. $x_0$ and $\theta$ were sampled from the prior density given by
\[ p\left(x_0,\theta\right) = p\left(\beta\right)p\left(\gamma\right)p\left(\nu\right)p\left(b\right)p\left(\varsigma\right)p\left(\sigma\right)p\left(\eta\right)p\left(i_0,s_0\right) \]
\noindent with $p(i_0,s_0)$ defined as in Section \ref{sec:pf}. We used the log-normal priors for $p(\beta)$, $p(\gamma)$, and $p(\nu)$ as in Section \ref{sec:pf}. $p(b)$, $p(\varsigma)$, $p(\sigma)$, and $p(\eta)$ are specified by $\log\left(b\right) \sim  N\left(-1.6090, 0.3536^2\right)$, $\log\left(\varsigma\right) \sim N\left(-0.0114, 0.0771^2\right)$, $\log\left(\sigma\right) \sim N\left(-7.0516, 0.2803^2\right)$, and $\eta \sim N\left(2.5, 1\right)$. Log-normal priors restrict $b$, $\varsigma$, and $\sigma$ to be nonnegative, while $\eta$ is allowed to be any real number. The choice of prior mean and standard deviation on the log scale were made such that random draws of $b$, $\varsigma$, and $\sigma$ on the original scale would be within $(0.1, 0.4)$, $(0.85, 1.15)$, and $(0.0005, 0.0015)$, respectively, with 95\% probability. For comparison purposes, the KDPF with 40000 particles was also run with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed to be known at their true values used for simulating the data (we refer to this run as the original analysis).

Figure \ref{fig:ext} shows 95\% credible intervals over time for the marginal filtered distributions of each of the elements of $\theta$ and $x_t$ for both the extended (blue lines) and the original (red lines) analyses. Most noticeable from Figure \ref{fig:ext} is that the intervals for $\beta$, $\gamma$, $\nu$, $s_t$, and $i_t$ are wider for the extended analysis than they are for the original. This is due to the added uncertainty in $b$, $\varsigma$, $\sigma$, and $\eta$ in the extended analysis. Nonetheless, we are still able to obtain credible intervals for the unknown parameters that cover the true values and intervals for the states that cover the true epidemic curves by increasing the number of particles used in the KDPF.

%Nonetheless, we are still able to adequately estimate the epidemic curves even with less data available and imprecise knowledge of the parameters $b$, $\varsigma$, $\sigma$, and $\eta$ by increasing the number of particles used in the KDPF. In addition, we get a very precise estimate of $\eta$, the parameter that controls the baseline value of the syndromic observations.

We also notice from this figure that the lines appear choppier than in earlier plots, with flat periods followed by sharp changes in uncertainty. This is due to the fact that data are coming from only one stream, leading to more time points where no data are available and making the analysis more sensitive to abnormal data. Gaps in the data lead to a lack of resampling of particles and cause more drastic shifts in the posterior distribution of $\theta$ once data arrive. For example, a sharp shift in the distribution of $\beta$ occurs around $t = 25$ because of an influx of data following a period of no data and hence scarce resampling of particles. Also, we notice a spike in the $s$ and $i$ curves right before $t = 40$ because of a shift in the trajectory of data points.

Lastly, we comment on a widening of the credible intervals for $\nu$ in the extended analysis. This phenomenon suggests that the log-normal prior on $\nu$ that samples particles in $[0.9,1.3]$ is too tight, and that our model provides even less insight about this parameter than our prior belief. Scarce knowledge about $\nu$ is gained over the course of the epidemic in the original analysis due to the nonlinear nature of the evolution equation with respect to $\nu$, and we learn even less about $\nu$ in the extended analysis.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-ext-KD-stratified-normal-40000}
\caption{2.5\%/97.5\% quantiles of the marginal filtered distributions of the unknown parameters and states from the extended analysis (blue lines) over time compared with that from the original analysis (red lines). The KDPF with $J = 40000$ particles was run with stratified resampling. Tick marks are shown along the bottom of the plots for $\beta$ at time points when data were observed (dark gray) and when particles were resampled (blue and red for the extended and original analyses, respectively).} \label{fig:ext}
\end{figure}

\section{Discussion \label{sec:discussion}}

\input{discussion}

\clearpage

\bibliographystyle{model1-num-names}
\bibliography{jarad}

\end{document} 