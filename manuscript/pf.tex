\documentclass{elsarticle}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}

\graphicspath{{/Users/niemi/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/Users/niemi/Dropbox/SIR_Particle_Filtering/doesnotexist/}
                       %{/Users/Danny/"My Documents"/"UCSB - Research"/pf/graphs/}
                       {/Users/Danny/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/data/home/faculty/meiring/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {./Graphs/}
                       % Add your directories here
}

\newcommand{\jarad}[1]{{\color{red}JARAD: #1}}
\newcommand{\danny}[1]{{\color{blue}DANNY: #1}}

\begin{document}

\section{Introduction}

The rest of the article proceeds as follows. Section \ref{sec:model} contains a description of state-space models and the nonlinear dynamic model used in \citet{skvortsov2012monitoring}. Section \ref{sec:filtering} describes a variety of particle filtering methodologies including the kernel density approach of \cite{Liu:West:comb:2001}. In Section \ref{sec:apply}, we apply these methods to the \citet{skvortsov2012monitoring} model. In Section \ref{sec:extend}, we use the efficiency of these particle filtering methods to estimate a more complicated model. In Section \ref{sec:discussion}, we conclude with some additional particle filtering methods that may be helpful in certain model structures.

\section{State-space models \label{sec:model}}

State-space models are a general class of statistical models used for analysis of dynamic data in many fields including \jarad{references to uses of state-space models}. State space models are constructed using an observation equation, $y_t \stackrel{ind}{\sim} p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \stackrel{ind}{\sim} p_{x,t}(x_t|x_{t-1},\theta)$ where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, and $\theta$ is the unknown fixed parameter, all of which could be vectors. The distributions are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only vary with $x_t$ and $\theta$ and therefore the $t$ subscript is dropped.
%The particle filtering methodology discussed in Sections \ref{sec:filtering} and \ref{sec:discussion} apply equally well when the distributions depend on $t$ so long as they are known.
For simplicity, we will also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the state-space model is
\begin{align*}
y_t &\stackrel{ind}{\sim} p(y_t|x_t,\theta) \\
x_t &\stackrel{ind}{\sim} p(x_t|x_{t-1},\theta).
\end{align*}

Special cases of these state-space models include hidden Markov models \jarad{HMM reference}, where the state $x_t$ has finite support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997}, where the distributions are both Gaussian whose means are linear functions of the states.

\subsection{Sequential estimation}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, the distribution of the current state and parameter conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters. It can be updated recursively using Bayes' rule:
\begin{equation}
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$. Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically, e.g. in DLMs when $\theta$ is the observation variance \cite[Sec 4.3,][]{petris2009dynamic}. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. extended Kalman filter \jarad{ref} and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions, i.e. particle filters.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is a sequential Monte Carlo inferential technique based on sequential use of importance sampling. It aims to approximate equation \eqref{eqn:filtered} through a weighted Monte Carlo realization from this distribution, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j}^J w_t^{(j)} \delta_{(x_t^{(j)},\theta^{(j)})} \label{eqn:approx}
\end{equation}
where $w_t^{(j)},j=1,\ldots,J$ are the particle \emph{weights}, $(x_t,\theta)$ is the particle \emph{location}, and $\delta$ is a Dirac delta function. A variety of techniques have been developed to provide efficient approximations to equation \eqref{eqn:filtered} in the sense that with the same computation time a better approximation is achieved. We now introduce three fundamental particle filtering techniques: bootstrap filter, auxiliary particle filter, and the kernel density filter.

\subsection{Bootstrap Filter}

The first successful version of particle filtering is known as the bootstrap filter \citep{Gord:Salm:Smit:nove:1993}. Since this method and the auxiliary particle filter were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Suppose you have a current approximation as in equation \eqref{eqn:approx}. Then for each particle $j$,

\begin{enumerate}
\item Resample: sample an index $k\in \{1,\ldots,j,\ldots,J\}$ with probability $w_t^{(j)}$,
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(\left. x_{t+1}^{(k)}\right|x_t\right)$, and
\item Calculate weights: $\tilde{w}_{t+1}^{(j)} = p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)$.
\end{enumerate}

\noindent After weights for all particles have been calculated, these weights need to be renormalized via $w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right.$

This procedure provides an approximation to $p(x_t,\theta| y_{1:t})$ and therefore can be applied recursively provided an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$.

\subsection{Auxiliary Particle Filter}

A key problem that arises in implementing the bootstrap filter is that $w_t^{(j)}$ may be small for samples $x_t^{(j)}$ that are less likely given $y_t$. These samples will likely be eliminated during resampling, resulting in \emph{particle degeneracy}. One way to fight particle degeneracy is to increase the number of particles, $J$, used in the filter. A more efficient method, however, has been developed by \citet{Pitt:Shep:filt:1999} called the auxiliary particle filter.  The idea behind the auxiliary particle filter is to generate particles with higher likelihood given the data by "looking ahead" at $p(x_{t+1}|x_t^{(j)})$ prior to sampling. Given a weighted random sample of particles at time $t$, the auxiliary particle filter approximates $p(x_{t+1}|y_{1:t+1})$ by the following:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$.  %This is typically taken to be the mean of $p(x_{t+1}|x_t^{(j)})$.
\item Calculate auxiliary weights and renormalize
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p(y_{t+1}|\mu_{t+1}^{(j)}) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^l\right. \]
\item For each particle $j=1,\ldots,J$
	\begin{enumerate}
	\item Sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with probability $g_{t+1}^{(j)}$.
	\item Propagate the particle $x_{t+1}^{(j)}\sim p\left(x_{t+1}|x_t^{(k)}\right)$.
	\item Calculate new weights and renormalize
\[\tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}|x_{t+1}^{(j)}\right)}{p\left(y_{t+1}|\mu_{t+1}^{(k)}\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l \right.\]
	\end{enumerate}
\end{enumerate}

The bootstrap filter and the auxiliary particle filter were constructed with the idea that all fixed parameters were known. In order to simultaneously estimate the states and fixed parameters using these methods it is necessary to incorporate the fixed parameters into the state with degenerate evolutions. Due to the resampling step, the number of unique values of the fixed parameters in out particle set will decrease over time, exacerbating the degeneracy problem \citep{Liu:West:comb:2001} \jarad{more references}.

\subsection{Kernel Density Particle Filter}

The particle filter introduced by \cite{Liu:West:comb:2001} is a general way of fighting this degeneracy problem by approximating the set of fixed parameter values by a kernel density estimate and then resampling from this approximation at each step in the filter. This enables us to refresh the distribution of $\theta$ to avoid degeneracy and require a lower number of particles to efficiently run the filter. We will refer to this filter as the kernel density particle filter.

We now reintroduce the fixed parameter $\theta$ into the notation. The kernel density particle filter provides an approximation to the filtered distribution via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ in particle $j$. This does not imply that $\theta$ is dynamic, but rather that the particle can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of the posterior sample $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The kernel density particle filter also uses a tuning parameter $\delta$, the discount factor, and two derived quantities $h^2 = 1 - ((3\delta - 1)/2\delta)^2$ and $a^2 = 1 - h^2$ that determine how sharp the kernel density approximation is. Typically $\delta$ is taken to be between 0.95 and 0.99.

With the kernel density particle filter, we move to an approximation of $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $\left(x_{t+1}^{(j)},\theta\right)$ given by $\left(\mu_{t+1}^{(j)},m_t^{(j)}\right)$ where
    \[
    \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right) \qquad
    m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t
    \]
\item Calculate auxiliary weights and renormalize
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^l. \right. \]
\item For each particle $j=1,\ldots,J$
	\begin{enumerate}
	\item Sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with probability $g_{t+1}^{(j)}$.
	\item Regenerate the fixed parameters
	\[ \theta_{t+1}^{(j)} \sim N\left( m_t^{(k)}, h^2V_t \right). \]
	\item Propagate the particle
	\[ x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)},\theta_{t+1}^{(j)}\right.\right). \]
	\item Calculate weights and renormalize
	\[\tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)},m_t^{(k)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^l \right.\]
	\end{enumerate}
\end{enumerate}

To use the kernel density particle filter with normal kernels it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$ and then the logit transformation can be applied.

\subsection{General advice}

Practical implementation of any particle filter involves a choice of resampling function and a decision about when resampling should be performed. Throughout our discussion we have implicitly used multinomial resampling, i.e. whenever we say `Sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with probability $w$'. Alternatively, better resampling functions exist including stratified, residual, and systematic resampling. In addition, the frequency of resampling should be minimized to reduce Monte Carlo variability introduced during resampling. Typically a measure of the nonuniformity is used including effective sample size, entropy, and coefficient of variance. For a discussion of these topics please see \cite{Douc:Capp:Moul:comp:2005}.

\section{Particle filtering in epidemiological models \label{sec:apply}}

\subsection{Disease Dynamics}

As in \citet{skvortsov2012monitoring}, we consider the modified SIR model with stochastic fluctuations to describe the dynamics of the disease outbreak.  According to this model, we keep track of the number of susceptible, infected, and recovered individuals - denoted by $S$, $I$, and $R$, respectively - over the course of the epidemic.  We require $S + I + R = P$ where $P$ represents the total population size, and we define $s = S/P$, $i = I/P$, and $r = R/P$ so that $s + i + r = 1$.  The dynamics of the epidemic with respect to time, $t$, are then described by the following differential equations:

\begin{align}
\frac{ds}{dt} &= -\beta is^\nu + \epsilon_\beta \label{eqn:dsdt} \\
\frac{di}{dt} &= \beta is^\nu - \gamma i - \epsilon_\beta + \epsilon_\gamma \label{eqn:didt}
\end{align}

\noindent Here, $\beta$ represents the contact rate of spread of the disease, $\gamma$ is the recovery time, and $\nu$ is the mixing intensity of the population.  $\beta$, $\gamma$, and $\nu$ are each restricted to be non-negative.  $\epsilon_\beta$ and $\epsilon_\gamma$ are uncorrelated, Gaussian random variables with zero mean and standard deviations $\sigma_\beta$ and $\sigma_\gamma$, respectively.  Note that, by definition, $r$ is completely specified once $s$ and $i$ are known.

As in \citet{skvortsov2012monitoring} once again, we will approximate the standard deviation of the noise terms $\sigma_\beta$ and $\sigma_\gamma$ by the scaling rule \[\sigma_\beta \approx \frac{\sqrt{\beta}}{P} \mbox{, } \sigma_\gamma \approx \frac{\sqrt{\gamma}}{P}\] and use Euler's method to approximate the solution of the differential equations given in \eqref{eqn:dsdt} and \eqref{eqn:didt}.  Letting $s_t$ and $i_t$ be the proportion of the population susceptible to disease and infected by disease at time $t$, respectively, the evolution of the epidemic from time $t$ to time $t + \tau$, $\tau > 0$, is approximated by

\begin{align}
s_{t+\tau} &= s_t - \tau\beta i_ts^\nu_t + \xi_s \label{eqn:s} \\
i_{t+\tau} &= i_t + \tau i_t(\beta s^\nu_t - \gamma) + \xi_i \label{eqn:i}
\end{align}

\noindent where $\xi_s$ and $\xi_i$ are independent, Gaussian random variables with zero mean and variances $(\beta + \gamma)\tau^2/P^2$ and $\beta\tau^2/P^2$, respectively.  This approximation is more accurate for small $\tau$.

\subsection{Model}

When monitoring the epidemic, the true $s$, $i$ and $r$ are unknown and regarded as hidden states of the model.  We instead observe measurements consisting of public health data, medical data such as hospital visits, and non-medical data such as absenteeism from work or school \danny{reference?}.  Thus, we characterize a state-space model of the epidemic by two equations: the observation equation and the state equation.  The observation equation specifies how the observed data depend on the state of the epidemic and other parameters.  We can observe data from multiple syndromes of a disease that arrive asynchronously in time.  That is, at any time $t$, we can observe data from 0, 1, or more than 1 syndrome.

Let $z_{l,t}$ represent data coming from syndrome $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$.  \citet{skvortsov2012monitoring} assumes a power law model relating observations to the proportion of infected individuals by $z_{l,t} = b_li_t^{\varsigma_l} + \tau^*_l$, where $b_l \ge 0$ and $\varsigma_l$ are constants and $\tau^*_l$ is a random component with variance $\sigma_l^2$.  Since $z_{l,t}$ is constrained to be nonnegative, however, we let $y_{l,t} = \log z_{l,t}$ and model the log of the observations by

\begin{equation}
y_{l,t} = \log \left(b_li_t^{\varsigma_l}\right) + \tau_l \qquad \tau_l \sim N\left(0,\sigma_l^2 / (b_li_t^{\varsigma_l})^2\right) \label{eqn:obs}
\end{equation}

\noindent The variance of $\tau_l$ is obtained by assuming $\tau^*_l \sim N(0,\sigma_l^2)$ and applying the delta method to $z_{l,t}$ \danny{reference?}.  Then, we define $y_t = (y_{1,t},y_{2,t},\ldots,y_{L,t})'$ to be an $L$-length vector with independent elements, each of which could possibly be missing.

Next, we specify the state evolution equation.  The state equation explains how the unobserved states of the epidemic evolve over time.  Let $x_t = (i_t,s_t)'$ represent the state of the epidemic at time $t$, and let $\theta$ represent any unknown, fixed parameters in the model.  For the time being, we let $\theta = (\beta, \gamma, \nu)'$ and regard the $b_l$'s, $\varsigma_l$'s, and $\sigma_l$'s as known.  Letting $\tau > 0$ be small, the state evolution equation is given according to equations \eqref{eqn:s} and \eqref{eqn:i} by the transition density

\begin{equation}
p\left(x_{t+\tau}\left|x_t,\theta\right.\right) = N_{[0,1]\times[0,1]}\left(x_{t+\tau};f_\tau(x_t,\theta),Q_{\tau}(\theta)\right) \label{eqn:state}
\end{equation}

\noindent where

\[
f_\tau(x_t,\theta) = \left(
\begin{array}{c}
i_t + \tau i_t(\beta s^{\nu}_t - \gamma) \\
s_t - \tau\beta i_ts^{\nu}_t
\end{array}
\right)
\qquad
Q_\tau(\theta) = \left(
\begin{array}{ccccc}
(\beta + \gamma)\tau^2/P^2 & 0 \\
0 & \beta\tau^2/P^2
\end{array}
\right)
\]

\noindent and $N_{\Omega}(.;\mu,\Sigma)$ represents the pdf of the normal distribution with mean $\mu$ and covariance matrix $\Sigma$ truncated onto the set $\Omega$.  Notice that in equation \eqref{eqn:state}, the marginal distributions of $s$ and $i$ have been truncated onto $[0,1]$ since they are proportions and it is possible from equations \eqref{eqn:s} and \eqref{eqn:i} to generate values outside of $[0,1]$.

Having formulated the observation and state equations, we can now express the likelihood of an observation $y_t$ given $x_t$ and $\theta$ by

\begin{equation}
p\left(y_t\left|x_t,\theta\right.\right) = N\left(y_t;\mu_t,\Sigma_t\right) \label{eqn:lik}
\end{equation}

\noindent where $\mu_t$ is an $L$-length vector with element $l$ equal to $\log(b_li_t^{\varsigma_l})$ and $\Sigma_t$ is an $L \times L$ diagonal matrix with the $l^{\mbox{th}}$ diagonal equal to $\sigma_l^2 / (b_li_t^{\varsigma_l})^2$.  If the $j^{\mbox{th}}$ element of $y_t$ is missing, then the $j^{\mbox{th}}$ element of $\mu_t$ and $j^{\mbox{th}}$ row and column of $\Sigma_t$ are also missing and thus omitted in the calculation of the likelihood.

Since $p(x_{t+\tau}|x_t,\theta)$ and $p(y_t|x_t,\theta)$ are nonlinear functions of $x_t$ and $\theta$, a closed-form solution to the posterior $p(x_t,\theta|y_{1:t})$ cannot be obtained.  Thus, we use particle filtering techniques described in section \ref{sec:filtering} to approximate $p(x_t,\theta|y_{1:t})$ for all $t$.

\subsection{A Note on Propagation}

Notice from equation \eqref{eqn:state} that the transition density needed to propagate $x_t$ in our model is a multivariate normal density with mean $f_\tau(x_t,\theta)$, and recall that the mean function approximates the disease dynamics better for small $\tau$.  Thus, for data observed at times $t_1$ and $t_2$ where the time between observations, $t_2 - t_1$, is large, the mean of $p(x_{t_2}|x_{t_1}^{(j)})$ for each particle $j$ may be inaccurate if $\tau$ is set to $t_2 - t_1$.  To ensure that the transition density matches the actual disease dynamics more closely, we can choose $d$ to be an integer larger than 1, set $\tau = (t_2 - t_1) / d$, and propagate $x_{t_1}^{(j)}$ forward $d$ times to generate $x_{t_2}^{(j)}$.

\section{Results}

\subsection{Simulation Setup} \label{sec:sim}

An epidemic lasting $T = 125$ days was simulated from our model for a population of size $P = 5000$.  True values of unknown parameters were set to $\beta = 0.2399$, $\gamma = 0.1066$, and $\nu = 1.2042$, and values of known constants for $L = 4$ syndromes are given in table \ref{tab:true}.  Infection was introduced in 10 people in the population at day 0 (i.e. $i_0 = 10/5000$ and $s_0 = 4990/5000$).  Data were observed daily from the different syndromes asynchronously, i.e. at each day we can observe data from 0, 1, or more than 1 syndrome.  The epidemic evolved over time according to equation \eqref{eqn:state} with $\tau = 0.2$ days.

\begin{table}[ht]
\begin{center}
\caption{Values of known constants in model.}
\label{tab:true}
\begin{tabular}{|cccc|}
\hline
$l$ & $b_l$ & $\varsigma_l$ & $\sigma_l$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\end{center}
\end{table}

The bootstrap filter (BF), auxilliary particle filter (APF), and kernel density particle filter (KD) were each run using $J = 100, 1000, 10000, \mbox{ and } 20000$ particles to obtain weighted sample approximations of $p(x_t,\theta|y_{1:t})$ for $t = 1,\ldots,T$.  For a comparison of resampling schemes, this process was carried out using multinomial, residual, stratified, and systematic resampling.  For each resampling technique, an effective sample size threshold of 0.8 was used.  For the kernel density particle filter, the tuning parameter $\delta$ was set to 0.99.

$x_0$ and $\theta_0$ were sampled from the prior density given by

\[p\left(x_0,\theta\right) = p\left(\beta\right)p\left(\gamma\right)p\left(\nu\right)p\left(i_0,s_0\right)\]

\noindent where $p(i_0,s_0)$ is the joint pdf of the random variables $i_0$ and $s_0$ with
\[p\left(i_0\right) = N_{[0,1]}\left(i_0;0.002,0.00255^2\right) \qquad s_0 = 1 - i_0 \]
The prior on $i_0$ and $s_0$ comes from the thought that a very small percentage of the population is infected during the initial stage of an epidemic.  Also, $s_0 = 1 - i_0$ before any infected individuals have recovered from illness, and thus $i_0$ and $s_0$ can be sampled jointly.

To investigate the impact of different prior distributions of $\theta$ on performance, the particle filters described above were run using both uniform and normal priors on $\theta$.  Using uniform priors, $p(\beta)$, $p(\gamma)$, and $p(\nu)$ are given by

\begin{align*}
p(\beta) &= U_{[0.14, 0.50]}(\beta) \\
p(\gamma) &= U_{[0.09, 0.143]}(\gamma) \\
p(\nu) &= U_{[0.95,1.3]}(\nu)
\end{align*}

\noindent where $U_{[a,b]}(.)$ is the pdf of a uniform distribution on the interval $[a,b]$.  The uniform bounds given above are the same as those used in \citet{skvortsov2012monitoring}.  Using normal priors, $p(\beta)$, $p(\gamma)$, and $p(\nu)$ are specified by

\begin{align*}
\log(\beta) &\sim  N(-1.3296, 0.3248^2) \\
\log(\gamma) &\sim N(-2.1764, 0.1183^2) \\
\log(\nu) &\sim N(0.1055, 0.0800^2)
\end{align*}

\noindent The normal priors are specified on the log scale so that $\beta$, $\gamma$, and $\nu$ are constrained to be positive.  The mean and variance of the log of the unknown parameters were chosen so that random draws on the original scale would fall within the uniform bounds with 95\% probability.  To aid comparison of the particle filters, the same prior draws were used for the BF, APF, and KD particle filters, as long as $J$ and the type of prior (normal or uniform) were kept constant.

\subsection{Comparison of Particle Filters}

First, we examine the performance of the three different particle filtering algorithms using uniform priors on $\theta$ and systematic resampling, since this choice of prior and resampling scheme was used in \citet{skvortsov2012monitoring}.  Figure \ref{fig:unisys} shows 95\% credible bounds of the marginal posterior distributions of the unknown parameters over time as $J$ increases.  The bounds for the BF (red lines) and APF (blue lines) start out wide and then eventually degenerate toward a single value because of the elimination of unique particles during resampling.  Although the time of degeneracy increases as $J$ gets larger, the bounds become uninformative during the second half of the epidemic even for $J = 20000$.  The bounds for the KD particle filter (green lines), on the other hand, never degenerate because values of $\theta$ are resampled from the normal density kernel at each iteration of the particle filter.  

Not only does the KD particle filter outperform the BF and APF in terms of avoiding degeneracy, but it runs more efficiently as well.  Notice that the bounds for the KD particle filter become wider as $J$ increases, but they do not change much between $J = 10000$ and $J = 20000$ particles.  This suggests that by 10000 particles the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ has converged to the true posterior.  Thus, even though the bounds of the APF approximately match that of the KD particle filter for the first 25 days of the epidemic at 20000 particles, the KD particle filter provides roughly the same measure of uncertainty using only 10000 particles.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-systematic-uniform}
\caption{Displays 2.5\%/97.5\% quantiles of the filtered distributions over time of the unknown parameters (columns) for increasing number of particles (rows).  Uniform priors and systematic resampling were used for all particle filter runs, and the scale of the plot axes are identical within columns.} \label{fig:unisys}
\end{figure}

\subsection{Comparison of Priors}

Next, we investigate the effect of normal versus uniform priors on $\theta$ for the KD particle filter using 10000 particles and systematic resampling.  Figure \ref{fig:priors} shows scatterplots of $\beta$ versus $\gamma$ taken from their joint filtered distribution at $t = 15, 30, 45, 60$.  Notice in the top half of the figure that when uniform priors on $\theta$ are used, some particles are pushed up against the top and bottom of the graphs for $t = 15 \mbox{ and } 30$, indicating that the uniform bounds on $\gamma$ are too restrictive.  This problem does not present itself with $\beta$ because during the first half of the epidemic more people are getting sick than recovering from illness.  Hence, we learn more about $\beta$, the rate of spread of disease, than $\gamma$, the recovery time.
 
We see from both the top and bottom rows of the figure that even though $\beta$ and $\gamma$ are sampled independently at $t = 0$, they become correlated during the climb of the epidemic, suggesting that knowledge of $\beta$ provides information about $\gamma$ and vice versa.  However, when uniform priors are used we lose some of this correlation by $t = 60$ while the correlation remains strong under normal priors. The loss of correlation in the top row of the figure appears to be an artifact of the truncation of sampled $\gamma$ values that result from the over-restrictive uniform bounds.  We prefer to use normal priors in order to relax the constraint on the sample space of $\theta$ and allow the filtered distribution of $\theta$ to evolve over time more freely.

\begin{figure}
\centering
\begin{minipage}{1.0\linewidth}
\caption*{uniform priors}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-systematic-10000-betagamma}
\end{minipage}
\begin{minipage}{1.0\linewidth}
\caption*{normal priors}
\includegraphics[width=1.0\textwidth]{Hist-KD-normal-systematic-10000-betagamma}
\end{minipage}
\caption{Displays scatterplots of $\beta$ versus $\gamma$ at $t = 15, 30, 45, 60$ days using the kernel density particle filter with $J = 10000$ particles and systematic resampling.  For each panel, 500 particles were sampled from the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ and plotted.  Red crosses indicate the true values of $\beta$ and $\gamma$ used for simulation ($\beta = 0.2399$ and $\gamma = 0.1066$).  Plot axes are all on the same scale, and dashed horizontal lines indicate the bounds of the uniform prior on $\gamma$.  Uniform bounds on $\beta$ are not shown because they lay outside the range of the x-axis.} \label{fig:priors}
\end{figure}

\subsection{Comparison of Resampling Schemes}

Now preferring to run the KD particle filter with 10000 particles using normal priors on the unknown parameters, we turn to a comparison of different techniques for the resampling step of the particle filtering algorithm.  Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy.  The four resampling methods used in this paper - multinomial, residual, stratified, and systematic - achieve this at the cost of increasing the Monte Carlo variability of the particle sample.  That is, additional variance is introduced into the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ due to resampling of particles.  \citet{Douc:Capp:Moul:comp:2005} explains these four methods in detail and determines the following:

\begin{enumerate}
\item Multinomial resampling introduces more Monte Carlo variability than do residual and stratified resampling.
\item Residual and stratified resampling introduce the same amount of Monte Carlo variability on average.
\item Systematic resampling may introduce more or less Monte Carlo variability than does multinomial resampling.
\end{enumerate}

Figure \ref{fig:resamp} shows 95\% credible bounds of the marginal filtered distributions of the unknown parameters as $J$ increases for the four different resampling schemes.  Ideally, the resampling scheme that performed the best would stand out in that the marginal particle filtered distribution of $\theta$ approaches the true posterior (approximated by the white area in the plots) the fastest with increasing $J$.  Noticing in particular at the panel for $\nu$ at $J = 10000$, multinomial resampling appears to be outperformed by the other three methods.  However, it is difficult to discern which of the remaining three methods performs the best.  We prefer to use stratified resampling since, according to \cite{Douc:Capp:Moul:comp:2005}, systematic resampling can perform worse than multinomial in certain cases and stratified resampling is guaranteed to decrease the Monte Carlo variability in the particle samples, on average, relative to multinomial sampling.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-KD-normal}
\caption{Displays 2.5\%/97.5\% quantiles of the filtered distributions over time of the unknown parameters (columns) for increasing number of particles (rows), color coded by resampling technique.  Normal priors on $\theta$ and the KD particle filter were used for all runs.  Plot axes are identical within columns, and areas shaded gray are outside of 95\% credible bounds calculated by taking the average quantiles of the four resampling techniques at $J = 20000$ particles.} \label{fig:resamp}
\end{figure}

\section{Extending the model \label{sec:extend}}

We now turn our focus to extending the model to include the $b_l$'s, $\varsigma_l$'s, and $\sigma_l$'s as unknown parameters.  We consider data coming from just $L = 1$ syndrome and let $\theta = (\beta, \gamma, \nu, b, \varsigma, \sigma)'$, dropping the subscript $l$ for convenience.  Data were simulated from the model with true values set at $b = 0.25$, $\varsigma = 1$, $\sigma = 0.001$, and the remaining parameters set at the same values as in section \ref{sec:sim}.  Again, we let the epidemic run for $T = 125$ days in a population of size $P = 5000$ with infection introduced in 10 people at day 0.  Date were observed daily from either 1 or 0 syndromes, and the epidemic evolved over time according to equation \eqref{eqn:state} with $\tau = 0.2$ days.

The KD particle filter with tuning parameter $\delta$ set to 0.99 was run with $J = 10000$ particles, and stratified resampling was used with an effective sample size threshold of 0.8.  $x_0$ and $\theta$ were sampled from the prior density given by

\[p\left(x_0,\theta\right) = p\left(\beta\right)p\left(\gamma\right)p\left(\nu\right)p\left(b\right)p\left(\varsigma\right)p\left(\sigma\right)p\left(i_0,s_0\right)\]

\noindent where $p(i_0,s_0)$, $p(\beta)$, $p(\varsigma)$, and $p(\sigma)$ are defined the same way as in section \ref{sec:sim}.  $p(b)$, $p(\varsigma)$, and $p(\sigma)$ are defined by

\begin{align*}
\log(b) &\sim  N(-1.609, 0.3536^2) \\
\log(\varsigma) &\sim N(-0.0114, 0.0771^2) \\
\log(\sigma) &\sim N(-7.0516, 0.2803^2)
\end{align*}

As with the other three parameters, the prior distributions of $b$, $\varsigma$, and $\sigma$ are defined on the log scale so that they are constrained to be positive.  The choice of prior mean and standard deviation on the log scale were made such that random draws of $b$, $\varsigma$, and $\sigma$ on the original scale would be within $(0.1, 0.4)$, $(0.85, 1.15)$, and $(0.0005, 0.0015)$, respectively, with 95\% probability.  For comparison purposes, the KD particle filter with 10000 particles was also run assuming $b$, $\varsigma$, and $\sigma$ known. 

Figure \ref{fig:ext} shows 95\% credible intervals over time for marginal filtered distributions of $\theta$ and $x$ for both the extended model and the original model that assumes $b$, $\varsigma$, and $\sigma$ known.  The intervals for $s$, $i$, and $r$ are wider for the extended model (green lines) than for the original model due to the uncertainty in $b$, $\varsigma$, and $\sigma$.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-ext-KD-stratified-normal-10000}
\caption{Displays 2.5\%/97.5\% quantiles of the filtered distributions of the unknown states and parameters of the extended model (green) over time compared with that of the model assuming $b$, $\varsigma$, and $\sigma$ known.  $J = 10000$ particles for the KD particle filter were used with normal priors on $\theta$ and stratified resampling.} \label{fig:ext}
\end{figure}

\section{Discussion \label{sec:discussion}}

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document} 