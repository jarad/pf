\documentclass{elsarticle}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}

\graphicspath{{/Users/niemi/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/Users/niemi/Dropbox/SIR_Particle_Filtering/doesnotexist/}
                       {/Users/Danny/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {/data/home/faculty/meiring/Dropbox/SIR_Particle_Filtering/Graphs/}
                       {./Graphs/}
                       % Add your directories here
}

\newcommand{\jarad}[1]{{\color{red}JARAD: #1}}
\newcommand{\danny}[1]{{\color{blue}DANNY: #1}}

\journal{Mathematical Biosciences}

\begin{document}

\begin{frontmatter}

\title{Estimation of a disease epidemic using particle filtering}

\author[danny]{Daniel M. Sheinson}
\author[jarad]{Jarad Niemi}
\author[wendy]{Wendy Meiring}

\address[danny]{Corresponding author -- Department of Statistics and Applied Probability, University of Califonia--Santa Barbara, \\
 Santa Barbara, CA, U.S.A., sheinson@pstat.ucsb.edu, 1-847-609-7824}
 \address[jarad]{Department of Statistics, Iowa State University, Ames, IA, U.S.A., niemi@iastate.edu}
 \address[wendy]{Department of Statistics and Applied Probability, University of Califonia--Santa Barbara, \\
 Santa Barbara, CA, U.S.A., meiring@pstat.ucsb.edu}

\begin{abstract}
This paper presents methodology for tracking and prediction of a disease outbreak via a syndromic surveillance system. We present a nonlinear stochastic state-space model with unknown fixed parameters defining the outbreak's infectivity and recovery rate. We show that standard particle filters fail due to degeneracy of these fixed parameters and suggest the use of a kernel density approximation to the filtered distribution of the fixed parameters to allow their regeneration. In addition, we show that seemingly uninformative uniform priors on fixed parameters can affect posterior inferences and suggest the use of priors bounded only by the support of the parameter. We show the negative impact of using multinomial resampling and suggest the use of either stratified or residual resampling within the particle filter. Finally, we use this improved particle filtering methodology to relax prior assumptions of model parameters yet still provide reasonable estimates for model parameters and disease states.
\end{abstract}


\begin{keyword}
Bayesian estimation; epidemiological modeling; state-space modeling; sequential Monte Carlo; particle filtering.
\end{keyword}

\end{frontmatter}

\let\thefootnote\relax\footnotetext{Abbreviations: BF - bootstrap filter; APF - auxiliary particle filter; KDPF - kernel density particle filter}

\section{Introduction} \label{sec:intro}
% Introduce syndromic surveillance
Syndromic surveillance systems \citep{henning2004overview, wagner2006biosurveillance, wilson2006synsurveillance, hakenewerth2009north, Gins:Mohe:Pate:Bram:Smol:Bril:dete:2009} are commonly used throughout the world to identify emerging disease outbreaks \citep{neill2006bayesian}, estimate their severity \citep{merl2009statistical}, and predict their duration \cite{ludkovski2010optimal}. Examples of these surveillance systems are emergency department visits for influenza-like illness, over-the-counter sales of cold and flu medicines, or poison control center calls due to gastrointestinal symptoms. A single disease outbreak may result in additional observations in some or all of these surveillance systems.

State-space models are frequently used to simultaneously model the underlying disease dynamics and the observation process \citep{Mart:Cone:Lope:Lope:baye:2008,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring,unkel2012statistical}. Although the general form of these models may be known, the models will have unknown fixed parameters that define the disease dynamics and the observation process for this particular outbreak. Based on similar previous outbreaks, the range and likely values for these fixed parameters may be available.

% Introduce Bayesian framework, MCMC
In statistical applications where prior knowledge or beliefs about unknown quantities are available, the Bayesian framework is often convenient for performing statistical analysis.  Bayesian inference is conducted through the posterior distribution of any unknown quantities given the observed data. However, the calculation of the posterior distribution frequently involves complicated integrals without an explicit analytical form. The most common approach to approximating these posterior distributions is Markov chain Monte Carlo (MCMC) \citep{Gelf:Smit:samp:1990}. In a sequential context, e.g. syndromic surveillance, MCMC is inefficient due to the need for the entire MCMC to be rerun for each new observation and subsequent increase in computational cost. Sequential Monte Carlo (SMC) - or particle filtering - methods enable on-line inference by updating the estimate of the posterior as new data become available. Furthermore, SMC methods can be flexible, general, easy to implement, and amenable to parallel computing. For a general introduction, please see \cite{Douc:deFr:Gord:sequ:2001} and \cite{cappe2007overview}.

% Introduce bootstrap filter, auxiliary particle filter
Early SMC methods, including the bootstrap filter \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996} and the auxiliary particle filter \citep{Pitt:Shep:filt:1999}, assumed all fixed parameters were known. A key defining step in these filters is the use of resampling, which results in particles with low probability being eliminated and particles with high probability being duplicated. When all fixed parameters are known, these filters work remarkably well. In the presence of unknown fixed parameters, these filters suffer dramatically from a \emph{degeneracy} issue due to the fixed parameters (being treated as dynamic states with degenerate evolutions) never being regenerated, and thus only a few distinct values for the fixed parameters remain after a few time points. To combat this degeneracy, a number of alternative approaches have been introduced. In this paper, we focus on the \emph{kernel density particle filter} \citep{Liu:West:comb:2001} due to its wide applicability, ease of implementation, and good performance.

% Introduce layout of paper (see main document)
The rest of the article proceeds as follows. Section \ref{sec:ss} contains a description of state-space models and sequential estimation. Section \ref{sec:filtering} describes a variety of particle filtering methodologies including the kernel density approach of \cite{Liu:West:comb:2001}. In Section \ref{sec:apply}, we introduce a nonlinear dynamic model for a disease epidemic similar to the one used in \citet{skvortsov2012monitoring}. In Section \ref{sec:results}, we apply the particle filtering methods described in Section \ref{sec:filtering} to the model described in Section \ref{sec:apply}. In Section \ref{sec:extend}, we benefit from the efficiency of more recent particle filtering methods to estimate a more complicated model. In Section \ref{sec:discussion}, we conclude by reviewing more advanced particle filtering methods.

\section{State-space models \label{sec:ss}}

State-space models are a general class of statistical models used for analysis of dynamic data and have been used extensively in modeling disease outbreaks \citep{Mart:Cone:Lope:Lope:baye:2008,watkins2009disease,merl2009statistical,ludkovski2010optimal,skvortsov2012monitoring}.  State space models are constructed using an observation equation, $y_t \sim p_{y,t}(y_t|x_t,\theta)$, and a state evolution equation, $x_t \sim p_{x,t}(x_t|x_{t-1},\theta)$ where $y_t$ is the observed response, $x_t$ is a latent, dynamic state, and $\theta$ is an unknown fixed parameter, all of which could be vectors. The distributions are assumed known conditional on the values of $\theta$ and $x_t$ in the observation equation and $\theta$ and $x_{t-1}$ in the evolution equation. Depending on whether the observations and the states are continuous or discrete, the distributions themselves may be continuous or discrete. The distributions are typically assumed to only vary with $x_t$ and $\theta$ and therefore the $t$ subscript is dropped.
%The particle filtering methodology discussed in Sections \ref{sec:filtering} and \ref{sec:discussion} apply equally well when the distributions depend on $t$ so long as they are known.
For simplicity, we will also drop the $x$ and $y$ subscript and instead let the arguments make clear which distribution we are referring to. Thus, the general state-space model is
\[
y_t \sim p(y_t|x_t,\theta) \qquad
x_t \sim p(x_t|x_{t-1},\theta).
\]
A fully specified Bayesian model is obtained by specifying the prior $p(x_0,\theta)$.

Special cases of these state-space models include hidden Markov models \citep{cappe2005inference}, where the state $x_t$ has finite support, and dynamic linear models (DLMs) \citep{West:Harr:baye:1997}, where each distribution is Gaussian whose mean is a linear function of the states and whose variance does not depend on the mean. The disease outbreak models discussed in Section \ref{sec:apply} are specific cases of state-space models, but we introduce these models in generality here because the particle filtering methods discussed in Section \ref{sec:filtering} apply to any model with this form.

\subsection{Sequential estimation \label {sec:sequential}}

When data are collected sequentially, it is often of interest to determine the \emph{filtered distribution}, the distribution of the current state and parameters conditional on the data observed up to that time. This distribution describes all of the available information up to time $t$ about the current state of the system and any fixed parameters. It can be updated recursively using Bayes' rule:
\begin{equation}
p(x_t,\theta| y_{1:t}) \propto p(y_t|x_t,\theta)p(x_t,\theta|y_{1:t-1}) \label{eqn:filtered}
\end{equation}
where $y_{1:t} = (y_1,\ldots,y_t)$. Only in special cases can $p(x_t,\theta| y_{1:t})$ be evaluated analytically, e.g. in DLMs when $\theta$ is the observation variance \cite[Sec 4.3,][]{petris2009dynamic}. When analytical tractability is not present, we turn to numerical methods including deterministic versions, e.g. extended Kalman filter and the Gaussian sum filter \citep{Alsp:Sore:nonl:1972}, or Monte Carlo versions such as particle filters.

\section{Particle filtering \label{sec:filtering}}

Particle filtering is a sequential Monte Carlo (SMC) inferential technique based on repeated use of importance sampling. It aims to approximate the filtered distribution through a weighted Monte Carlo realization from this distribution, i.e.
\begin{equation}
p(x_t,\theta| y_{1:t}) \approx \sum_{j=1}^J w_t^{(j)} \delta_{\left(x_t^{(j)},\theta^{(j)}\right)} \label{eqn:approx}
\end{equation}
where $w_t^{(j)}$ is the particle \emph{weight} with $\sum_{j=1}^J w_t^{(j)}=1$, $(x_t^{(j)},\theta^{(j)})$ is the particle \emph{location}, and $\delta$ is the Dirac delta function. A variety of SMC techniques have been developed to provide more efficient approximations to equation \eqref{eqn:filtered} in the sense that with the same computation time a better approximation is achieved. We now introduce three fundamental particle filtering techniques: the bootstrap filter, auxiliary particle filter, and kernel density particle filter. In Section \ref{sec:results}, we will compare the efficiency of these techniques in the syndromic surveillance context.

\subsection{Bootstrap filter \label{sec:bf}}

The first successful version of particle filtering is known as the bootstrap filter (BF) \citep{Gord:Salm:Smit:nove:1993,Kita:mont:1996}. Since this method and the auxiliary particle filter were developed for the situation when $\theta$ is known, we will (for the moment) drop $\theta$ from the notation. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, to obtain an approximation to the filtered distribution at time $t+1$, perform the following steps for each particle $j=1,\ldots,J$:

\begin{enumerate}
\item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\left\{w_t^{(1)},\ldots,w_t^{(j)},\ldots,w_t^{(J)}\right\}$,
\item Propagate: sample $x_{t+1}^{(j)} \sim p\left( x_{t+1}\left|x_t^{(k)}\right.\right)$.
\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right) \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{w}_{t+1}^{(l)} \right. .\]
\end{enumerate}

\noindent This procedure can be applied recursively provided an initial set of weights $w_0^{(j)}$ and locations $x_0^{(j)}$ for all $j$, usually obtained by sampling from the prior.

\subsection{Auxiliary particle filter \label{sec:apf}}

One problem that arises in implementing the BF is that $w_t^{(j)}$ will be small for particles where $p\left(y_{t}\left|x_{t}^{(j)}\right.\right)$ is small and these particles will contribute little to the approximation of $p(x_{t}|y_{1:t})$. The auxiliary particle filter (APF) tries to anticipate which particles will have small weight using a look ahead strategy \citep{Pitt:Shep:filt:1999}. Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the APF approximates $p(x_{t+1}|y_{1:t+1})$ by the following:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $x_{t+1}^{(j)}$ called $\mu_{t+1}^{(j)}$, e.g.
\[ \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)} \right.\right). \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p(y_{t+1}|\mu_{t+1}^{(j)}) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{g}_{t+1}^{(l)}.\right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$,
	\item Propagate: sample $x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)}\right.\right)$, and
	\item Calculate weights and renormalize:
\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)}\right.\right)} \qquad w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent The point estimate used in Step 1 can be any point estimate, although the expectation is commonly used. Step 3 is exactly the same as the BF with appropriate modifications to the weight calculation to adjust for the `look ahead' in Step 1. APF weights tend to be more uniform than BF weights, which results in a better approximation of $p(x_{t}|y_{1:t})$.

The BF and the APF were constructed with the idea that all fixed parameters were known. In order to simultaneously estimate the states and fixed parameters using these methods, it is necessary to incorporate the fixed parameters into the state with degenerate evolutions. That is, we would regard the fixed parameters as elements of the state vector $x_t$ and specify the state evolution equation such that these elements would not change over time. Due to the resampling step, the number of unique values of the fixed parameters in the particle set will decrease over time, resulting in \emph{degeneracy} in the fixed parameters \citep{Liu:West:comb:2001}.

\subsection{Kernel density particle filter \label{sec:kd}}

The particle filter introduced by \citet{Liu:West:comb:2001} is a general way of fighting this degeneracy problem by approximating the set of fixed parameter values by a kernel density estimate and then regenerating values from this approximation. We refer to this filter as the kernel density particle filter (KDPF). This filter provides an approximation to $p(x_t,\theta| y_{1:t})$ via equation \eqref{eqn:approx}. To make the notation transparent, we introduce subscripts for our fixed parameters, e.g. $\theta_t^{(j)}$ represents the value for $\theta$ at time $t$ for particle $j$. This does not imply that $\theta$ is dynamic, but rather that the particle can have different values for $\theta$ throughout time.

Let $\bar{\theta}_t$ and $V_t$ be the weighted sample mean and weighted sample covariance matrix of the posterior sample $\theta_t^{(1)},\ldots,\theta_t^{(J)}$.  The KDPF uses a tuning parameter $\Delta$, the discount factor, and two derived quantities $h^2 = 1 - ((3\Delta - 1)/2\Delta)^2$ and $a^2 = 1 - h^2$ that determine how smooth the kernel density approximation is. Lower values of $\Delta$ result in a smoother approximation, but since we are simply interested in jittering the particles around, $\Delta$ is typically taken to be between 0.95 and 0.99 \citep{Liu:West:comb:2001}.

Given an approximation to the filtered distribution at time $t$ as in equation \eqref{eqn:approx}, the KDPF provides an approximation of $p(x_{t+1},\theta|y_{1:t+1})$ by the following steps:

\begin{enumerate}
\item For each particle $j$, calculate a point estimate of $\left(x_{t+1}^{(j)},\theta\right)$ given by $\left(\mu_{t+1}^{(j)},m_t^{(j)}\right)$ where
    \[
    \mu_{t+1}^{(j)} = E\left(x_{t+1}\left|x_t^{(j)},\theta_t^{(j)} \right.\right) \qquad
    m_t^{(j)} = a\theta_t^{(j)} + (1-a)\bar{\theta}_t.
    \]
\item Calculate auxiliary weights and renormalize:
\[ \tilde{g}_{t+1}^{(j)} = w_t^{(j)} p\left(y_{t+1}\left|\mu_{t+1}^{(j)},m_t^{(j)}\right.\right) \qquad g_{t+1}^{(j)} = \tilde{g}_{t+1}^{(j)}\left/ \sum_{l=1}^J \tilde{g}_{t+1}^{(l)}. \right. \]
\item For each particle $j=1,\ldots,J$,
	\begin{enumerate}
    \item Resample: sample an index $k\in\{1,\ldots,j,\ldots,J\}$ with associated probabilities $\{g_{t+1}^{(1)},\ldots,g_{t+1}^{(j)},\ldots,g_{t+1}^{(J)}\}$.
	\item Regenerate the fixed parameters:
	\[ \theta_{t+1}^{(j)} \sim N\left( m_t^{(k)}, h^2V_t \right), \]
	\item Propagate:
	\[ x_{t+1}^{(j)} \sim p\left(x_{t+1}\left|x_t^{(k)},\theta_{t+1}^{(j)}\right.\right), \mbox{ and} \]
	\item Calculate weights and renormalize:
	\[ \tilde{w}_{t+1}^{(j)} = \frac{p\left(y_{t+1}\left|x_{t+1}^{(j)},\theta_{t+1}^{(j)}\right.\right)}{p\left(y_{t+1}\left|\mu_{t+1}^{(k)},m_t^{(k)}\right.\right)}
	\qquad
	w_{t+1}^{(j)} = \tilde{w}_{t+1}^{(j)}\left/\sum_{l=1}^J \tilde{w}_{t+1}^{(l)}. \right. \]
	\end{enumerate}
\end{enumerate}

\noindent The KDPF adds the kernel density regeneration to the auxiliary particle filter.

To use the KDPF with normal kernels it is necessary to parameterize the fixed parameters so that their support is on the real line. This is not a constraint, but rather a practical implementation detail. We typically use logarithms for parameters that have positive support and the logit function for parameters in the interval (0,1). A parameter $\psi$ bounded on the interval (a,b) can first be rebounded to (0,1) through $(\psi-a)/(b-a)$, and then the logit transformation can be applied. We discuss the sensitivity of the performance of the KDPF to the choice of transformation later on.

\subsection{Resampling \label{sec:advice}}

In addition to choosing which particle filter algorithm to use, successful implementation also depends on which resampling scheme to use and when to resample. Throughout our discussion, we have explicitly used multinomial resampling, but alternative resampling schemes exist including stratified, residual, and systematic resampling \citep{Douc:Capp:Moul:comp:2005}. Resampling is meant to rebalance the weights of the particles in order to avoid degeneracy, but this introduces additional Monte Carlo variability to the particle sample. %This cost can be understood by considering the situation where no data are observed and multinomial resampling is performed at every iteration of the particle filter. In this case, our knowledge about the states and unknown parameters should not change over time, but we lose information about the filtered distribution by resampling. The four resampling techniques mentioned in this paper may add different amounts of Monte Carlo variability to the particle sample for a given model setting. 
In Section \ref{sec:resample}, we discuss some advantages and disadvantages of the different resampling methods and suggest the use of stratified or residual sampling.

In addition, the frequency of resampling should be reduced to balance the loss of information due to degeneracy with the loss of information due to the additional Monte Carlo variability introduced during resampling. Typically, a measure of the nonuniformity of particle weights is used to determine if resampling should be performed at a given iteration of a particle filter. The common measures are effective sample size, coefficient of variation, and entropy. We use effective sample size \citep{Liu:Chen:Wong:reje:1998}, a value that can be interpreted as the number of independent particle samples, ranging between 1 and $J$, with $J$ corresponding to all particle weights being equal and 1 corresponding to one particle weight being 1 with the rest 0. Using this measure of nonuniformity in our runs, we set a threshold of $0.8J$, meaning that if the number of independent samples is less than 80\% of the total number of particles, resampling is performed. 

The BF, APF, and KDPF algorithms described in the previous sections were constructed under the assumption that resampling is performed at every iteration of the filter. If resampling is not performed, we modify the algorithm at that timepoint by 1) omitting the `Resample' step, 2) replacing all instances of the sampled index $k$ with the particle index $j$, and 3) adjusting the calculation of $\tilde{w}_{t+1}^{(j)}$ by multiplying by $w_t^{(j)}$, i.e. the particle weights get carried over. For the KDPF, regeneration is not performed when resampling is not performed since, in this case, there is no reduction in the number of unique fixed parameter values. When resampling is performed, the `Regenerate the fixed parameters' step should read $\theta_{t+1}^{(j)} = \theta_t^{(j)}$.

\subsection{Theoretical justification of particle filters}

\danny{Should we put something in here with links to literature on theoretical justification of particle filters? Essentially the idea would be to state that all these methodologies are justified as the number of particles becomes large which is why in Section \ref{sec:results} we use an increasing number of particles.}

\section{Particle filtering in epidemiological models \label{sec:apply}}

We now describe how epidemiological modeling can be addressed via state-space models with sequential estimation as described in Section \ref{sec:ss}. Thereby we set the stage for application of the particle filtering methodology described in Section \ref{sec:filtering}. As in \citet{skvortsov2012monitoring}, we consider a modified SIR model with stochastic fluctuations describing the dynamics of the disease outbreak \citep{herwaarden1995stochepid, dangerfield2009stochepid, anderson2004sars}. According to this model, we track the population proportion that is susceptible ($s_t$), infectious ($i_t$), and recovered ($r_t$), i.e. no longer able to be infected, at time $t$. Mathematically, $s_t$, $i_t$, and $r_t$ are all nonnegative and $s_t + i_t + r_t = 1$ for all $t$.

When monitoring an epidemic, the true $s_t$, $i_t$ and $r_t$ are unknown and regarded as hidden states of the model. The observed data are gathered via syndromic surveillance. In our state-space model of a disease outbreak, the observation equation specifies how the observed data depend on the state of the epidemic and the state equation describes how the epidemic evolves over time.

\subsection{Model \label{sec:model}}

First, we describe the state equation. Let $x_t = (s_t,i_t)'$ denote the state of the epidemic at time $t$. Note that, by definition, $r_t$ is completely specified once $s_t$ and $i_t$ are known, and hence is not needed in the state vector. Our compartmental model of disease transmission is governed by three parameters:

\begin{itemize}
\item $\beta$, the contact rate of spread of disease,
\item $\gamma$, the recovery time from infection, and
\item $\nu$, the mixing intensity of the population.
\end{itemize}

\noindent $\beta$, $\gamma$, and $\nu$ are each restricted to be nonnegative. Define $\theta = (\beta,\gamma,\nu)'$ to be the vector of unknown parameters in our model, and let $P$ be the size of the population. Then, we describe the evolution of the epidemic from time $t$ to $t + 1$ by
\begin{equation}
x_{t+1}\left|x_t,\theta\right. \sim N_\Omega\left(f(x_t,\theta),Q(\theta)\right) \label{eqn:state}
\end{equation}
\noindent where
\[
f(x_t,\theta) = \left(
\begin{array}{c}
s_t - \beta i_ts^{\nu}_t \phantom{- \gamma i_t}\,\, \\
i_t +  \beta i_ts^\nu_t - \gamma i_t
\end{array}
\right)
\qquad
Q(\theta) = \frac{\beta}{P^2} \left(
\begin{array}{ccccc}
1 & -1 \\
-1 & 1 + \gamma/\beta
\end{array}
\right)
\]

\noindent and $\Omega = \{(s_t,i_t): s_t \ge 0, i_t \ge 0, s_t + i_t \le 1\}$. $N_{\Omega}(\mu,\Sigma)$ represents the normal distribution with mean $\mu$ and covariance matrix $\Sigma$ truncated onto the set $\Omega$. %Notice that in equation \eqref{eqn:state}, the joint distribution of $s_t$ and $i_t$ has been truncated since they are proportions that must have sum no greater than 1. To generate values from this density during the `Propagate' step of a particle filter, we sample from the untruncated normal distribution, reject any samples outside of $\Omega$, and resample when needed.

%The dynamics of the epidemic with respect to time, $t$, are then described by the following differential equations:
%
%\begin{align}
%\frac{ds}{dt} &= -\beta is^\nu + \epsilon_\beta \label{eqn:dsdt} \\
%\frac{di}{dt} &= \beta is^\nu - \gamma i - \epsilon_\beta + \epsilon_\gamma \label{eqn:didt}
%\end{align}

%The standard deviations of the noise terms are approximated by
%\[\sigma_\beta \approx \frac{\sqrt{\beta}}{P} \mbox{, } \sigma_\gamma \approx \frac{\sqrt{\gamma}}{P}\]
%This comes from a well-known scaling rule of random fluctuations in the contact rate and recovery time \citep{ovaskainen2010extinction, herwaarden1995stochepid, dangerfield2009stochepid}.

We consider observed data that are positive real numbers related to counts of emergency room visits, prescription sales, or calls to a hotline, for example, and we can observe data from these different streams/sources asynchronously in time. That is, at any time $t$, we can observe data from any subset of the streams (or possibly none of them). Let $y_{l,t}$ represent data coming from stream $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$. We model the the log of the observations (so that $y_{l,t}$ is restricted to be positive) by
\begin{equation}
\log y_{l,t} \sim N\left(b_li_t^{\varsigma_l} + \eta_l,\sigma_l^2\right) \label{eqn:obs}
\end{equation}
where $b_l$, $\varsigma_l$, and $\sigma_l$ are nonnegative constants \citep{skvortsov2012monitoring} and $\eta_l$ is a real number that determines the baseline level of incoming syndromic data from stream $l$. Initially, we assume $b_l$, $\varsigma_l$, $\sigma_l$, and $\eta_l$ are known for all $l$. In an extended analysis described in Section \ref{sec:extend}, we demonstrate that improved SMC methods enable us to regard these constants as unknown and include them in $\theta$.

Having formulated the data-generating model, we define $y_t = (y_{1,t},\ldots,y_{L,t})'$ and specify the likelihood of an observation $y_t$ given $x_t$ and $\theta$ - i.e. $p(y_t|x_t,\theta)$ - by $y_t|x_t,\theta \sim \log N(\mu_t,\Sigma_t)$, where $\mu_t$ is an $L$-length vector with element $l$ equal to $b_li_t^{\varsigma_l} + \eta_l$ and $\Sigma_t$ is an $L \times L$ diagonal matrix with the $l^{\mbox{th}}$ diagonal equal to $\sigma_l^2$. Elements of $y_t$ may be missing, in which case the dimension of $p(y_t|x_t,\theta)$ shrinks by the number of missing elements. %If the $j^{\mbox{th}}$ element of $y_t$ is missing, then the $j^{\mbox{th}}$ element of $\mu_t$ and $j^{\mbox{th}}$ row and column of $\Sigma_t$ are also missing and thus omitted in the calculation of the likelihood.
If all elements of $y_t$ are empty (i.e. if no syndromic data are observed at time $t$), particle weights are kept the same as they were at the previous time step.

%A further note on propagating particles is necessary.  Notice from equation \eqref{eqn:state} that the transition density needed to propagate $x_t$ in our model is a multivariate normal density with mean $f_\tau(x_t,\theta)$, and recall that the mean function approximates the disease dynamics better for small $\tau$.  Thus, for data observed at times $t_1$ and $t_2$ where the time between observations, $t_2 - t_1$, is large, the mean of $p(x_{t_2}|x_{t_1}^{(j)})$ for each particle $j$ may be inaccurate if $\tau$ is set to $t_2 - t_1$.  To ensure that the transition density matches the actual disease dynamics more closely, we can choose $d$ to be an integer larger than 1, set $\tau = (t_2 - t_1) / d$, and propagate $x_{t_1}^{(j)}$ forward $d$ times to generate $x_{t_2}^{(j)}$.

Since $p(x_{t+\tau}|x_t,\theta)$ and $p(y_t|x_t,\theta)$ are nonlinear functions of $x_t$ and $\theta$, a closed-form solution to the posterior $p(x_t,\theta|y_{1:t})$ cannot be obtained. Thus, we use the particle filtering techniques described in Section \ref{sec:filtering} to approximate $p(x_t,\theta|y_{1:t})$ for all $t$.

\section{Comparing particle filters} \label{sec:results}

We now compare the performance of the BF, APF and KDPF using simulated data analogous to that used in \citep{skvortsov2012monitoring}. In addition, we examine the performance of the KDPF when using bounded versus unbounded priors on the fixed parameters and under different resampling schemes.

\subsection{Simulation}

An epidemic lasting $T = 125$ days was simulated from our model for a population of size $P = 5000$. True values of parameters that we estimate (termed "unknown") were set to $\beta = 0.2399$, $\gamma = 0.1066$, and $\nu = 1.2042$, and values of known constants for $L = 4$ streams are given in Table \ref{tab:constants} ($\eta_l$ was set to 0 for all $l$). Infection was introduced in 10 people in the population at day 0 (i.e. true $i_0 = 10/5000$ and $s_0 = 4990/5000$). The epidemic evolved over time according to equation \eqref{eqn:state} and data from randomly selected streams at each day were generated from equation \eqref{eqn:obs}. The simulated epidemic peaks around $t = 50$ days an results in around 80\% of the susceptible population becoming infected (see Figure \ref{fig:data}).

\begin{table}[hb]
\begin{center}
\caption{Values of known constants in model.}
\label{tab:constants}
\begin{tabular}{|cccc|}
\hline
$l$ & $b_l$ & $\varsigma_l$ & $\sigma_l$ \\
\hline
1 & 0.25 & 1.07 & 0.0012 \\
2 & 0.27 & 1.05 & 0.0008 \\
3 & 0.23 & 1.01 & 0.0010 \\
4 & 0.29 & 0.98 & 0.0011 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\centering
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-epid}
\end{minipage}
\begin{minipage}{0.48\linewidth}
\includegraphics[width=1.0\textwidth]{sim-orig-z}
\end{minipage}
\caption{Simulated epidemic curves (left) and syndromic observations (right).} \label{fig:data}
\end{figure}

\subsection{Particle filter runs} \label{sec:pf}

The BF, APF, and KDPF were each run on the simulated data using $J = 1000, 10000, 20000, \mbox{ and } 40000$ particles to obtain weighted sample approximations of $p(x_t,\theta|y_{1:t})$ for $t = 1,\ldots,T$. For each $J$, separate runs using multinomial, residual, stratified, and systematic resampling were implemented, and an effective sample size threshold set at 80\% of the total number of particles was used to determine when to resample particles \citep{Liu:Chen:Wong:reje:1998}. For the KDPF, the discount factor $\Delta$ was set to 0.99 to determine the variance of the normal kernels used to approximate $p(\theta|y_{1:t})$.

The initial state, $x_0$, and fixed parameters, $\theta_0$, were sampled from prior density $p(x_0,\theta) = p(\theta)p(i_0,s_0)$, where $p(i_0,s_0)$ is the joint pdf of the random variables $i_0$ and $s_0$ and $p(\theta) = p(\beta)p(\gamma)p(\nu)$. Since a very small percentage of the population is infected during the initial stage of the epidemic, we let $p(i_0) = N_{[0,1]}(i_0;0.002,0.0005^2)$ and set $s_0 = 1 - i_0$, i.e. no infected individuals have recovered from illness yet.

To investigate the impact of different prior distributions of $\theta$ on performance of the particle filters, the runs described above were performed once using uniform priors on $\theta$ and then again using log-normal priors. Uniform priors on $\theta$ were chosen to be the same as those used in \citet{skvortsov2012monitoring}, i.e. $p(\beta) \sim U[0.14, 0.50]$, $p(\gamma) \sim U[0.09, 0.143]$, and $p(\nu) \sim U[0.95,1.3]$. Log-normal priors on $\theta$ are given by $\beta \sim \ln N\left(-1.3296, 0.3248^2\right)$, $\gamma \sim \ln N\left(-2.1764, 0.1183^2\right)$, and $\nu \sim \ln N\left(0.1055, 0.0800^2\right)$. These log-normal priors constrain $\beta$, $\gamma$ and $\nu$ to be positive. The mean and variance of the log of the unknown parameters were chosen so that random draws on the original scale would fall within the uniform bounds with 95\% probability.

Logit and log transformations were applied to the components of $\theta$ in the manner described at the end of Section \ref{sec:kd} so that the normal kernel could be used in the KDPF while constraining $\beta$, $\gamma$, and $\nu$ to be within their respective prior domains (i.e. logit was used with uniform priors and log with log-normal priors). %To aid in comparing particle filters, the same prior draws were used in the BF, APF, and KDPF as long as the number of particles and prior were the same. \danny{But isn't the prior different, e.g. uniform vs log-normal?}

\subsection{Comparison of particle filter algorithms under uniform priors} \label{sec:pfcomparison}

First, we compare the performance of the particle filtering algorithms using uniform priors on $\theta$ and systematic resampling, since these priors and resampling scheme were used in \citet{skvortsov2012monitoring}. For ease of comparison, the same prior draws were used in each particle filtering algorithm for fixed $J$. Figure \ref{fig:pfs} shows 95\% credible bounds of $p(\beta|y_{1:t})$, $p(\gamma|y_{1:t})$, and $p(\nu|y_{1:t})$ for $t = 1,2,\ldots,T$ and $J = 1000, 10000, 20000, 40000$. Initially, bounds for the BF and APF match those of KDPF, but quickly degenerate toward a single value due to elimination of unique particles during resampling. Although the time of degeneracy increases as $J$ gets larger, the BF and APF bounds become misleading during the second half of the epidemic even for $J = 40000$. The bounds for the KDPF, on the other hand, have dramatically reduced degeneracy since new values of $\theta$ are regenerated from the kernel density approximation.

The KDPF also has an advantage over the BF and APF in terms of computational efficiency. Notice that the bounds for the KDPF become wider as $J$ increases, but they do not change much for $J > 10000$. This suggests that by 20000 particles, the weighted sample approximation of $p(x_t,\theta|y_{1:t})$ has converged to the true posterior over the entire epidemic period, unlike with the BF and APF. Even though the bounds for $\beta$ with the BF and APF seem to roughly match those of the KDPF for $J = 20000$ and $J = 40000$ over the first half of the epidemic, the KDPF provides the same measure of uncertainty even for $J = 10000$ and does not degenerate in the second half of the epidemic.

Also noteworthy is how the bounds for $\nu$ expand between $t = 70$ and $t = 80$ for $J > 1000$. We typically expect to see the width of credible intervals decrease monotonically over time as data is accumulated. A more plausible explanation is that the marginal filtered distribution of $\nu$ has been squeezed against the upper bound in the prior for $\nu$. Rerunning the analysis using more relaxed prior bounds on $\nu$ shows a shift in the distribution toward lower values as opposed to the widening of the interval that we see in the figure (results not shown).

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-systematic-uniform-40000}
\caption{Sequential 95\% credible intervals for $\beta$ (left column), $\gamma$ (middle column), and $\nu$ (right column) for increasing number of particles (rows) for the BF (red), APF (blue), and KDPF (green) with the truth (black) when using systematic resampling and uniform priors.} \label{fig:pfs}
\end{figure}

\subsection{Comparison of logit versus log transformation in the KDPF}

As mentioned in \ref{sec:kd}, implementing the KDPF using a normal kernel density approximation of $p(\theta|y_{1:t})$ to regenerate the fixed parameters requires applying some transformation to the components of $\theta$ so that their support is on the real line. The logit function is a convenient choice for mapping fixed parameters with bounded support to the real line, and likewise the log function is convenient for fixed parameters with positive support. Thus, we investigate the sensitivity of the performance of the KDPF with $J = 10000$ to these two types of transformations.

Figure \ref{fig:priors} compares scatterplots of $\beta$ versus $\gamma$ sampled jointly from the filtered distribution $p(\beta,\gamma|y_{1:t})$ at $t = 15, 30, 45, 60$. In the top row, the logit transformation was applied to each element of $\theta$ before the `Regenerate the fixed parameters' step in the KDPF, while the log transformation was applied in the KDPF run in the bottom row. The same uniform prior samples were drawn at $t = 0$ in both rows of the figure. In the first row, we notice a concentration of points squeezed against the boundaries of the uniform prior on $\gamma$, particularly for $t = 15$ and $t = 30$. In addition, we notice the points forming an S-shape at $t = 45$. This is a byproduct of the logit transformation extending values of $\gamma$ that are close to the boundary to the whole real line, in effect truncating the points so that they are kept within the prior bounds for $\gamma$. This suggests to us that the prior bounds on $\gamma$ are too restrictive to account for the uncertainty in the recovery time during the climb of the epidemic, when more people are getting sick than recovering from illness. This problem does not present itself with $\beta$, however, because the uniform bounds for $\beta$ were chosen to be wide enough to capture the uncertainty about this parameter over the course of the epidemic.

In the second row of Figure \ref{fig:priors}, we notice that points stray outside the uniform prior bounds for $\gamma$ enforced by the logit transformation in the first row. This is allowed to happen because the log transformation only requires that fixed parameter values be positive. However, the normal density kernel used in the KDPF should, on average, pull random draws toward the overall mean of the particles. The fact that a significant number of particles lie outside of the bounds for the uniform prior draws for $\gamma$ indicates that the data are trying tell us that there is mass in the filtered distribution outside of those bounds. We also notice that the correlation between $\beta$ and $\gamma$ decreases less between $t = 45$ and $t = 60$ in the second row than it does in the first. This is also an artifact of the truncation of points caused by the logit transformation and suggests that this choice of parametrization to enforce uniform prior bounds introduces bias into the filtered distribution of $\theta$.

%In addition, points begin to form an S-shape at $t = 30$. This is a byproduct of the logit transformation extending values of $\gamma$ that are close to the boundary to the whole real line. To relax the restriction on $\gamma$ imposed by the prior bounds, we ran the particle filter again using the same prior draws as in the first row of Figure \ref{fig:priors}, but without any transformation on $\theta$. Scatterplots of $\beta$ versus $\gamma$ for this run are shown in the second row of Figure \ref{fig:priors}, and we see now that particles at $t = 15$ and $t = 30$ that would have been squeezed against the boundary by the logit transformation now lie outside of the uniform bounds for $\gamma$. There is no longer an S-shape in any of the graphs, and also noticeable is that the sample correlation between $\beta$ and $\gamma$ at $t = 60$ is larger than when the logit transformation was applied to $\theta$.

%\danny{perhaps say something about how the prior is introducing bias} This suggests that the more drastic decrease in correlation between $t = 45$ and $t = 60$ in the first row is not driven by the data, but rather is an artifact of the restrictive uniform bounds enforced by the logit transformation.

Ideally, we'd like to apply some transformation to extend $\theta$ to the whole real line so that the KDPF operates smoothly, but we don't want the transformation to influence the particles in any way. %The third row of Figure \ref{fig:priors} shows the KDPF run using log-normal priors with a log transformation on the components of $\theta$. As in the second row of Figure \ref{fig:priors}, the distribution of points do not appear truncated or S-shaped. While a majority of the points in the third row of Figure \ref{fig:priors} are within the uniform bounds, some lie outside because the transformation is not overly restrictive. 
We prefer log-normal priors on positive elements of $\theta$ because it allows us to use prior knowledge of the epidemic to encourage points to lie between the bounds at $t = 0$, but is more flexible in the event of model mis-specification and allows the data to play a greater role in dictating the evolution of $p(x_t,\theta|y_{1:t})$.

\begin{figure}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-uniform-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, logit transformation}
\end{minipage}

%\vspace{0.5cm}
%
%\begin{minipage}{1.0\linewidth}
%\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-semi-uniform-systematic-10000-betagamma}
%\vspace{-1.0cm}
%\caption*{uniform prior draws, no transformation}
%\end{minipage}

\vspace{0.5cm}

\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{Hist-KD-uniform-lognormal-systematic-10000-betagamma}
\vspace{-1.0cm}
\caption*{uniform prior draws, log transformation}
\end{minipage}

%\vspace{0.5cm}
%
%\begin{minipage}{1.0\linewidth}
%\includegraphics[width=1.0\textwidth]{Hist-KD-lognormal-lognormal-systematic-10000-betagamma}
%\vspace{-1.0cm}
%\caption*{log-normal prior draws, log transformation}
%\end{minipage}

\caption{Scatterplots of $\beta$ (x-axis) versus $\gamma$ (y-axis) at $t = 15, 30, 45, 60$ days using the KDPF with $J = 10000$ particles and systematic resampling. The top row corresponds to the KDPF run using a logit transformation on $\theta$ prior to regenerating the fixed parameters, and the bottom row uses a log transformation. The same uniform prior draws of $\theta$ were sampled in each row at $t = 0$. For demonstration, each panel shows 500 particles sampled from the weighted sample approximation of $p(x_t,\theta|y_{1:t})$. The sample correlation coefficient, $r$, displayed in the subtitle of each plot, is computed from the entire particle sample. Red crosses indicate the true values of $\beta$ and $\gamma$ used for simulation. Axes are the same in each panel. Dashed horizontal lines indicate the bounds of the uniform prior on $\gamma$ that was used in the top two rows. Uniform bounds on $\beta$ are not shown because they lie outside the range of the x-axis.} \label{fig:priors}

\end{figure}

\subsection{Comparison of resampling schemes \label{sec:resample}}

As mentioned in Section \ref{sec:advice}, resampling is meant to rebalance the weights of the particles in order to avoid degeneracy at the cost of increasing the Monte Carlo variability of the particle sample.  Up to this point, we have used only systematic resampling, as in \citet{skvortsov2012monitoring}. Alternatively, we could have chosen multinomial, residual, or stratified resampling. \citet{Douc:Capp:Moul:comp:2005} explains each of these methods in detail and shows that 1) multinomial resampling introduces more Monte Carlo variability than does residual or stratified resampling, 2) residual and stratified resampling introduce the same amount of Monte Carlo variability, on average, and 3) systematic resampling can introduce more Monte Carlo variability than does multinomial resampling. \danny{now that I read this it would be nice if we could set up an example within the context of this model where systematic resampling does worse}

With this in mind, we turn to a comparison of different techniques for the resampling step of the KDPF with 10000 particles and log-normal priors on $\theta$. To aid in comparison of the different resampling techniques, the same prior draws were used in all particle filter runs for fixed $J$. We would like to choose the resampling scheme for which the filtered distribution, $p(x_t,\theta|y_{1:t})$, approaches the true posterior the fastest as a function of the number of particles. Figure \ref{fig:resamp} shows that multinomial resampling appears to be outperformed by the other three resampling techniques. For example, the bounds for $\nu$ using 10000 or 20000 particles with multinomial resampling deviates from the true posterior (approximated by the white area in the plots) more than with the other three resampling techniques. This is also clear when looking at the plot for $\gamma$ at $J = 20000$. While systematic resampling seems to perform just as well as the remaining two resampling schemes in this example, we prefer to use stratified (or residual) resampling due to the example shown in \cite{Douc:Capp:Moul:comp:2005} where systematic resampling has more Monte Carlo variability than any of the other three resampling schemes.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-KD-normal-40000}
\caption{Sequential 95\% credible intervals for the three model parameters (columns) for increasing number of particles (rows) for multinomial (red), residual (blue), stratified (green), and systematic (purple) using the KDPF with log-normal priors on $\theta$. White are is the time-wise average among the four resampling techniques using 40000 particles.} \label{fig:resamp}
\end{figure}

\section{Additional Unknown Parameters \label{sec:extend}}

We now turn our focus to extending the analysis to include $\{b_l,\varsigma_l,\sigma_l,\eta_l:l\in1,\ldots,L\}$ as unknown parameters. For this section, we consider data coming from only $L = 1$ stream and let $\theta = (\beta, \gamma, \nu, b, \varsigma, \sigma, \eta)'$, dropping the subscript $l$. Using the same simulated epidemic, data were simulated from equation \eqref{eqn:obs} with true values of $b$, $\varsigma$, $\sigma$, and $\eta$ set to $0.25$, $1$, $0.001$ and $2$, respectively. Days at which data were observed from the single stream were randomly selected.

The KDPF with tuning parameter $\Delta$ set to 0.99 was run with $J = 40000$ particles, and stratified resampling was used with an effective sample size threshold of 0.8. As before, fixed parameter values were regenerated only when resampling was performed. Initial states and parameters were sampled from their prior with all parameters being independent of each other and of the state. The priors for the state and for $\beta$, $\gamma$, and $\eta$, are the same as that defined in Section \ref{sec:pf}. The priors for $b$, $\varsigma$, $\sigma$, and $\eta$ are $\log\left(b\right) \sim  N\left(-1.6090, 0.3536^2\right)$, $\log\left(\varsigma\right) \sim N\left(-0.0114, 0.0771^2\right)$, $\log\left(\sigma\right) \sim N\left(-7.0516, 0.2803^2\right)$, and $\eta \sim N\left(2.5, 1\right)$. The choice of prior mean and standard deviation on the log scale were made such that random draws of $b$, $\varsigma$, and $\sigma$ on the original scale would be within $(0.1, 0.4)$, $(0.85, 1.15)$, and $(0.0005, 0.0015)$, respectively, with 95\% probability. To assess the loss in precision of our estimates due to incorporating more uncertainty into our analysis, we also ran the KDPF using 40000 particles with $b$, $\varsigma$, $\sigma$, and $\eta$ assumed known at their true values used for simulating the data as in Section \ref{sec:results} (we refer to this run as the original analysis).

Figure \ref{fig:ext} shows sequential 95\% credible intervals for both the extended (blue lines) and the original (red lines) analyses. Most noticeable from Figure \ref{fig:ext} is that the intervals for $\beta$, $\gamma$, $\nu$, $s_t$, and $i_t$ are wider for the extended analysis than they are for the original. This is due to the added uncertainty in $b$, $\varsigma$, $\sigma$, and $\eta$ in the extended analysis. Nonetheless, we are still able to obtain credible intervals for the unknown parameters that cover the true values and intervals for the states that cover the true epidemic curves by increasing the number of particles used in the KDPF.

%Nonetheless, we are still able to adequately estimate the epidemic curves even with less data available and imprecise knowledge of the parameters $b$, $\varsigma$, $\sigma$, and $\eta$ by increasing the number of particles used in the KDPF. In addition, we get a very precise estimate of $\eta$, the parameter that controls the baseline value of the syndromic observations.

We also notice from this figure that the lines appear choppier than in earlier plots, with flat periods followed by sharp changes in uncertainty. This is due to the fact that data are coming from only one stream, leading to more time points where no data are available and making the analysis more sensitive to abnormal data. Gaps in the data lead to a lack of resampling of particles and cause more drastic shifts in the posterior distribution of $\theta$ once data arrive. For example, a sharp shift in the distribution of $\beta$ occurs around $t = 25$ because of an influx of data following a period of no data and hence scarce resampling of particles. Also, we notice a spike in the $s$ and $i$ curves right before $t = 40$ because of a shift in the trajectory of data points.

Lastly, we comment on a widening of the credible intervals for $\nu$ in the extended analysis. This phenomenon suggests that the log-normal prior on $\nu$ that samples particles in $[0.9,1.3]$ is too tight, and that our model provides even less insight about this parameter than our prior belief. Scarce knowledge about $\nu$ is gained over the course of the epidemic in the original analysis due to the nonlinear nature of the evolution equation with respect to $\nu$, and we learn even less about $\nu$ in the extended analysis.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{PF-ext-KD-stratified-normal-40000}
\caption{Sequential 95\% credible intervals for the states and fixed parameters from the extended analysis (blue) and the original analysis (red). The KDPF with $J = 40000$ particles was run with stratified resampling. Tick marks are shown along the bottom of the plots for $\beta$ at time points when data were observed (dark gray) and when particles were resampled (blue and red for the extended and original analyses, respectively).\danny{do we need these tick marks anymore?}} \label{fig:ext}
\end{figure}

\section{Discussion \label{sec:discussion}}

\input{discussion}

\clearpage

\bibliographystyle{model1-num-names}
\bibliography{jarad}

\end{document} 